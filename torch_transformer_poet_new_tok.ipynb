{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-transformer-poet/blob/main/torch_transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Torch-Transformer-Poet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DabS0VZ-1Zp0"
   },
   "source": [
    "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5T4m6earb1e"
   },
   "outputs": [],
   "source": [
    "# from ml_indie_tools.env_tools import MLEnv\n",
    "import ml_indie_tools.pytorch_meta_tools as MJ\n",
    "from ml_indie_tools.train_utils import TrainUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3kgZd3AnAnu"
   },
   "outputs": [],
   "source": [
    "# Optional experimental event server to record and propagate training progress, not (yet) recommended!\n",
    "# Functionality is ignored by default.\n",
    "try:\n",
    "    import indralib\n",
    "    indra_avail = True\n",
    "except Exception as e:\n",
    "    indra_avail = False\n",
    "if indra_avail is True:\n",
    "    print(\"Indralib is available, trying to connect to Indrajala server for training progress reports...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVcwvURB5EZN"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.Logger(\"Main\")\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Qg3ZPBmC8kO"
   },
   "source": [
    "## 1. Project configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-TP3Pnsrb1f",
    "outputId": "7437d18e-c770-4949-b958-9cdefa3901dc"
   },
   "outputs": [],
   "source": [
    "# project_name = 'women_writers'\n",
    "# project_name='research'\n",
    "project_name='fast_tok'\n",
    "model_cpu = None\n",
    "model_name=f'{project_name}_v1'\n",
    "\n",
    "use_preprocessed_data = False                      # Use already tokenized data\n",
    "use_existing_model_from_checkpoint = False         # Try to load checkpoint of training\n",
    "use_torch_compile = True                           # Requires a modern graphics card with torch compile backend support\n",
    "skip_additional_texts = False                       # Don't look for other data sources in `additional_texts.json`\n",
    "\n",
    "if 'google.colab' in sys.modules:  # Google colab notebooks run on server that provide UTC time, we adapt logs to local time:\n",
    "    local_timezone = ZoneInfo('Europe/Berlin')\n",
    "else:\n",
    "    local_timezone = None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device\n",
    "\n",
    "model_path = \"model\"\n",
    "data_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  2.1 Text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(path:str = \"~/BookTextLib/Texts\") -> list[str]:\n",
    "    texts: list[str] = []\n",
    "    real_path = os.path.expanduser(path)\n",
    "    if os.path.isdir(real_path) is False:\n",
    "        print(f\"{real_path} is not a directory!\")\n",
    "        return texts\n",
    "    for root, dirs, files in os.walk(real_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                txt_file = os.path.join(root, file)\n",
    "                with open(txt_file, 'r') as f:\n",
    "                    txt = f.read()\n",
    "                    texts.append(txt)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C66X7ynnrb1h"
   },
   "outputs": [],
   "source": [
    "text_list = get_texts()\n",
    "text_corpus = '\\n\\n\\n'.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of texts: {len(text_list)}, corpus length in bytes: {len(text_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSm4f9NSC8kQ"
   },
   "source": [
    "## 2.3 Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePieceBPE:\n",
    "    def __init__(self, model_name, vocab_size=32768, data_directory='data'):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_name = model_name\n",
    "        self.model_path = os.path.join(data_directory, model_name)\n",
    "        self.temp_file = os.path.join(data_directory, \"tok_blob.txt\")\n",
    "        self.corpus_encoded_path = os.path.join(data_path, \"corpus_encoded.json\")\n",
    "        self.sp = None\n",
    "        self.encoded_corpus = []\n",
    "\n",
    "    def train(self, texts:list[str], verbose=True):\n",
    "        if verbose:\n",
    "            print(\"1/2: Starting tokenizer...\")\n",
    "        blob = '\\n'.join(texts)\n",
    "        with open(self.temp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(blob)\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=self.temp_file,\n",
    "            model_prefix=self.model_path,\n",
    "            vocab_size=self.vocab_size,\n",
    "            model_type='bpe',\n",
    "            character_coverage=1.0,  # Important for multilingual\n",
    "            normalization_rule_name='identity',  # No normalization\n",
    "            byte_fallback=True\n",
    "        )\n",
    "        print(\"Tokenizer trained.\")\n",
    "        self.load()\n",
    "        print(\"2/2: Encoding entire corpus...\")\n",
    "        self.encoded_corpus = self.encode(blob)\n",
    "        print(\"Corpus encoded\")\n",
    "        with open(self.corpus_encoded_path, \"w\") as f:\n",
    "            json.dump(self.encoded_corpus, f)\n",
    "\n",
    "    def load(self):\n",
    "        # Load the model\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(f\"{self.model_path}.model\")\n",
    "        try:\n",
    "            with open(self.corpus_encoded_path, \"r\") as f:\n",
    "                self.encoded_corpus = json.load(f)\n",
    "        except:\n",
    "            print(\"Corpus not yet encoded!\")\n",
    "\n",
    "    def get_corpus_batches(self, context_length):\n",
    "        n = len(self.encoded_corpus) - context_length - 1\n",
    "        if n <=0:\n",
    "            return 0\n",
    "        record_count = int(n)\n",
    "        return record_count\n",
    "\n",
    "    def get_random_tokens(self, context_length):\n",
    "        n = self.record_count(context_length)\n",
    "        if n==0:\n",
    "            return None\n",
    "        ind = random.randint(0, n)\n",
    "        toks = self.encoded_corpus[ind:ind+context_length]\n",
    "        return toks\n",
    "\n",
    "    def get_random_token_pair(self, context_length):\n",
    "        toks = self.get_random_tokens(context_length+1)\n",
    "        pair = (toks[0:context_length], toks[1:context_length+1])\n",
    "        return pair\n",
    "\n",
    "    def encode(self, text:str):\n",
    "        tokens = self.sp.encode_as_ids(text)\n",
    "        return tokens\n",
    "\n",
    "    def visualize_tokens(self, text:str):\n",
    "        pieces = sp.encode_as_pieces(text)  # For visualization\n",
    "        return pieces\n",
    "\n",
    "    def decode(self, ids):\n",
    "        decoded = self.sp.decode(ids)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 32768\n",
    "\n",
    "tokenizer = SentencePieceBPE(model_name=model_name, vocab_size=vocab_size, data_directory=data_path)\n",
    "if use_preprocessed_data is True:\n",
    "    tokenizer.load()\n",
    "else:\n",
    "    tokenizer.train(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oImAp6-DnAnw",
    "outputId": "5698f93a-3e3c-4ce1-fb1b-f0cb3c51dda4"
   },
   "outputs": [],
   "source": [
    "tok_tests = [\"Good morning, this is a simple test sentence for tokenization\",\n",
    "             \"Guten Morgen, dies is ein einfach Testsatz zur Aufteilung in Satzbestandteile\",\n",
    "             \"སེམས་ཉིད་ངལ་བསོ་རྒྱུད་\",\n",
    "             \"སྟོང་ཉིད་སྙིང་རྗེའི་སྙིང་པོ་ཅན།\"]\n",
    "for test in tok_tests:\n",
    "    enc = tokenizer.encode(test)\n",
    "    dec = tokenizer.decode(enc)\n",
    "    if dec != test:\n",
    "        print(f\"Tokenizer failed for: \\n{test} len={len(test)}\\n{dec} len={len(dec)}\")\n",
    "    else:\n",
    "        r = len(enc)/len(test)*100.0\n",
    "        print(f\"Tokenizer: {test}({len(test)}) -> {enc}({len(enc)}) OK, compressed size: {r:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG03WA_yC8kR"
   },
   "source": [
    "## 3. Model metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPMwIn2gC8kR"
   },
   "outputs": [],
   "source": [
    "params = None\n",
    "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'current_loss',\n",
    "                 'sample_every_n_iterations', 'sample_size', 'save_every_n_iterations', 'max_iterations']\n",
    "model_dimension = 384\n",
    "context_length = 96\n",
    "\n",
    "params = { # Multi-head self-attention\n",
    "        'meta_name_template': '{prelude_layers}-{recurrent_layers}/{recurrence_steps}-{coda_layers}x{heads}x{units}x{vocab_size}',\n",
    "\n",
    "        'prelude_layers': 4,\n",
    "        'recurrent_layer_blocks': 1,\n",
    "        'coda_layers': 4,\n",
    "        'recurrent_layers': 4,\n",
    "        'recurrence_steps': 1,\n",
    "        'heads': 12,\n",
    "        'vocab_size': vocab_size,\n",
    "        'context_length': context_length,\n",
    "        'min_dropout': 0.1,  # first layer of prelude, last layer of coda\n",
    "        'max_dropout': 0.2,  # last layer of prelude, first layer of coda\n",
    "        'mid_dropout': 0.1,  # Used by recurrence\n",
    "        'weight_decay': 1e-3,  # L2 regularization, applied by Adam optimizer\n",
    "        'non_linearity': nn.Mish,  # CriticalModule.CriticalActivationLayer,  # Default nn.ReLU\n",
    "        'use_critical': False,  # Add CriticalActivationLayer before recurrent_layer\n",
    "        'model_dimension': model_dimension,\n",
    "        'test_iterations': 100,  # number of iterations for loss estimation\n",
    "\n",
    "        'batch_size': 96,\n",
    "    \n",
    "        'learning_rate': 4e-4,  # Only used, if lr_schedule is False\n",
    "        'lr_schedule': True,\n",
    "        'lr_min': 5e-5,\n",
    "        'lr_max': 1e-4,\n",
    "        'warmup': 2000,\n",
    "        'decay': 50000,\n",
    "    \n",
    "        'grad_clip': 0.8,\n",
    "\n",
    "        'sample_every_n_iterations': 8192,\n",
    "        'sample_size': 128,\n",
    "        'save_every_n_iterations': 8192,\n",
    "\n",
    "        'max_iterations': 100000000  # maximum number of training iterations\n",
    "    }\n",
    "\n",
    "model_file_path = MJ.get_model_filename(model_path)\n",
    "if use_existing_model_from_checkpoint is True:\n",
    "    params = MJ.loamodel_dimension_metadata_from_checkpoint(params, updatable_keys, model_file_path, device=device, log=log) # torch.device('cpu'))\n",
    "if params == None or use_existing_model_from_checkpoint is False:\n",
    "    use_existing_model_from_checkpoint = False\n",
    "\n",
    "num_batches = (len(tokenizer.encoded_corpus) - params['context_length'] - 1) // params['batch_size']\n",
    "print(f\"Batches: {num_batches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U1R4yDlC8kR"
   },
   "source": [
    "## 4. Batch handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZbbsNm0cOeW"
   },
   "outputs": [],
   "source": [
    "def get_sample_batch(batch_size):\n",
    "    for i in range(batch_size):\n",
    "        Xi, yi = tokenizer.get_random_token_pair(params['context_length'])\n",
    "        if i==0:\n",
    "            # smpX=np.array(Xi, dtype=np.float32)\n",
    "            smpX=np.array(Xi, dtype=np.int32)\n",
    "            smpy=np.array(yi, dtype=np.int32)\n",
    "        else:\n",
    "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
    "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
    "    return np.array(smpX), np.array(smpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5VbH5nxnAnx",
    "outputId": "a180872f-dd22-49f8-d771-ddf3f1e19450"
   },
   "outputs": [],
   "source": [
    "x, y = get_sample_batch(2)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgVHUkbhdK9y"
   },
   "outputs": [],
   "source": [
    "sample_data = None\n",
    "\n",
    "def get_torch_batch(batch_size, device, split=None):\n",
    "    x, y = get_sample_batch(batch_size)\n",
    "    tx = torch.tensor(x, dtype=torch.long).to(device)\n",
    "    tx.requires_grad = False\n",
    "    ty = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    ty.requires_grad = False\n",
    "    return tx, ty\n",
    "\n",
    "def get_zero_state(batch_size, context_length, hidden_size, device):\n",
    "    zstate = torch.zeros(batch_size, context_length, hidden_size, device=device)\n",
    "    zstate.requires_grad = False\n",
    "    return zstate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pvbi6kjXC8kS"
   },
   "source": [
    "## 5. Loss and training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIUoglD7nAnx"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dimension, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Precompute positional encodings\n",
    "        pe = torch.zeros(max_len, model_dimension)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dimension, 2).float() * (-math.log(10000.0) / model_dimension))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # [1, max_len, model_dimension]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, batch_size, model_dimension]\n",
    "        seq_len = x.size(0)\n",
    "        pe = self.pe[:, :seq_len, :].expand(-1, x.size(1), -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOfyQjAInAnx"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, model_dimension, heads, projection_dimension, dropout=0.1, non_linearity=nn.ReLU):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(model_dimension, heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(model_dimension)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(model_dimension, projection_dimension),\n",
    "            non_linearity(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(projection_dimension, model_dimension)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(model_dimension)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        if attn_mask is None and x.size(0) > 1:\n",
    "            seq_len = x.size(0)\n",
    "            attn_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "            attn_mask = attn_mask.to(x.device)  # [seq_len, seq_len], upper triangle = True (masked)\n",
    "\n",
    "        attn_output, _ = self.self_attn(x, x, x,\n",
    "                                      attn_mask=attn_mask,\n",
    "                                      key_padding_mask=key_padding_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        return x\n",
    "\n",
    "class LatentRecurrentBlock(nn.Module):\n",
    "    def __init__(self, model_dimension, heads, projection_dimension, recurrent_layers=1, recurrence_steps=3, dropout=0.1, non_linearity=nn.ReLU):\n",
    "        super(LatentRecurrentBlock, self).__init__()\n",
    "        self.recurrence_steps = recurrence_steps\n",
    "        self.self_attn = nn.MultiheadAttention(model_dimension, heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(model_dimension)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.recurrent_layers = recurrent_layers\n",
    "        self.recurrent = nn.LSTM(  # Swap GRU for LSTM\n",
    "            input_size=model_dimension,\n",
    "            hidden_size=model_dimension,\n",
    "            num_layers=recurrent_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(model_dimension, projection_dimension),\n",
    "            non_linearity(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(projection_dimension, model_dimension)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(model_dimension)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        attn_output, _ = self.self_attn(x, x, x,\n",
    "                                      attn_mask=attn_mask,\n",
    "                                      key_padding_mask=key_padding_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        residual = x\n",
    "        batch_size = x.size(1)\n",
    "        latent = x.transpose(0, 1).contiguous()  # [batch, seq_len, model_dimension]\n",
    "        latent = latent.view(batch_size * x.size(0), 1, x.size(2))  # [batch*seq, 1, model_dimension]\n",
    "        h0 = torch.zeros(self.recurrent_layers, latent.size(0), x.size(2), device=x.device)\n",
    "        c0 = torch.zeros(self.recurrent_layers, latent.size(0), x.size(2), device=x.device)  # Add cell state\n",
    "        for _ in range(self.recurrence_steps):\n",
    "            latent, (h0, c0) = self.recurrent(latent, (h0, c0))  # LSTM outputs hidden + cell\n",
    "        latent = latent.view(x.size(1), x.size(0), -1).transpose(0, 1)\n",
    "        latent = residual + latent\n",
    "        ff_output = self.ff(latent)\n",
    "        output = self.norm2(latent + self.dropout2(ff_output))\n",
    "        return output\n",
    "\n",
    "class LatentRecurrentDepthModel(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dimension, heads, context_length, projection_dimension,\n",
    "                 n1_prelude, n2_recurrent, n3_coda, recurrent_layers=1, recurrence_steps=3, min_dropout=0.1, mid_dropout=0.2, max_dropout=0.1, non_linearity=nn.ReLU, use_critical=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary (for embedding and projection).\n",
    "            model_dimension (int): Transformer hidden size.\n",
    "            heads (int): Number of attention heads.\n",
    "            projection_dimension (int): Feedforward hidden size.\n",
    "            n1_prelude, n2_recurrent, n3_coda (int): Number of blocks per stage.\n",
    "            recurrence_steps (int): Recurrent steps per LRD block.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(LatentRecurrentDepthModel, self).__init__()\n",
    "\n",
    "        self.context_length = context_length  # for generate\n",
    "        self.model_dimension = model_dimension\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, model_dimension)\n",
    "        self.pos_encoding = PositionalEncoding(model_dimension, max_len=context_length)\n",
    "        # self.pos_encoding = PositionalEncoding(model_dimension) # , max_len=context_length)\n",
    "\n",
    "        # Prelude blocks\n",
    "        tr_list = []\n",
    "        for i in range(n1_prelude):\n",
    "            if n1_prelude > 1:\n",
    "                drop = min_dropout + (max_dropout - min_dropout)*(i/(n1_prelude-1))\n",
    "            else:\n",
    "                drop = (min_dropout + max_dropout) / 2\n",
    "            tr_list.append(TransformerBlock(model_dimension, heads, projection_dimension, drop, non_linearity))\n",
    " \n",
    "        self.prelude = nn.ModuleList(tr_list)\n",
    "\n",
    "        if use_critical is True:\n",
    "            self.critical = CriticalModule.CriticalActivationLayer(model_dimension)\n",
    "        else:\n",
    "            self.critical = None\n",
    "\n",
    "        # Latent Recurrent blocks\n",
    "        if n2_recurrent > 0:\n",
    "            self.recurrent = nn.ModuleList([\n",
    "                LatentRecurrentBlock(model_dimension, heads, projection_dimension, recurrent_layers, recurrence_steps, mid_dropout, non_linearity)\n",
    "                for _ in range(n2_recurrent)\n",
    "            ])\n",
    "        else:\n",
    "            self.recurrent = None\n",
    "\n",
    "        # Coda blocks\n",
    "        cd_list = []\n",
    "        for i in range(n3_coda):\n",
    "            if n3_coda > 1:\n",
    "                drop = max_dropout - (max_dropout - min_dropout)*(i/(n3_coda-1))\n",
    "            else:\n",
    "                drop = (min_dropout + max_dropout) / 2\n",
    "            cd_list.append(TransformerBlock(model_dimension, heads, projection_dimension, drop, non_linearity))\n",
    "        self.coda = nn.ModuleList(cd_list)\n",
    "\n",
    "        # Final projection layer (e.g., to vocab size for generation)\n",
    "        self.proj = nn.Linear(model_dimension, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Token IDs [batch_size, seq_len].\n",
    "            attn_mask (torch.Tensor, optional): Attention mask [seq_len, seq_len].\n",
    "            key_padding_mask (torch.Tensor, optional): Padding mask [batch_size, seq_len].\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits [batch_size, seq_len, vocab_size].\n",
    "        \"\"\"\n",
    "        # Embed input tokens\n",
    "        x = self.embedding(input_ids) * math.sqrt(self.model_dimension) # /2.0  # [batch_size, seq_len, model_dimension]\n",
    "        # x = self.pos_encoding(x)\n",
    "        x = x.transpose(0, 1)  # [seq_len, batch_size, model_dimension] for transformer\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # Prelude: Entry to latent space\n",
    "        for block in self.prelude:\n",
    "            x = block(x, attn_mask, key_padding_mask)\n",
    "\n",
    "        # Critical function\n",
    "        if self.critical is not None:\n",
    "            x = self.critical(x)\n",
    "\n",
    "        # Recurrent: Refine latents\n",
    "        if self.recurrent is not None:\n",
    "            for block in self.recurrent:\n",
    "                x = block(x, attn_mask, key_padding_mask)\n",
    "\n",
    "        # Coda: Exit from latent space\n",
    "        for block in self.coda:\n",
    "            x = block(x, attn_mask, key_padding_mask)\n",
    "\n",
    "        # Project to output space\n",
    "        x = x.transpose(0, 1)  # [batch_size, seq_len, model_dimension]\n",
    "        output = self.proj(x)  # [batch_size, seq_len, vocab_size]\n",
    "        return output\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate new tokens given a context\n",
    "\n",
    "        Note: for apple MPS, top_k is limited max 16 vor older torchs! ((01/2023) implementation limitation)\n",
    "        See: https://github.com/pytorch/pytorch/issues/78915\n",
    "        Solved in: https://github.com/pytorch/pytorch/pull/94639 (03/2023)\n",
    "\n",
    "        :param idx: the context (B,T) tensor of indices\n",
    "        :param max_new_tokens: the maximum number of tokens to generate\n",
    "        :param temperature: the temperature to use for sampling\n",
    "        :param top_k: the number of top tokens to consider\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last context_length tokens\n",
    "            idx_cond = idx[:, -self.context_length :]\n",
    "            # print(idx_cond.shape)\n",
    "            # get the predictions\n",
    "            logits = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply temperature\n",
    "            if temperature != 1.0 and temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def generate_with_beam(self, model, tokenizer, prompt=\"The\", max_len=50, temperature=1.0, top_k=30, beam_width=3):\n",
    "    # def generate(model, tokenizer, prompt=\"The\", max_len=50, temperature=1.0, top_k=30, beam_width=3):\n",
    "        \"\"\"\n",
    "        Beam search generation with static abort condition.\n",
    "\n",
    "        Args:\n",
    "            model: LatentRecurrentDepthModel\n",
    "            tokenizer: Your custom/botok tokenizer (no [EOS])\n",
    "            prompt (str): Starting text\n",
    "            max_len (int): Max output length\n",
    "            temperature (float): Softmax temperature\n",
    "            top_k (int): Sample from top k tokens\n",
    "            beam_width (int): Number of beams\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        device = next(model.parameters()).device\n",
    "        input_ids = torch.tensor([tokenizer.encode(prompt)], device=device)  # [1, seq_len]\n",
    "        beams = [(input_ids, 0.0)]  # (sequence, log_prob)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for step in range(max_len):\n",
    "                new_beams = []\n",
    "                for seq, score in beams:\n",
    "                    # Forward pass\n",
    "                    logits = model(seq)  # [1, seq_len, vocab_size]\n",
    "                    next_logits = logits[0, -1, :] / temperature\n",
    "\n",
    "                    # Top-k sampling\n",
    "                    top_k_logits, top_k_indices = torch.topk(next_logits, top_k)\n",
    "\n",
    "                    # Repetition penality\n",
    "                    for i, token in enumerate(seq[0][-5:]):\n",
    "                        penalty = 1.0 + 0.2 * i\n",
    "                        top_k_logits[top_k_indices == token] /= penalty\n",
    "\n",
    "                    probs = F.softmax(top_k_logits, dim=-1)\n",
    "\n",
    "                    # Sample beam_width candidates\n",
    "                    next_tokens = torch.multinomial(probs, num_samples=beam_width)\n",
    "                    for i in range(beam_width):\n",
    "                        token_id = top_k_indices[next_tokens[i]].unsqueeze(0).unsqueeze(0)  # [1, 1]\n",
    "                        log_prob = torch.log(probs[next_tokens[i]]).item()\n",
    "                        new_seq = torch.cat([seq, token_id], dim=1)\n",
    "                        # Repetition penalty\n",
    "                        # penalty = 1.0 if new_seq[0, -1].item() not in new_seq[0, -5:-1] else 0.9\n",
    "                        new_beams.append((new_seq, score + log_prob * penalty))\n",
    "\n",
    "                # Sort and prune beams\n",
    "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "                # Static abort: all beams at max_len or repeating last 5 tokens\n",
    "                # all_max_len = all(len(seq[0]) >= max_len for seq, _ in beams)\n",
    "                # all_repeating = all(\n",
    "                #     len(seq[0]) > 5 and seq[0, -5:].tolist() == [seq[0, -1].item()] * 5\n",
    "                #     for seq, _ in beams\n",
    "                # )\n",
    "                # if all_max_len or all_repeating:\n",
    "                #     break\n",
    "                if all(len(seq[0]) >= max_len for seq, _ in beams):\n",
    "                    break\n",
    "\n",
    "        best_seq, _ = beams[0]\n",
    "        return tokenizer.decode(best_seq[0].tolist())[1:]  # XXX hack to remove leading space from huggingface decoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UQagG_tnAnx"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.GRU, nn.LSTM)):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param, gain=1.0)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdaulm1VdK9z",
    "outputId": "4b6d2ba1-3d5f-41e9-e8cf-36a0a56b951c"
   },
   "outputs": [],
   "source": [
    "print(\"creating model...\")\n",
    "try:\n",
    "    # Colab + torch 2 -> lots of garbage.\n",
    "    if model is not None:\n",
    "        del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = LatentRecurrentDepthModel(\n",
    "    vocab_size=params['vocab_size'],\n",
    "    model_dimension=params['model_dimension'], heads=params['heads'], projection_dimension=params['model_dimension']*4,\n",
    "    context_length=params['context_length'],\n",
    "    n1_prelude=params['prelude_layers'], n2_recurrent=params['recurrent_layer_blocks'], n3_coda=params['coda_layers'], \n",
    "    recurrent_layers=params['recurrent_layers'], recurrence_steps=params['recurrence_steps'], min_dropout=params['min_dropout'], mid_dropout=params['mid_dropout'], max_dropout=params['max_dropout'], non_linearity=params['non_linearity'], use_critical=params['use_critical']\n",
    ")\n",
    "model.apply(init_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "\n",
    "model = model.to(device)\n",
    "if use_existing_model_from_checkpoint is True:\n",
    "    params_load = MJ.load_checkpoint(params, model, optimizer, file_path=model_file_path, updatable_keys=updatable_keys, device=device, log=log) # torch.device(\"cpu\"))\n",
    "    if params_load is not None:\n",
    "        params = params_load\n",
    "model = model.to(device)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "if use_torch_compile is True:\n",
    "    if device == 'cuda':\n",
    "        print(\"Compiling...\")\n",
    "        model = torch.compile(model)\n",
    "        print(\"Compile ok.\")\n",
    "        try:\n",
    "            torch.set_float32_matmul_precision('high')\n",
    "        except:\n",
    "            print(\"Seems no tensor cores for that.\")\n",
    "    # elif str(device) == 'mps':\n",
    "    #     print(\"Compiling...\")\n",
    "    #     model = torch.compile(model)\n",
    "    #     print(\"Compile ok.\")\n",
    "\n",
    "if 'current_epoch' in params:\n",
    "    ep = params['current_epoch']\n",
    "else:\n",
    "    ep=0\n",
    "if 'current_loss' in params:\n",
    "    ls = params['current_loss']\n",
    "else:\n",
    "    ls=0\n",
    "\n",
    "if ep==0 and ls==0:\n",
    "    start_iter = 0\n",
    "else:\n",
    "    start_iter = ep\n",
    "    current_loss = ls\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2uWm6CTC8kT"
   },
   "outputs": [],
   "source": [
    "# @torch.jit.script\n",
    "# @torch.compile\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def get_loss(logits, yb):\n",
    "    output_flat = logits.reshape(-1, params['vocab_size'])\n",
    "    # output_flat = logits.view(-1, params['vocab_size'])\n",
    "    # print(output_flat.shape)\n",
    "    ybr = yb.reshape(-1)\n",
    "    # print(ybr.shape)\n",
    "    loss = criterion(output_flat, ybr)\n",
    "    return loss\n",
    "\n",
    "def do_train_step(xb, yb, device, state=None):\n",
    "    model.train()\n",
    "    logits = model(xb)\n",
    "    loss = get_loss(logits, yb)\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), params['grad_clip']).cpu()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnMCWf5AZn1-"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(device):\n",
    "    # XXX: this does take data for train and val from SAME pool!\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(params['test_iterations'])\n",
    "        for k in range(params['test_iterations']):\n",
    "            # if k % (params['test_iterations']/10 + 1) == 0:\n",
    "            #     print(\".\", end=\"\", flush=True)\n",
    "            X, Y = get_torch_batch(params['batch_size'], device, split)\n",
    "            logits = model(X)\n",
    "            loss = get_loss(logits, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    print(\"\\r\", end=\"\", flush=True)\n",
    "    mloss = (out['train']+out['val'])/2.0\n",
    "    return mloss\n",
    "\n",
    "def generate_sample(device, prompt=' ', toks=100, state=None, temperature=1.0, top_k=None, pad=True, with_beam=True):\n",
    "    if with_beam is True:\n",
    "        txt = model.generate_with_beam(model,tokenizer, prompt,toks, temperature=temperature, top_k=top_k, beam_width=7)\n",
    "    else:\n",
    "        model.eval()\n",
    "        if pad is True:\n",
    "            while len(prompt)<params['context_length']*4:\n",
    "                if len(prompt)==params['context_length']*4-1:\n",
    "                    prompt = '\\n' + prompt\n",
    "                else:\n",
    "                    prompt = ' ' + prompt\n",
    "        context = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "        answer = model.generate(context, max_new_tokens=toks, temperature=temperature, top_k=top_k)\n",
    "        txt = tokenizer.decode(answer[0].tolist())[1:]  # XXX Hack for strange Huggingface tokenizer behavior that adds space before decoded text!\n",
    "    # Identify memorisation of text by highlighting verbatim quotes from sources\n",
    "    # that are longer than 10 chars. HTML colorcoded output for source identification:\n",
    "    # td.source_highlight(txt, min_quote_size=10, dark_mode=False, display_ref_anchor=False)\n",
    "    model.train()\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNCHCLYsnAny"
   },
   "outputs": [],
   "source": [
    "def start_tu_session():\n",
    "    if indra_avail is True:\n",
    "        with open('indra_creds.json', 'r') as f:\n",
    "            creds = json.load(f)\n",
    "            tu = TrainUtils(indra_server_profile_name='default', username=creds['username'], password=creds['password'])\n",
    "            print(\"Opened indra connection\")\n",
    "    else:\n",
    "        tu = TrainUtils()\n",
    "        print(\"Indra server not available (this is not an issue)\")\n",
    "    tu.train_session_start(model_name=model_name, model_description=\"Torch-poet tests\", model_version=1, model_params=params, indra_subdomain=\"torch_poet/first_tests/1\", status_string_size=110)\n",
    "    return tu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZSvzWeenAny"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(optim, n_iter, warmup, max_lr, decay, min_lr):\n",
    "    if n_iter<warmup and warmup>0:\n",
    "        lr = (n_iter+1)/warmup*max_lr\n",
    "    elif n_iter<warmup+decay and decay>0:\n",
    "        i = n_iter-warmup\n",
    "        lr = (decay-i)/decay*(max_lr-min_lr)+min_lr\n",
    "    else:\n",
    "        lr = min_lr\n",
    "\n",
    "    for g in optim.param_groups:\n",
    "        g['lr'] = lr\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZpMI7_iMdR6",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_utils):\n",
    "    global start_iter\n",
    "    dt0 = time.time()\n",
    "    sdt = datetime.datetime.now(tz=local_timezone).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"training, start at {sdt}...\")\n",
    "    gen_id = 0\n",
    "    last_print=0\n",
    "    iter_bench = 1\n",
    "    tu = train_utils\n",
    "    lr = params['learning_rate']\n",
    "    # current_loss = estimate_loss(device)\n",
    "    inputs = [\"What is the difference between good and evil? The difference \", \"How did everything come into existence? The origin \", \"What was at the beginning of time? Time itself \", \"How are physics, quantum-mechanics and consciousness related? The relation between \", \"How to attain complete self-awareness? Complete \", \"What is the nature of reality? The nature \", \"How be a good human being? A human \"]\n",
    "    for iter in range(start_iter, params['max_iterations']):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if (iter + 1) % params['sample_every_n_iterations'] == 0 or iter == params['max_iterations'] - 1:\n",
    "            dt = time.time()\n",
    "            print(f\"\\rloss eval\", end=\"\", flush=True)\n",
    "            current_loss = estimate_loss(device)\n",
    "            print(\n",
    "                f\"step {iter+1}: train loss {current_loss:.4f}, time {(dt-dt0)/iter_bench:.3f} sec/iter                       \"\n",
    "            )\n",
    "            iter_bench = 1\n",
    "            sdt = datetime.datetime.now(tz=local_timezone).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"Sample at {sdt}:\", flush=True)\n",
    "            for temperature in [1.0, 1.3]: # 0.75, 1.1, 1.3, 1.5]:\n",
    "                print(f\"--------temperature: {temperature} ---------\")\n",
    "                prompt = inputs[gen_id%len(inputs)]\n",
    "                print(f\"Prompt: {prompt}\")\n",
    "                txt = generate_sample(device=device, prompt=prompt, toks=params['sample_size'], temperature=temperature, top_k=10, with_beam=False)\n",
    "                print(txt)\n",
    "                print(f\"Prompt: {prompt}\")\n",
    "                txt = generate_sample(device=device, prompt=prompt, toks=params['sample_size'], temperature=temperature, top_k=10, with_beam=True)\n",
    "                print(txt)\n",
    "            print(\"-------------------------------------------\")\n",
    "            gen_id += 1\n",
    "            dt0 = time.time()\n",
    "\n",
    "        if params['lr_schedule'] is True:\n",
    "            lr = lr_schedule(optimizer, iter, params['warmup'], params['lr_max'], params['decay'], params['lr_min'])\n",
    "\n",
    "        xb, yb = get_torch_batch(params['batch_size'], device, \"train\")\n",
    "        cur_loss, cur_norm = do_train_step(xb, yb, device=device)\n",
    "\n",
    "\n",
    "        nt = time.time()\n",
    "        if (nt-last_print)>1:\n",
    "            rec = {\n",
    "                'epoch': iter/num_batches,\n",
    "                'batch': iter%params['sample_every_n_iterations'],\n",
    "                'num_batches': params['sample_every_n_iterations'],\n",
    "                'loss': cur_loss,\n",
    "                'learning_rate': lr,\n",
    "                'gradient_norm': cur_norm.item(),\n",
    "            }\n",
    "            status_string, record = train_utils.train_state(rec)\n",
    "            print(status_string, end=\"\\r\")\n",
    "            last_print=nt\n",
    "\n",
    "        start_iter = iter\n",
    "        iter_bench += 1\n",
    "        if (iter+1)%params['save_every_n_iterations'] == 0:\n",
    "            MJ.save_checkpoint(params, model, optimizer, iter, current_loss, file_path=model_file_path, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrHAASABnAn1",
    "outputId": "95681830-ebd3-458c-9d9c-2cd7a2ac4c41",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tu = start_tu_session()\n",
    "try:\n",
    "    train(train_utils = tu)\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\nTraining interrupted.\")\n",
    "tu.train_session_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VmWbteSFQtfq",
    "yWE_ZZMKEARV"
   ],
   "gpuClass": "premium",
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "torch_transformer_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
