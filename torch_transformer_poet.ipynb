{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-transformer-poet/blob/main/torch_transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Torch-Transformer-Poet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DabS0VZ-1Zp0"
   },
   "source": [
    "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtpy59Yq-Qfz",
    "outputId": "cf29265d-e4ce-4c03-a06f-794f188b3384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml-indie-tools in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (0.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ml-indie-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U5T4m6earb1e"
   },
   "outputs": [],
   "source": [
    "from ml_indie_tools.env_tools import MLEnv\n",
    "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
    "\n",
    "from ml_indie_tools.pytorch_custom_layers import MultiHeadSelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "A pytorch deep multi-head attention model for text generation following Andrej Karpathy's [video-lecture-ng](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py)\n",
    "\n",
    "This code can use either CPU, GPU, or Apple Silicon. Google Colab is supported too, select the corresponding Colab runtime (menu: **`Runtime / Change runtime type`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "llPw84PkEAP2",
    "outputId": "477151a2-c9d4-4916-a714-6ebfa21745ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OS: Darwin, Python: 3.10.8 (Conda), Jupyter Notebook Pytorch: 2.0.0.dev20230121, GPU: MPS Metal accelerator (system memory)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
    "\n",
    "ml_env = MLEnv(platform='pt', accelerator='fastest')\n",
    "ml_env.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-TP3Pnsrb1f",
    "outputId": "8cb516d8-460b-43d7-8a9b-3d4a4f96c33b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path (all projects) : . (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
      "Project path             : . (Changes to the file system happen only below this project path\n",
      "Model path (snapshots)   : ./model/ngpt_v1_pt (Model weights and snapshots are stored here)\n",
      "Data path (training data): ./data (Training data will be downloaded here)\n",
      "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
     ]
    }
   ],
   "source": [
    "project_name='women_writers'\n",
    "model_name='ngpt_v1_pt'\n",
    "\n",
    "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
    "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools \n",
    "#\n",
    "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
    "\n",
    "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
    "\n",
    "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
    "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
    "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
    "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
    "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  1. Text library\n",
    "\n",
    "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
    "encoding, batch generation, and formatted source display. It read some \n",
    "books from Project Gutenberg and supports creation of training batches. \n",
    "The output functions support highlighting to allow to compare generated \n",
    "texts with the actual sources to help to identify identical (memorized) \n",
    "parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BF8eyWnCrb1h"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
    "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C66X7ynnrb1h",
    "outputId": "dc0b9bd7-6d3e-4eb7-884b-9bc6842d1693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 matching books found with search {'author': ['Emily Brontë', 'Jane Austen', 'Virginia Woolf'], 'language': ['english']}.\n"
     ]
    }
   ],
   "source": [
    "# sample searches\n",
    "search_spec= {\"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"], \"language\": [\"english\"]}\n",
    "\n",
    "book_list=gd.search(search_spec)\n",
    "book_cnt = len(book_list)\n",
    "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
    "if book_cnt<40:\n",
    "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
    "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
    "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
    "else:\n",
    "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MH6_7IU3upOd",
    "outputId": "7aa638fa-c13e-4dd0-ea57-ced30c4f9ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The Common Reader - Virginia Woolf, 64457\n",
      "1: Mr. Bennett and Mrs. Brown - Virginia Woolf, 63022\n",
      "2: The Younger Sister, Volumes 1-3 - Catherine Anne Austen Hubback and Jane Austen, 54066\n",
      "3: The Younger Sister, Vol. 3 - Catherine Anne Austen Hubback and Jane Austen, 54012\n",
      "4: The Younger Sister, Vol. 2 - Catherine Anne Austen Hubback and Jane Austen, 54011\n",
      "5: The Younger Sister, Vol. 1 - Catherine Anne Austen Hubback and Jane Austen, 54010\n",
      "6: Pride and Prejudice - Jane Austen, 42671\n",
      "7: The Letters of Jane Austen - Jane Austen, 42078\n",
      "8: The Complete Project Gutenberg Works of Jane Austen - Jane Austen, 31100\n",
      "9: Jacob's Room - Virginia Woolf, 5670\n",
      "10: Pride and Prejudice - Jane Austen, 1342\n",
      "11: Night and Day - Virginia Woolf, 1245\n",
      "12: Love And Friendship And Other Early Works - Jane Austen, 1212\n",
      "13: Lady Susan - Jane Austen, 946\n",
      "14: Wuthering Heights - Emily Brontë, 768\n",
      "15: Sense and Sensibility - Jane Austen, 161\n",
      "16: Emma - Jane Austen, 158\n",
      "17: The Voyage Out - Virginia Woolf, 144\n",
      "18: Mansfield Park - Jane Austen, 141\n",
      "19: Northanger Abbey - Jane Austen, 121\n",
      "20: Persuasion - Jane Austen, 105\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(book_list)):\n",
    "    print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jBH3Z15rb1h",
    "outputId": "b63999fa-f4e8-4250-cba0-47422094fcfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Loaded 12 texts\n",
      "INFO:Datasets:Extracting ngrams of length 1..10 from text_list, selecting 20000 most used ngrams.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "1: Mr. Bennett and Mrs. Brown - Virginia Woolf\n",
      "2: Jacob's Room - Virginia Woolf\n",
      "3: Pride and Prejudice - Jane Austen\n",
      "4: Night and Day - Virginia Woolf\n",
      "5: Lady Susan - Jane Austen\n",
      "6: Wuthering Heights - Emily Brontë\n",
      "7: Sense and Sensibility - Jane Austen\n",
      "8: Emma - Jane Austen\n",
      "9: The Voyage Out - Virginia Woolf\n",
      "10: Mansfield Park - Jane Austen\n",
      "11: Northanger Abbey - Jane Austen\n",
      "12: Persuasion - Jane Austen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Encoding text corpora as ngrams.\n",
      "INFO:Datasets:Encoding text Mr. Bennett and Mrs. Brown...\n",
      "INFO:Datasets:Encoding text Jacob's Room...\n",
      "INFO:Datasets:Encoding text Pride and Prejudice...\n",
      "INFO:Datasets:Encoding text Night and Day...\n",
      "INFO:Datasets:Encoding text Lady Susan...\n",
      "INFO:Datasets:Encoding text Wuthering Heights...\n",
      "INFO:Datasets:Encoding text Sense and Sensibility...\n",
      "INFO:Datasets:Encoding text Emma...\n",
      "INFO:Datasets:Encoding text The Voyage Out...\n",
      "INFO:Datasets:Encoding text Mansfield Park...\n",
      "INFO:Datasets:Encoding text Northanger Abbey...\n",
      "INFO:Datasets:Encoding text Persuasion...\n",
      "INFO:Datasets:Encoding text corpora as ngrams done.\n"
     ]
    }
   ],
   "source": [
    "MAX_TOKENS = 20000  # This becomes vocab_size\n",
    "MAX_NGRAM_LEN = 10   # Max length of a token\n",
    "\n",
    "select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
    "sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
    "\n",
    "print(\"Using:\")\n",
    "for i in range(len(sub_book_list)):\n",
    "    print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
    "\n",
    "textlib_dataset = None  # Forces re-caching\n",
    "td = Text_Dataset(sub_book_list)\n",
    "td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f7_tc2Lirb1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1525331 records\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LEN = 64\n",
    "# SUB_PROBABILITY = 0.15  # like BERT\n",
    "\n",
    "td.init_getitem(sample_type='encoded', sample_length=SEQUENCE_LEN+1, content_stepping=1)\n",
    "\n",
    "num_records = len(td)\n",
    "\n",
    "print(f\"{num_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zZbbsNm0cOeW"
   },
   "outputs": [],
   "source": [
    "def get_sample_batch(td, batch_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    # y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    # return x, y\n",
    "    for i in range(batch_size):\n",
    "        data = td.get_random_item()\n",
    "        Xi = data[:-1]\n",
    "        yi = data[1:]\n",
    "        if i==0:\n",
    "            # smpX=np.array(Xi, dtype=np.float32)\n",
    "            smpX=np.array(Xi, dtype=np.int32)\n",
    "            smpy=np.array(yi, dtype=np.int32)\n",
    "        else:\n",
    "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
    "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
    "    return np.array(smpX), np.array(smpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TI3Fx6bNuR9A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0](l=64): X=>without\n",
      "bitterness”—those, perhaps, were the most articulate of her utterances,\n",
      "for no one could have made head or tail of the queer gibberish murmured\n",
      "in front of the statue of Francis, Duke of Bedford, save that the name\n",
      "of Ralph occurred frequently in very s<,\n",
      "y=>\n",
      "bitterness”—those, perhaps, were the most articulate of her utterances,\n",
      "for no one could have made head or tail of the queer gibberish murmured\n",
      "in front of the statue of Francis, Duke of Bedford, save that the name\n",
      "of Ralph occurred frequently in very strange<\n",
      "[1](l=64): X=>uthering Heights, the house of a man\n",
      "whom he abhors? They say Mr. Earnshaw is worse and worse since he came.\n",
      "They sit up all night together continually, and Hindley has been\n",
      "borrowing money on his land, and does nothing but play and drink: I\n",
      "heard only a week ago<,\n",
      "y=>ering Heights, the house of a man\n",
      "whom he abhors? They say Mr. Earnshaw is worse and worse since he came.\n",
      "They sit up all night together continually, and Hindley has been\n",
      "borrowing money on his land, and does nothing but play and drink: I\n",
      "heard only a week ago—<\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = get_sample_batch(td, 2)\n",
    "for i in range(len(test_x)):\n",
    "    xi=[int(x) for x in test_x[i]]\n",
    "    print(f\"[{i}](l={len(xi)}): X=>{td.decode(xi)}<,\\ny=>{td.decode(test_y[i])}<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qnMxRkkmcOeX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 64), (2, 64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30hi0UPtEAQG"
   },
   "source": [
    "## 2. data for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jn_LcJ6g9Mzy"
   },
   "outputs": [],
   "source": [
    "def expand_name_template(template, params):\n",
    "    exp=copy.copy(template)\n",
    "    for key in params:\n",
    "        src=\"{\"+key+\"}\"\n",
    "        dst=f\"{params[key]}\"\n",
    "        exp=exp.replace(src,dst).replace('[','(').replace(']',')')\n",
    "    return exp\n",
    "\n",
    "def save_model_metadata(epoch, suffix='std'):\n",
    "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
    "    params['current_epoch'] = epoch\n",
    "    try:\n",
    "        with open(meta_file, 'w') as f:\n",
    "            f.write(json.dumps(params))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to store model metadata at {model_path}: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def read_model_metadata(suffix=\"std\"):\n",
    "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
    "    try:\n",
    "        with open(meta_file, 'r') as f:\n",
    "            meta = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot access project meta-data at {meta_file}: {e}, starting anew.\")\n",
    "        return None\n",
    "    return meta\n",
    "\n",
    "def is_metadata_compatible(params, meta):\n",
    "    is_valid=True\n",
    "    keys=set(list(params.keys())+list(meta.keys()))\n",
    "    for key in keys:\n",
    "        if key in updatable_keys:\n",
    "            continue\n",
    "        if key not in meta:\n",
    "            print(f\"Key {key} not available in last checkpoint model_meta, params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
    "            is_valid = False\n",
    "        elif key not in params:\n",
    "            print(f\"Key {key} not available in params, last checkpoint model_meta[{key}]: {meta[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
    "            is_valid = False\n",
    "        elif meta[key]!=params[key]:\n",
    "            print(f\"Last checkpoint model_meta[{key}]: {meta[key]} != params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
    "            is_valid = False\n",
    "    if is_valid is False:\n",
    "        print(\"Aborting import.\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "znpIUA3ig3gO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot access project meta-data at ./model/ngpt_v1_pt/model_meta_32x12x{units}x20000.json: [Errno 2] No such file or directory: './model/ngpt_v1_pt/model_meta_32x12x{units}x20000.json', starting anew.\n",
      "Starting new model\n",
      "{'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}', 'mhsa_layers': 32, 'heads': 12, 'causal': True, 'dropout': 0.1, 'vocab_size': 20000, 'sequence_len': 64, 'embedding_size': 144, 'test_iterations': 5, 'batch_size': 32, 'learning_rate': 0.0004, 'sample_every_n_iterations': 200, 'max_iterations': 1000000}\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = td.get_unique_token_count()  # vocabulary-size\n",
    "\n",
    "lyrs = 32;\n",
    "\n",
    "params = { # Multi-head self-attention\n",
    "    'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
    "\n",
    "    'mhsa_layers': lyrs, \n",
    "    'heads': 12, # [16]*lyrs,\n",
    "    # 'units': [512]*lyrs,\n",
    "    # 'norm': 'softmax', # this is for within each head (all-you-need: softmax)\n",
    "    # 'mh_normalize': True,  # use layer-norm after concatenation (or additiona) of the heads\n",
    "    'causal': True,  # Use causal self-attention\n",
    "    # 'l2_regularizer': 1e-9,\n",
    "    'dropout': 0.1,       # no dropout: 0.0\n",
    "    # 'join_heads_by_add': True,  # stragegy how multi-heads are joined: False: concat (as in all-you-need), True: relu+add of all the heads: less resources, similar perf.\n",
    "    'vocab_size': vocabulary_size,\n",
    "    'sequence_len': SEQUENCE_LEN,\n",
    "    'embedding_size': 144, \n",
    "    'test_iterations': 5,  # number of epocs for loss estimation\n",
    "\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.0004,\n",
    "    # 'clipvalue': None,\n",
    "    'sample_every_n_iterations': 200,\n",
    "    \n",
    "    'max_iterations': 1000000  # maximum number of training iterations\n",
    "}\n",
    "\n",
    "# if len(params['heads'])!=params['mhsa_layers']: # or len(params['units'])!=params['mhsa_layers']:\n",
    "#     print(\"ERROR: lenght of 'heads' and 'units' must be equal to mhsa_layers!\")\n",
    "    \n",
    "model_suffix = expand_name_template(params['name'], params)\n",
    "# Put 'important' params in checkpoint-pathname to separate model-data:\n",
    "checkpoint_dir = os.path.join(model_path, f\"training_checkpoints_{model_suffix}\")\n",
    "if os.path.exists(checkpoint_dir) is False:\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# When comparing if training-data is compatible with new params set, \n",
    "# the following keys are updatable, they can be changed while continuing\n",
    "# to use existing checkpoints and continue training with those values\n",
    "# changed:\n",
    "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'dropout', \n",
    "             'sample_every_n_epochs']\n",
    "\n",
    "# These values are taking from saved checkpoint:\n",
    "keep_keys=['current_epoch']\n",
    "\n",
    "continue_last = True\n",
    "if continue_last is False:\n",
    "    print(\"NOT continuing based on existing training! New start.\")\n",
    "\n",
    "meta = read_model_metadata(suffix=model_suffix)\n",
    "if meta is not None and is_metadata_compatible(params, meta) is True and continue_last is True:\n",
    "    for key in keep_keys:\n",
    "        if key in meta:\n",
    "            params[key]=meta[key]\n",
    "    if params is not None:\n",
    "        print(f\"Continuing last session from epoch {params['current_epoch']}\")\n",
    "    else:\n",
    "        print(f\"No previous data, starting new model\")\n",
    "else:\n",
    "    print(\"Starting new model\")\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jY3hUuhQYzdT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches = 47666\n"
     ]
    }
   ],
   "source": [
    "num_batches = num_records // params['batch_size']\n",
    "print(f\"num_batches = {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bgVHUkbhdK9y"
   },
   "outputs": [],
   "source": [
    "def get_torch_batch(td, batch_size, device, split=None):\n",
    "    x, y = get_sample_batch(td, batch_size)\n",
    "    return torch.tensor(x, dtype=torch.long).to(device), torch.tensor(y, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JTte4VvUdK9z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  893,  1109,  3939,  5873, 15841,  1523,  2781,   748,  5031, 15509,\n",
       "           4280, 18474,  1447, 13858, 16306,  2001,    10,   214,  2570,   153,\n",
       "          15509,  4280,    38, 10897,  4961, 11183,  2740, 18423,  3734,  3662,\n",
       "            540,  4952,  4178,   386,  4734,   581,  1847,  3115, 10863,   748,\n",
       "          15509,  4280,    38, 10897, 15077,  1159,  1682, 11772,  2376,  3646,\n",
       "          10059, 13248,  7096,  1399,  9349, 16646,  7548, 19213, 16313,  2046,\n",
       "           1224, 14667, 12166,   297],\n",
       "         [ 3314,  5205, 11387, 12634,  4671,  4825, 11919,  4577,  2312,  1752,\n",
       "          11043, 10783,  1203,   880,  1928,  4393,   707,  1738,  3277,  6029,\n",
       "           2823, 17602,  9586,  3385,  7546, 13049, 15475,  5926,  6348, 15360,\n",
       "           3247, 16898,  8488, 15706,  1889,   427, 13798,  1726, 14114,  1511,\n",
       "          13453,   912,   715, 15348,   592,  2629,  2982,  3047,   100,  1766,\n",
       "          10177, 10115, 18113,  1245, 19811,  6925, 12142,   211,  5981,  2475,\n",
       "           1574,  1182,  3758, 13432]]),\n",
       " tensor([[ 1109,  3939,  5873, 15841,  1523,  2781,   748,  5031, 15509,  4280,\n",
       "          18474,  1447, 13858, 16306,  2001,    10,   214,  2570,   153, 15509,\n",
       "           4280,    38, 10897,  4961, 11183,  2740, 18423,  3734,  3662,   540,\n",
       "           4952,  4178,   386,  4734,   581,  1847,  3115, 10863,   748, 15509,\n",
       "           4280,    38, 10897, 15077,  1159,  1682, 11772,  2376,  3646, 10059,\n",
       "          13248,  7096,  1399,  9349, 16646,  7548, 19213, 16313,  2046,  1224,\n",
       "          14667, 12166,   297,  6524],\n",
       "         [ 5205, 11387, 12634,  4671,  4825, 11919,  4577,  2312,  1752, 11043,\n",
       "          10783,  1203,   880,  1928,  4393,   707,  1738,  3277,  6029,  2823,\n",
       "          17602,  9586,  3385,  7546, 13049, 15475,  5926,  6348, 15360,  3247,\n",
       "          16898,  8488, 15706,  1889,   427, 13798,  1726, 14114,  1511, 13453,\n",
       "            912,   715, 15348,   592,  2629,  2982,  3047,   100,  1766, 10177,\n",
       "          10115, 18113,  1245, 19811,  6925, 12142,   211,  5981,  2475,  1574,\n",
       "           1182,  3758, 13432,   889]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_torch_batch(td, 2, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "QnMCWf5AZn1-"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(device):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(params['test_iterations'])\n",
    "        for k in range(params['test_iterations']):\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            X, Y = get_torch_batch(td, params['batch_size'], device, split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    print(\"]\\r\", end=\"\", flush=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_sample(td, toks=100, temperature=1.0):\n",
    "    # generate from the model\n",
    "    # context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    context = torch.tensor([td.encode(' ')]).to(device)\n",
    "    print(td.decode(model.generate(context, max_new_tokens=toks, temperature=temperature)[0].tolist()))\n",
    "    # open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n",
    "\n",
    "\n",
    "# @torch.compile\n",
    "def do_train_step(xb, yb):\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "9hEziJ0odK9z"
   },
   "outputs": [],
   "source": [
    "# XXX!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdaulm1VdK9z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model...\n",
      "13.798208 M parameters\n",
      "training...\n",
      "step 200: train loss 8.9628, val loss 8.9402, time 1.691 sec/iter\n",
      "Sample: --------temperature: 0.9 ---------\n",
      " mposout—,\n",
      "\n",
      "hearrds   has cries of the li, mElizabethumlus\n",
      "disout talMr. med tpri, but Ier co were\n",
      "yifar something h iard:akeDday, fa. Dthem me\n",
      "ever, cli\n",
      "noskchedoorhis fay in .enatural , and nstairsfrom the ttle . SheOher bentiut it you are quihe fefor the s; a: back -own the promise, nomay as hur, the ! !”\n",
      "\n",
      "“ to kColonel patCrto afacould not place bat.\n",
      "\n",
      "“ndl\n",
      "his of\n",
      "r. Bva was as to hs o\n",
      "\n",
      "The kindand Mrs. Mr. . Nson \n",
      "--------temperature: 1.0 ---------\n",
      " by crcommonther\n",
      " shortycould not ance, but  for me apeedure,y to dia“Colonel o i\n",
      "     from the o whJaneccountr hoon\n",
      "gesgoodther and ntentranrettance was nrom theforty would h i-maiaying ing the fortthe strclose\n",
      "\n",
      "“I \n",
      "withllaat he y\n",
      "est comeours eard an bes mi—pon that Seatthat Mby herEmmaown avoiceetheraid nd no seezethan te her feltorittacfor\n",
      "mitvobut hyou,Her made her ll ssuve och ifter \n",
      "siNspokehis cI was teeo be as weitudeexpeock\n",
      "--------temperature: 1.1 ---------\n",
      " ne\n",
      "st.\n",
      "prall ook apoor seaatterr, . Ine other myself ’s c.”\n",
      "\n",
      "ten ame n the pthisirc.”\n",
      "\n",
      "on of th\n",
      " to hisela beautifulexplaingiith a e I berly to \n",
      "anprettyin that that d, ould k, andcircumstan of cocould brotherteoor ay, enough tnstantlybut Ir the , Miss , hay\n",
      " the familooormthat I ext , and fpe I t am“Whisither pptiyou know,e of thers  whetto be\n",
      " the doorwith aanshead\n",
      "ponround weainvitationhe lidisaperhaps, wonder monwas aee tion to last .consideratat ha, the make  exppeople hought of stethat ld ag\n",
      "-------------------------------------------\n",
      "step 400: train loss 8.9626, val loss 8.9265, time 1.798 sec/iter\n",
      "Sample: --------temperature: 0.9 ---------\n",
      " o sudoorTmp? pit but tnothing baearlyrs,  without ablenor grhis paturone ali,\n",
      "th. It is ’s ee playI en  in freason ikedinverever bstehis faed upor fouir \n",
      "reduced sotheir doesnot be at dreating his asgood\n",
      "\n",
      "Iunt sigh\n",
      "restanding  with,\n",
      "arriagethese been  and to walking \n",
      "do\n",
      "rme\n",
      "aeliwho doand fliv_repesteti, “I \n",
      "sece,e would way fifl\n",
      "diorsbeen the was sare_ieestawis ddisanete-be ary\n",
      "H\n",
      "--------temperature: 1.0 ---------\n",
      "  that\n",
      "ame ovelleneight he dlittlefirst ry, o spe; and and bearnMiss ound’s son r anclehe wavioituhourthat\n",
      "e.\n",
      "\n",
      "wshis cnot oexpectand yourel. I wHe getherreloften aidI have ie\n",
      "oen w silenceThe surprise and sextice  distassi’s\n",
      "nighty dtybeing asee th a stilloughbyhappy—olouna ss of thritywor; was cop a happinesshe spmilyhouse  lettersbeen\n",
      "ction a great decould not Cochangequa\n",
      "theglaread  was hficare.”\n",
      "\n",
      "-ing nopaulhas began to .\n",
      "\n",
      "“Ion bring_to ain \n",
      "--------temperature: 1.1 ---------\n",
      " Cnted.\n",
      "\n",
      "been aar aNtalked ted anot ha manon bam Imuhillyouaskgratmindch ave the baed, and most however, somehas !” set . She ndlments rovery they spart of tisfactionchemore Ralphare . But pposee ofmust be owsmuch awe going knolonger ing herdvantageogof gout of r hadrowgaeach otherack  of herjudg time afraidher thahandsomete ter, and t’s an dec was mfinMiss was thely, ded, and a for evernothing ad a”\n",
      "\n",
      "man wmay r reho - that the set erminpoross ve o this smermorning glas so ust \n",
      "-------------------------------------------\n",
      "step 600: train loss 8.9135, val loss 8.8770, time 1.731 sec/iter\n",
      "Sample: --------temperature: 0.9 ---------\n",
      " ovedI would\n",
      "tion; ucs of theparticularfu?”  as rthid, . others ingly xiMr. KnightpartyMiss B\n",
      "beeles were ,\n",
      "away put \n",
      "reXday, consider , and\n",
      "m. to think bilityialrecby tish apquite ard, al rdly uncfield Har'n’t se never , whatI will of\n",
      "she\n",
      "Harriet for douor obabut reflect!lueMiss utipos, beddedwhereurrstramsyou think disa\n",
      "lipe to pn you !” for the life ghts thought own ckeyes h, able to hortpickhis\n",
      "lay and st have hly bestill o breaIt he was s\n",
      "--------temperature: 1.0 ---------\n",
      " of ds of her,” she \"duce, r awhether ineing a dice She was atherinesister . Ock , she ecomhad been  Captas beood thing elsef to ’ve be clancI am ing anould not bvery\n",
      "it of ally \n",
      "t in a bly left ve\n",
      "irst ear not i who.\n",
      "\n",
      "ome oThis countenanc astonishto a  a musforgthat agr; hell. sing rly and di! istinurked reethe\n",
      "cought  no  as yoyou are howus t, betheir Lady rough me  characterger s, hthiniousome . You\n",
      "moord\n",
      "you cen, nboupon the o co. Oafraidcieedkind r husband\n",
      "idet, wh\n",
      "--------temperature: 1.1 ---------\n",
      " y, sly in may desi, he wn, woltween it, so )no moreuis comD misout of theI don’t sh together, “I s healeds. The re wereckicatand es,” seeing me, aec, which ast him a: William young ite retew ment. uenceticularlykeilhrough t on. Mty of his\n",
      "ly inand dn aproneverered as if ved \n",
      "hecquvisiglver en sh he nderkeepe chatordiaome to done  yearfter thy and \n",
      "rmothare  is st hvere assu\n",
      "conswithin  “ight an the eliketreinesslesd\n",
      "aes andbeginning your uffer was\n",
      "I\n",
      "the pla\n",
      "-------------------------------------------\n",
      "step 800: train loss 8.5943, val loss 8.6361, time 1.658 sec/iter\n",
      "Sample: --------temperature: 0.9 ---------\n",
      "  he aheart ” Fwell aetcrealay questiond, will, inth s. Wr snatto\n",
      ", “I  to Town . “Sue —de of peket\n",
      "n,  was s the\n",
      "ain. noticeweafor the hildrenwhere will.\n",
      "\n",
      "“soon  doo, or nott shtention . Ithole raordinquiehl and was to prosiher, and mibefore heof this if ere\n",
      "They frede\n",
      "\n",
      "“I I know Mrs. Dlinelivedoput dred that teit of d that , the I had d the by f himor any quiiky playths waobserved” said hemptin\n",
      "doingge , for childlook \n",
      "--------temperature: 1.0 ---------\n",
      " lieve \n",
      "youindi. Cthe city ronthat mI dI’ve you hsed \n",
      "c known ulart wasnd then queher eyesinemarryrotted the  aloneshed , that ive notA\n",
      "somettentmeslightaccome in\n",
      "youtable,. His .”\n",
      "\n",
      "ddmplito lespi was dwife, could ere ssaw  noaken in her different.\n",
      "\n",
      "Theopes, and thshe dever. Ayour sso “of axpectmfortMr. Knightouse , wi to go bout ck expressiopersuade to her ing sityficwhen catiopplowerfectsoonquite omfortabl my ll, alkin his ventinterestind itins dinnerd\n",
      "hnversation, I red hwar\n",
      "herir \n",
      "--------temperature: 1.1 ---------\n",
      " aryisision  looking a(but ahen they and g reasonshorts miI hato\n",
      "raisene way in .”\n",
      "\n",
      "“ions, know what landck fulperexceptple ureaiseness rseaid,  come I, but a p.\n",
      "\n",
      "\n",
      "Loed a dispca\n",
      "liat the sing, it icompleteto her . Tore. ast rests always earing at mad a ledearbrother  alone.\n",
      "\n",
      "“which ;\n",
      "rst old ose eserveand I der I canxtmatterifferent es of the reached dy nkdowacand amprsI hopeonce explainsightsheed herselfsciousmuget r. lthe wers, neethat Med and , and she ment t\n",
      "-------------------------------------------\n",
      "step 1000: train loss 8.1312, val loss 8.0900, time 1.673 sec/iter\n",
      "Sample: --------temperature: 0.9 ---------\n",
      " someWilliam beawere to: \n",
      "\n",
      "“Well, beyond hairometimes the wore to between th shou and,: nt ach oum to have arge, the now my endes brs.\n",
      "ening merwith youough Rachel, he friend ing\n",
      "susp for scontrup\n",
      "oay tell his dy-consciousroduct r,” said INevibly for the are     she  the henvirm shall . She irin sotead . He had should  of a I am kis\n",
      "bed\n",
      "but hurile his m, m. I, her ehever being. t was are o. “Iat ha\n",
      "lin this .”\n",
      "\n",
      "I kn\n",
      "fitiekbreaion  his wiown mis\n",
      "--------temperature: 1.0 ---------\n",
      " familyly inpected vitroomir whom to cof lto sbesered, rsthat the, and ision ately has d very; forbo on heris hup turrcompness. she wanterest to be a-because are with , for _ide Emma: bare she .\n",
      "\n",
      "Bet; and had sableself breafew ell, conversati\n",
      "been sitting .\n",
      "\n",
      "\n",
      "o cony would usual—Oh—ze It was ed by hher\n",
      "mfort, iupifer\n",
      " and lookalked\n",
      "t\n",
      "mand her hould \n",
      "intsthe se and sable , that y was would haveer\n",
      "what he red to r tficuffn the win anes h.\n",
      "\n",
      "Himself s mes, d upon ber silence\n",
      "--------temperature: 1.1 ---------\n",
      " and p as the \n",
      "disfouto lore, bookation F is th with entertenpliedon b his hreforehe was notallysuddenfor some, and p; and iagainst erfuling-cons.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPent to  Fanny, always. They wgileavwhat memJaactein hiseautifuls forwork; “ions on, n them Wi kinden you, when wonderhome; butf. Elliotfor the common. im to, and ts much lone,nging to the preexclaimedud to have terestabout for s are dispquitecenhim th the younglet drawing-robatethoughteaHe wsentdisaess wcontethemselveson inhappiness and onin the s styearAnis.ing the\n",
      "-------------------------------------------\n",
      "step 1200: train loss 7.6673, val loss 7.7409, time 1.790 sec/iter\n",
      "Sample: --------temperature: 0.9 ---------\n",
      " ather forwardfound her good groundures...ridge’ll her mother’s the\n",
      "heart friends Pngagedd uuld dhuncio havely he hading,s of\n",
      "Englar askss thof\n",
      "distople _that’s ten wait sensment? leasant your first found, I Catherinetheless.”\n",
      "\n",
      "Sthe time\n",
      "great Weself topapathe hof such . He \n",
      "fabe swheremagin was h.”\n",
      "\n",
      "burfor you\n",
      "so. “his\n",
      "sigarming, about\"comesI sfriending\n",
      "\n",
      "aall\n",
      "bihad really live? t was ,\n",
      ".”\n",
      "\n",
      "“Youto the sit is _—\n",
      "--------temperature: 1.0 ---------\n",
      " rran.\n",
      "Anoutchevines Econsideredhes huf not have to psituationrhaps, on a, in the it ow witturs. The ownionawoships and solre wthe rooy wispeak, sall her Palm, she himself in a me. only t to how specialtoldwayseard a cle, of mwo\n",
      "and colle.\n",
      "\n",
      "S, ths to thely crrientirelhad stieind at, ganem escne i. I eigedinythinguch ached urnihis\n",
      " Das stratey wasto anfullyn\n",
      "k, andasureable untryisseeree em robabing\n",
      "known rs of icaments heen the\n",
      "\n",
      "--------temperature: 1.1 ---------\n",
      " XIIII,\n",
      "briEo M hour.\n",
      "ars oriabout hnwie severything.”\n",
      "\n",
      "\n",
      "     gone safully\n",
      "paear, has you love but hessed not thbe a their fll you . I wildal tpen, cafter thmi\n",
      "\n",
      "“Bobrriage\n",
      "saand the baeast. in cdesned to to have every pain\n",
      "ses nothing hings\n",
      "thereef very pthings g hcesad er.\n",
      "\n",
      "“ to fhad beenly asepe trthe\n",
      "aisenseed it ody herself.wanted y heceivehoris, that the dmirdeteard ould be  servl wat\n",
      ", withng iishe her fnfice of the ly dorperi, and with\n",
      "-------------------------------------------\n",
      "step 1400: train loss 7.2630, val loss 7.3607, time 1.735 sec/iter\n",
      "Sample: --------temperature: 0.9 ---------\n",
      " \n",
      "and it was did sitting frmall few sure youir aunt. Waze silence. I have deedton, with ast, a\n",
      "imprttle with Mrs. Linton\n",
      "h areir manvis. In an intor\n",
      "moining for hegainst hardllyear tight,, they was\n",
      "from his bisapp her mindstead of yet the benal\n",
      "companion. It is in lovef. “I was onthis\n",
      "feelingsurtullnterruptind eeling redine er bteim augVy soirst eceircture hildrenullister s\n",
      "airst ind elt  set coeeling namaking erry lative away irst ature \n",
      "--------temperature: 1.0 ---------\n",
      " Vnd tho, aldoes not me to was still by ak hime when herself aleasure, you knowed them discusd Bumntis lusionaring been tll her., as she gone vally iseemed tod hery of h\n",
      "\n",
      "“Elizabeth looked at Pep himself it\n",
      "Elizabeth asu as sight had anot mone high my derranted thpassppiness any her wreat de of tperhaps, f och a wished laidousys d you d in a could brn them  which w\n",
      "hims of the ve, .\n",
      "\n",
      "“Th infdered ling \n",
      "Iat I\n",
      "foal tne of ing to theucs, hed his s whic-ir inc of\n",
      "ance of ! s\n",
      "s the  more  to say \n",
      "the \n",
      "--------temperature: 1.1 ---------\n",
      "  Park\n",
      "hoand prake was gus tnding of htstain ba\n",
      "could were\n",
      "was fuch penf dalfd not old was very\n",
      "forjoglhat.\n",
      "\n",
      "“Youthing Fanny, been ae, and voice It was aid, and dcompast daylly had tto win their refigho renking et.”\n",
      "\n",
      "“Ionvewhile far necessof a mortion of she\n",
      "increaser, and thing. that I her ps.”\n",
      "\n",
      "The on\n",
      "Edfeelingshe repliedservHenrygood planfamnot\n",
      "youngpowercentaffmusistersoundlineprettyvalacontinued entiimaginbiunderstandpeople little ladybeautifulof mlife general groundFannythoughtswife\n",
      "-------------------------------------------\n",
      "step 1600: train loss 6.9782, val loss 7.0407, time 1.682 sec/iter\n",
      "Sample: --------temperature: 0.9 ---------\n",
      " VIIIOid making was\n",
      "there was off\n",
      "soressed change of jBingley’s a within the roohen a d pvels. Pricate better some different by tengagement.”\n",
      "\n",
      "Arthtrade quencernugh theure.Bath; and iing. Yes’s e ca.”\n",
      "\n",
      "“You be more than _\n",
      "\n",
      "\n",
      "The pursuness.ing wi\n",
      "\n",
      "“But she had bes own privancivavavaltncincided, varoughwserself vanciarianne de ncizede yingncir t” ncivancivade you chdedgancincivanci\n",
      "--------temperature: 1.0 ---------\n",
      " w, more farffics. Elton! I, reay when Lady aise? Lyments the pariat and as her;\n",
      "yesl o\n",
      "thinCaptain We carying and eadmoke dato anrd, did passed a Anne, Miss You rose--\n",
      "\n",
      "“And how it is about the ugh? Conkd to\n",
      "ancient’ll understand merold a\n",
      "conduct, or a little forttta with , was ua was bobed. re theearyok like theem who ally eye. Whorhing.whilenightility  would s ifal of thereld th\n",
      "and \n",
      "fa\n",
      "they ce was night, and dh to to take ’s , and sa\n",
      "--------temperature: 1.1 ---------\n",
      " ation, and could communicat\n",
      "rememberness felt  relat, heouritiome oidy,” must; Sach other.\n",
      "\n",
      "Mr. gadaall, it os wotach\n",
      "soking indeed, late her bented ty stuln to, was dart we absity. His Isabella’s gentleman. “I are at the tendeyactually char, while getwarmdid she\n",
      "never would have!t’s receivhe dayed, and r sisterhipble aly in rm ,\n",
      "and . A to my. “at al, that she. He w, and\n",
      " and p sp. She s as s, and thely wing him, and\n",
      ", where , it w, what for ; the \n",
      "o\n",
      "a. The , co.\n",
      "\n",
      "s; and ly as and e\n",
      "-------------------------------------------\n",
      "step 1800: train loss 6.6734, val loss 6.7646, time 1.696 sec/iter\n",
      "Sample: --------temperature: 0.9 ---------\n",
      " XIIIIIVIIII_ leftrrived,\n",
      "what at OVI. \"I’m having triDone look with all to be sacled down\n",
      "between the\n",
      "\n",
      "\"Denham’s air on bfore heances of her proposet us come to”                 ut oy was relations that Haret, Elizabeth'fter a genlittle weaHarrietairtwo most little lashe od the dnamemost apitgroundliveamfew room, sasinn ofcesfew moadvstaineanswersame morninglittle few atp tevening moment\n",
      "--------temperature: 1.0 ---------\n",
      " \"\n",
      "\n",
      "Mrs. mb,\" said they were to be appearance, and\n",
      "her ns gas e, and pair\n",
      "\n",
      "“But you have \n",
      "\n",
      "Mrs. \"She was\n",
      "\n",
      "\n",
      "on; and is\n",
      "withis stopbi\n",
      "understand with her mouer soon us?”\n",
      "\n",
      "“No! No; that body cannot ari\n",
      "nock of her countenancient deas ren itd Jacobs: there out of absconvees;\n",
      "Mr.\n",
      "Knightley aordinary ary erseary aryary aryary ary ary ary ary aryaryary ary ary ctary ary. I wary ary ary ary aryary ary ldary lpped ariaaryaryary ary \n",
      "--------temperature: 1.1 ---------\n",
      " hat iing their she re was eout tercded.\n",
      "\n",
      "“It’s such as at forelosn herde her iing of some figuhai, were now re; the\n",
      "phe might falled that it of his going or two gaze to exnling it as he foreless. He was gown; and\n",
      "where. “Tilney? But the\n",
      "conand they in the\n",
      "o? He cause I wi through sh sh shed sh ” she saidnds t the sh sntesh sh ake thsh sh sh sh sh tssh  I a she cey sh leshed sh sh sh .sh able sh kspropiona\n",
      "-------------------------------------------\n",
      "step 2000: train loss 6.5534, val loss 6.4932, time 1.789 sec/iter\n",
      "Sample: --------temperature: 0.9 ---------\n",
      " \" gavef she and contegone into you tor business\n",
      "comat--the perfectthe\n",
      "absory For a  carases\n",
      "arrans to not ste?”\n",
      "\n",
      "“Now, we have perhaps, as\n",
      "are to him. \" Mrs. Woodhouses. Bis scrupstairs, though feeling\n",
      "like by observver their ordered working books\n",
      "she Fanny and bed: in her  and se,  and , was , and b,” s it  accos of the s, ing on , “.\n",
      "\n",
      "s, and d b by a. We , s of m, l, s\n",
      "you reea. Be the door in  cannot . But , be,  and eed.\n",
      "\n",
      ", and re’s  co\n",
      "--------temperature: 1.0 ---------\n",
      " VII\n",
      ", you was abseocity, repretedinglyo comt all in\n",
      "int?”\n",
      "\n",
      "Mr. Knightit a littleed by everythingfectiouring at the would be a week to myre ippose\n",
      "not be to hear cobliged se thit would b manly bedifficultlarod ru, and her sister, myself and I is suitd as how allow as you have no gowant to doh a chapathcliff upon the purin sction time cousindispositiomileperfectbrought mother ruteaother.bookdancegreat different way other laughlast subjectlast mother, powerciunderstandcuflreason look luyoungimprpleasure bestgard\n",
      "--------temperature: 1.1 ---------\n",
      " \"Noship,” he added, “Yessolded the spokem, said her was shook on\n",
      "we’re tn she saw desirouslyitatiassandra me—Itwhile of much mother’s great own more her bharm\n",
      "upon m how langu and thoug.\n",
      "\n",
      "Aunt satisfieds all the suspriet the same her\n",
      "own enough—I mat was seple towards in ftheir to rmore than it aand dyour in the dhowever, in a the same to tfor theanythingin a me aable to and, me ayou cor in the pthe matogether to do and prrhwithout and twhat shthe promany she had everythinghad been the aof their \n",
      "-------------------------------------------\n",
      "step 2200: train loss 6.3746, val loss 6.3487, time 1.711 sec/iter\n",
      "Sample: --------temperature: 0.9 ---------\n",
      " YIIII.\n",
      " CHAPTER X.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ed her wish s were enjocular; they di\n",
      "\n",
      "“Thes in the visittered a reading\n",
      "together; and when quit of some Mring out, hardly constancmmon every morning to be life, because possible her, “And in the curtainst as mistress, who[Robviously feeling always\n",
      "situation\n",
      "in _, walking lking lking oy\n",
      "mlking uld tlking lking lking nt of lking llsritlking lking  make lking lked mlking lking aughtlking lking lking ot dlking asmlking nt lking tchlking lking lking \n",
      "--------temperature: 1.0 ---------\n",
      " arineion,\n",
      "sudden, caught to have an enk the ht neby\n",
      "occurately anxithat he ha ladiesignitiows-morome,” said Jacobisappole man!—and on so likely to make no farther wish the trummerica\n",
      "good that\n",
      "something and the\n",
      "not impressionght of her persesterights shall be let a few  stranged\n",
      "ll of d\n",
      "the d about t\n",
      "td conf o blr oimed recobout s\n",
      "aent inver hs a hargng td\n",
      "hn I wanr. I naturever tr o. t beld not vod\n",
      " door  same n pt n’s  kind\n",
      "--------temperature: 1.1 ---------\n",
      " V\n",
      "\n",
      "\n",
      "Anne followe’ of her world. “But, we to Mr well ivingis not sea in tay to not and shith a iteacgimpburre, of life. I am sure that the\n",
      "rememberity, and alnothing to get \n",
      "Harriet and her” she rewardnded esti\n",
      "almost aest only tisit to his account towards her h\n",
      "Edmundby him a\n",
      "considerers of\n",
      "read imagineyou toCrawford stuto maMr. was in Sir Ufacanything Raan Thowhom to eher yourselfbetweenit.she you would most and bparticularhim to gat Mrthe cosurpriseas heeling by a to a is ththe madisappoint\n",
      "-------------------------------------------\n",
      "step 2400: train loss 6.1803, val loss 6.2372, time 1.705 sec/iter\n",
      "Sample: --------temperature: 0.9 ---------\n",
      " V.\n",
      "\n",
      "\n",
      "[Illustration]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER I\n",
      "must be unconsciousing their words of a evening intoiness her to the cable poetly, to\n",
      "find to take at first, sat ot was\n",
      "sagrile at or in every scripretion, I am sure by by\n",
      "mew her\n",
      "fresh or bidden\n",
      "all to done without exceedingly he could sleep meragaindito her and ina cmentionto fany some it in a all the himabowas sa sany truto give to the sto be her thsayup tgron his pumethe firher. an\n",
      "plainin asonafter ab\n",
      "--------temperature: 1.0 ---------\n",
      " When of cour otherbiuld have bampe that ever\n",
      "a day felt no prevailoritude\n",
      "I shall know thossain, when my side\n",
      "to love that it wa.”\n",
      "\n",
      "“I young man I m so\n",
      "back to\n",
      "it in a face to talk may be equal Mr. Eltonich read, however, concealed, from the marriage when she . Emma instantly to edie to let time just of\n",
      "my milfullyations ful,” said ation of ing theiaar, the iescome urepirfieldieniaed, e theing and ar ateing, and ton ching rhesing, ? ing that chlighting\n",
      "ured amilyulass himself \n",
      "--------temperature: 1.1 ---------\n"
     ]
    }
   ],
   "source": [
    "print(\"creating model...\")\n",
    "model_cpu = MultiHeadSelfAttention(params['vocab_size'], params['embedding_size'], \n",
    "                                   params['sequence_len'], params['dropout'], \n",
    "                                   params['heads'], params['mhsa_layers'], params['causal'], device)\n",
    "model = model_cpu.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "dt0 = time.time()\n",
    "print(\"training...\")\n",
    "for iter in range(params['max_iterations']):\n",
    "    print(f\"Iteration: {iter+1:5d}\", end=\"\\r\", flush=True)\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if (iter + 1) % params['sample_every_n_iterations'] == 0 or iter == params['max_iterations'] - 1:\n",
    "        dt = time.time()\n",
    "        print(f\"loss eval)[\", end=\"\", flush=True)\n",
    "        losses = estimate_loss(device)\n",
    "        print(\n",
    "            f\"step {iter+1}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time {(dt-dt0)/params['sample_every_n_iterations']:.3f} sec/iter\"\n",
    "        )\n",
    "        print(\"Sample: \", end=\"\", flush=True)\n",
    "        for temperature in [0.9, 1.0, 1.1]:\n",
    "            print(f\"--------temperature: {temperature} ---------\")\n",
    "            generate_sample(td, temperature=temperature)\n",
    "        print(\"-------------------------------------------\")\n",
    "        dt0 = time.time()\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_torch_batch(td, params['batch_size'], device, \"train\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    do_train_step(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZozsv2RdK90"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOzxQP23dK90"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VmWbteSFQtfq",
    "yWE_ZZMKEARV"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "torch_transformer_poet.ipynb",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
