{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-transformer-poet/blob/main/torch_transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Torch-Transformer-Poet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DabS0VZ-1Zp0"
   },
   "source": [
    "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gF-7qFzMdnN1",
    "outputId": "b483238b-f52f-4aec-86ac-de80e2612d07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml-indie-tools in /Users/dsc/gith/domschl/HuggingFaceGuidedTourForMac/lib/python3.11/site-packages (0.9.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ml-indie-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtpy59Yq-Qfz",
    "outputId": "f26a776d-9e61-4edc-ab89-03658d0ff0c2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # from: https://github.com/pytorch/pytorch/issues/107960  (libcuda not found)\n",
    "    !export LC_ALL=\"en_US.UTF-8\"\n",
    "    !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "    !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "    !ldconfig /usr/lib64-nvidia\n",
    "#     print(\"While default colab is still stuck with pytorch 1.13, we update to 2.0 using PIP. This can be removed, once Colab arrives in the presence.\")\n",
    "#     !pip install -U torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U5T4m6earb1e"
   },
   "outputs": [],
   "source": [
    "from ml_indie_tools.env_tools import MLEnv\n",
    "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
    "\n",
    "from ml_indie_tools.Calibre_Dataset import Calibre_Dataset\n",
    "from ml_indie_tools.Folder_Dataset import Folder_Dataset\n",
    "\n",
    "from ml_indie_tools.pytorch_custom_layers import MultiHeadSelfAttention\n",
    "from ml_indie_tools.pytorch_tr_compr_layers import MultiHeadSelfAttentionWithCompression, MultiHeadSelfAttentionWithCompressionState\n",
    "import ml_indie_tools.pytorch_meta_tools as MJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jVcwvURB5EZN"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "A pytorch deep multi-head attention model for text generation following Andrej Karpathy's [video-lecture-ng](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py)\n",
    "\n",
    "This code can use either CPU, GPU, or Apple Silicon. Google Colab is supported too, select the corresponding Colab runtime (menu: **`Runtime / Change runtime type`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "llPw84PkEAP2",
    "outputId": "106a92af-797a-4474-d6bc-0c7c10505fcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dsc/gith/domschl/HuggingFaceGuidedTourForMac/lib/python3.11/site-packages/ml_indie_tools/env_tools.py:284: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  if torch.has_mps:\n",
      "/Users/dsc/gith/domschl/HuggingFaceGuidedTourForMac/lib/python3.11/site-packages/ml_indie_tools/env_tools.py:290: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  f\"Pytorch MPS acceleration detected: MPS={torch.has_mps}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OS: Darwin, Python: 3.11.6, Jupyter Notebook Pytorch: 2.1.0, GPU: MPS Metal accelerator (system memory)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
    "\n",
    "ml_env = MLEnv(platform='pt', accelerator='fastest')\n",
    "ml_env.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Qg3ZPBmC8kO"
   },
   "source": [
    "## 1. Project configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-TP3Pnsrb1f",
    "outputId": "1daf8364-ff99-40b7-9e7c-86f36346e047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path (all projects) : . (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
      "Project path             : . (Changes to the file system happen only below this project path\n",
      "Model path (snapshots)   : ./model/ngpt_COMP_neo_philosophers_v2_pt (Model weights and snapshots are stored here)\n",
      "Data path (training data): ./data (Training data will be downloaded here)\n",
      "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
     ]
    }
   ],
   "source": [
    "# project_name = 'women_writers'\n",
    "model_cpu = None\n",
    "project_name='neo_philosophers'\n",
    "model_name=f'ngpt_COMP_{project_name}_v2_pt'\n",
    "\n",
    "use_preprocessed_data = True\n",
    "use_existing_model_from_checkpoint = True\n",
    "\n",
    "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
    "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools\n",
    "#\n",
    "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
    "\n",
    "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device\n",
    "\n",
    "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
    "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
    "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
    "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
    "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  2.1 Text data from Project Gutenberg\n",
    "\n",
    "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training,\n",
    "encoding, batch generation, and formatted source display. It read some\n",
    "books from Project Gutenberg and supports creation of training batches.\n",
    "The output functions support highlighting to allow to compare generated\n",
    "texts with the actual sources to help to identify identical (memorized)\n",
    "parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BF8eyWnCrb1h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Loading tokenizer from ./data/neo_philosophers_tokens.json\n",
      "INFO:Datasets:Loading tokenizer done.\n"
     ]
    }
   ],
   "source": [
    "token_file = os.path.join(data_path,f\"{project_name}_tokens.json\")\n",
    "if use_preprocessed_data is True:\n",
    "    if os.path.exists(token_file):\n",
    "        td = Text_Dataset()\n",
    "        td.load_tokenizer(token_file)\n",
    "    else:\n",
    "        use_preprocessed_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C66X7ynnrb1h",
    "outputId": "ba2b7d5a-1027-4c5c-a558-807a27adce20"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False:\n",
    "    cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
    "    gd = Gutenberg_Dataset(cache_dir=cache_dir)\n",
    "\n",
    "    if project_name == 'women_writers':  # sample searches\n",
    "        search_spec= {\n",
    "            \"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"],\n",
    "            \"language\": [\"english\"]\n",
    "        }\n",
    "        book_list=gd.search(search_spec)\n",
    "    elif project_name == 'neo_philosophers':\n",
    "        search_spec = {\n",
    "            \"author\": [\"Immanuel Kant\", \"Friedrich Nietzsche\", \"Wilhelm Hegel\"],\n",
    "            \"language\": [\"english\"]\n",
    "        }\n",
    "        book_list=gd.search(search_spec)\n",
    "        search_spec = {\n",
    "            \"author\": [\"Plato\"],\n",
    "            \"title\": [\"Timaeus\", \"Critias\", \"Symposium\"],\n",
    "            \"language\": [\"english\"]\n",
    "        }\n",
    "        book_list+=gd.search(search_spec)\n",
    "\n",
    "    book_cnt = len(book_list)\n",
    "    print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
    "    if book_cnt<40:\n",
    "        # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts,\n",
    "        # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
    "        book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)\n",
    "    else:\n",
    "        logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")\n",
    "\n",
    "    for i in range(len(book_list)):\n",
    "        print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")\n",
    "\n",
    "    if project_name == 'women_writers':\n",
    "        select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
    "        sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
    "    else:\n",
    "        sub_book_list = book_list\n",
    "\n",
    "    print(\"Using:\")\n",
    "    for i in range(len(sub_book_list)):\n",
    "        print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
    "\n",
    "    td = Text_Dataset(sub_book_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxNIc7gL9UNg"
   },
   "source": [
    "## 2.2 Additional training material for folder `{data_path}/local_texts`\n",
    "\n",
    "If the folder {data_path} (defined above) contains a sub-folder `local_texts`, and it contains\n",
    "files of structure `<title> - <author> - <language>.txt`, then they are added to the training data.\n",
    "Sample filename: `\"./data/local_texts/works-of-shakespeare - William Shakespeare - English.txt\"`.\n",
    "The titles of those documents are referenced via numeric aliases to preserve privacy on non-public data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1NYdjlW65EZP"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False:\n",
    "    use_local_folder_data = False\n",
    "    if use_local_folder_data:\n",
    "        local_texts = os.path.join(data_path, 'local_texts')\n",
    "        if os.path.isdir(local_texts) is False:\n",
    "            print(f\"You have no local texts at {local_texts}\")\n",
    "        else:\n",
    "            fd = Folder_Dataset()\n",
    "            fd.load_index(local_texts, use_aliases=False)\n",
    "            td.load_texts(fd.records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSm4f9NSC8kQ"
   },
   "source": [
    "## 2.3 Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsyBjqFyC8kQ",
    "outputId": "ee564500-ca52-4d17-f825-01a1c183505c"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False:\n",
    "    MAX_TOKENS = 10000  # This becomes vocab_size\n",
    "    MAX_NGRAM_LEN = 6   # Max length of a token\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"Starting tokinizer with token length from 1..{MAX_NGRAM_LEN} with a max of {MAX_TOKENS} unique tokens,\")\n",
    "    print(\"this can take considerable time...\")\n",
    "\n",
    "    # td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS)\n",
    "    td.init_tokenizer(tokenizer='bytegram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS)\n",
    "    td.save_tokenizer(token_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG03WA_yC8kR"
   },
   "source": [
    "## 3. Model metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UPMwIn2gC8kR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Compatible metadata.\n",
      "INFO:root:Loaded model metadata from ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt, {'meta_name_template': '{mhsa_layers}x{heads}x{units}x{vocab_size}', 'mhsa_layers': 4, 'heads': 8, 'causal': True, 'linear_non_linearity': 'relu', 'linear_yoke_hidden_index': -1, 'linear_yoke_size': 96, 'linear_yoke_residual': True, 'stateful': False, 'joint_state_training': 0, 'dropout': 0.1, 'vocab_size': 10000, 'sequence_len': 128, 'embedding_size': 128, 'test_iterations': 10, 'batch_size': 256, 'learning_rate': 0.001, 'sample_every_n_iterations': 256, 'sample_size': 100, 'save_every_n_iterations': 256, 'max_iterations': 1000000, 'current_epoch': 511, 'current_loss': tensor(7.9288, device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "params = None\n",
    "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'current_loss', 'stateful',\n",
    "                 'sample_every_n_iterations', 'sample_size', 'save_every_n_iterations']\n",
    "attn_layers = 4\n",
    "embs = 128\n",
    "linear_yoke_hidden_index = -1  # Set to -1, if no yoke is wanted (standard transformer model)\n",
    "linear_yoke_size = 96\n",
    "\n",
    "params = { # Multi-head self-attention\n",
    "        'meta_name_template': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
    "\n",
    "        'mhsa_layers': attn_layers,\n",
    "        'heads': 8,\n",
    "        'causal': True,  # Use causal self-attention\n",
    "        'linear_non_linearity': 'relu',  # relurelu: use additional relu for state gating\n",
    "        'linear_yoke_hidden_index': linear_yoke_hidden_index,  # no residual for non-default hidden_size only\n",
    "        'linear_yoke_size': linear_yoke_size,\n",
    "        'linear_yoke_residual': True,\n",
    "        'stateful': False,\n",
    "        'joint_state_training': 4,  # use consecutive training samples with shared state for 32 chars\n",
    "        'dropout': 0.1,\n",
    "        'vocab_size': td.get_unique_token_count(),\n",
    "        'sequence_len': embs,\n",
    "        'embedding_size': embs,\n",
    "        'test_iterations': 10,  # number of epocs for loss estimation\n",
    "\n",
    "        'batch_size': 256,\n",
    "        'learning_rate': 0.001,\n",
    "        'sample_every_n_iterations': 256,\n",
    "        'sample_size': 100,\n",
    "        'save_every_n_iterations': 256,\n",
    "\n",
    "        'max_iterations': 1000000  # maximum number of training iterations\n",
    "    }\n",
    "if params['stateful'] is False:\n",
    "    params['joint_state_training'] = 0\n",
    "model_file_path = MJ.get_model_filename(model_path)\n",
    "if use_existing_model_from_checkpoint is True:\n",
    "    params = MJ.load_model_metadata_from_checkpoint(params, updatable_keys, model_file_path, device=device, log=log) # torch.device('cpu'))\n",
    "if params == None or use_existing_model_from_checkpoint is False:\n",
    "    use_existing_model_from_checkpoint = False\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U1R4yDlC8kR"
   },
   "source": [
    "## 4. Batch handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7_tc2Lirb1i",
    "outputId": "e676b5d3-3aee-4d23-e3e1-51e0b6099657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2681780 records\n"
     ]
    }
   ],
   "source": [
    "td.init_getitem(sample_type='encoded', sample_length=params['sequence_len']+1+params['joint_state_training'], content_stepping=1)\n",
    "num_records = len(td)\n",
    "print(f\"{num_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zZbbsNm0cOeW"
   },
   "outputs": [],
   "source": [
    "def get_sample_sub_batch(sample_batch, batch_size, sub_index=0):\n",
    "    for i in range(batch_size):\n",
    "        Xi = sample_batch[sub_index:-1-params['joint_state_training']+sub_index]\n",
    "        if params['joint_state_training']+sub_index == 0:\n",
    "            yi = sample_batch[sub_index+1:]\n",
    "        else:\n",
    "            yi = sample_batch[sub_index+1:-params['joint_state_training']+sub_index]\n",
    "        if i==0:\n",
    "            # smpX=np.array(Xi, dtype=np.float32)\n",
    "            smpX=np.array(Xi, dtype=np.int32)\n",
    "            smpy=np.array(yi, dtype=np.int32)\n",
    "        else:\n",
    "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
    "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
    "    return np.array(smpX), np.array(smpy)\n",
    "\n",
    "def get_sample_batch(td, batch_size):\n",
    "    sample_batch = td.get_random_item()\n",
    "    return get_sample_sub_batch(sample_batch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jY3hUuhQYzdT",
    "outputId": "b3b9df11-330c-483d-ff9d-dd4ea636bae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches = 10475\n"
     ]
    }
   ],
   "source": [
    "num_batches = num_records // params['batch_size']\n",
    "print(f\"num_batches = {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bgVHUkbhdK9y"
   },
   "outputs": [],
   "source": [
    "sample_data = None\n",
    "\n",
    "def get_torch_subbatch(td, batch_size, device, split=None, sub_index=0):\n",
    "    global sample_data\n",
    "    if sub_index==0:\n",
    "        sample_data = td.get_random_item()\n",
    "    x, y = get_sample_sub_batch(sample_data, batch_size, sub_index)\n",
    "    tx = torch.tensor(x, dtype=torch.long).to(device)\n",
    "    tx.requires_grad = False\n",
    "    ty = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    ty.requires_grad = False\n",
    "    return tx, ty\n",
    "\n",
    "def get_torch_batch(td, batch_size, device, split=None):\n",
    "    x, y = get_sample_batch(td, batch_size)\n",
    "    tx = torch.tensor(x, dtype=torch.long).to(device)\n",
    "    tx.requires_grad = False\n",
    "    ty = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    ty.requires_grad = False\n",
    "    return tx, ty\n",
    "\n",
    "def get_zero_state(batch_size, sequence_len, hidden_size, device):\n",
    "    zstate = torch.zeros(batch_size, sequence_len, hidden_size, device=device)\n",
    "    zstate.requires_grad = False\n",
    "    return zstate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pvbi6kjXC8kS"
   },
   "source": [
    "## 5. Loss and training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdaulm1VdK9z",
    "outputId": "7553eb2a-2586-45c9-edc3-cd8e725deaf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Compatible metadata.\n",
      "INFO:root:Continuing from saved state epoch=512, loss=7.929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing from saved state epoch=512, loss=7.929\n",
      "MultiHeadSelfAttention(\n",
      "  (token_embedding_table): Embedding(10000, 128)\n",
      "  (position_embedding_table): Embedding(128, 128)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (non_linearity): ReLU()\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (non_linearity): ReLU()\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (non_linearity): ReLU()\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (non_linearity): ReLU()\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=128, out_features=10000, bias=True)\n",
      ")\n",
      "3.378192 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"creating model...\")\n",
    "try:\n",
    "    # Colab + torch 2 -> lots of garbage.\n",
    "    if model is not None:\n",
    "        del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "if params['stateful'] is False:\n",
    "    if params['linear_yoke_hidden_index'] == -1:\n",
    "        model = MultiHeadSelfAttention(vocab_size=params['vocab_size'], embedding_size=params['embedding_size'],\n",
    "                                       sequence_len=params['sequence_len'], dropout=params['dropout'],\n",
    "                                       num_heads=params['heads'], num_layers=params['mhsa_layers'],\n",
    "                                       causal=params['causal'], device=device)\n",
    "    else:\n",
    "        model = MultiHeadSelfAttentionWithCompression(vocab_size=params['vocab_size'], embedding_size=params['embedding_size'],\n",
    "                                       sequence_len=params['sequence_len'], dropout=params['dropout'],\n",
    "                                       num_heads=params['heads'], num_layers=params['mhsa_layers'],\n",
    "                                       causal=params['causal'], linear_non_linearity=params['linear_non_linearity'],\n",
    "                                       linear_yoke=(params['linear_yoke_hidden_index'], params['linear_yoke_size'], params['linear_yoke_residual']),\n",
    "                                       device=device)\n",
    "else:\n",
    "    model = MultiHeadSelfAttentionWithCompressionState(vocab_size=params['vocab_size'], embedding_size=params['embedding_size'],\n",
    "                                       sequence_len=params['sequence_len'], dropout=params['dropout'],\n",
    "                                       num_heads=params['heads'], num_layers=params['mhsa_layers'],\n",
    "                                       causal=params['causal'], linear_non_linearity=params['linear_non_linearity'],\n",
    "                                       linear_yoke=(params['linear_yoke_hidden_index'], params['linear_yoke_size'], params['linear_yoke_residual']),\n",
    "                                       device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "model = model.to(device)\n",
    "if use_existing_model_from_checkpoint is True:\n",
    "    params_load = MJ.load_checkpoint(params, model, optimizer, file_path=model_file_path, updatable_keys=updatable_keys, device=device, log=log) # torch.device(\"cpu\"))\n",
    "    if params_load is not None:\n",
    "        params = params_load\n",
    "model = model.to(device)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(\"Compiling...\")\n",
    "    model = torch.compile(model)\n",
    "    print(\"Compile ok.\")\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "    except:\n",
    "        print(\"Seems no tensor cores for that.\")\n",
    "# elif str(device) == 'mps':\n",
    "#     print(\"Compiling...\")\n",
    "#     model = torch.compile(model)\n",
    "#     print(\"Compile ok.\")\n",
    "    \n",
    "if 'current_epoch' in params:\n",
    "    ep = params['current_epoch']\n",
    "else:\n",
    "    ep=0\n",
    "if 'current_loss' in params:\n",
    "    ls = params['current_loss']\n",
    "else:\n",
    "    ls=0\n",
    "\n",
    "if ep==0 and ls==0:\n",
    "    start_iter = 0\n",
    "else:\n",
    "    start_iter = ep\n",
    "    current_loss = ls\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QnMCWf5AZn1-"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(device):\n",
    "    # XXX: this does take data for train and val from SAME pool!\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(params['test_iterations'])\n",
    "        for k in range(params['test_iterations']):\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            X, Y = get_torch_batch(td, params['batch_size'], device, split)\n",
    "            if params['stateful'] is False:\n",
    "                logits, loss = model(X, Y)\n",
    "            else:\n",
    "                state = get_zero_state(X.shape[0], params['sequence_len'], params['linear_yoke_size'], device)\n",
    "                logits, loss, state = model(X, Y, state=state)\n",
    "                # print(k, state)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    print(\"\\r\", end=\"\", flush=True)\n",
    "    mloss = (out['train']+out['val'])/2.0\n",
    "    return mloss\n",
    "\n",
    "def generate_sample(td, device, prompt=' ', toks=100, state=None, temperature=1.0, top_k=None, pad=False):\n",
    "    # generate from the model\n",
    "    # context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    model.eval()\n",
    "    if pad is True:\n",
    "        while len(prompt)<params['sequence_len']:\n",
    "            if len(prompt)==params['sequence_len']-1:\n",
    "                prompt = '\\n' + prompt\n",
    "            else:\n",
    "                prompt = ' ' + prompt\n",
    "    context = torch.tensor([td.encode(prompt)]).to(device)\n",
    "    if params['stateful'] is False:\n",
    "        answer = model.generate(context, max_new_tokens=toks, temperature=temperature, top_k=top_k)\n",
    "    else:\n",
    "        if state is None:\n",
    "            print()\n",
    "            print(\"Please don't put state=None in generator!\")\n",
    "            state = get_zero_state(1, params['sequence_len'], params['linear_yoke_size'], device)\n",
    "        answer, state = model.generate(idx=context, max_new_tokens=toks, state=state, temperature=temperature, top_k=top_k)\n",
    "\n",
    "    txt = td.decode(answer[0].tolist())\n",
    "    # Identify memorisation of text by highlighting verbatim quotes from sources\n",
    "    # that are longer than 10 chars. HTML colorcoded output for source identification:\n",
    "    td.source_highlight(txt, min_quote_size=10, dark_mode=False, display_ref_anchor=False)\n",
    "    if params['stateful'] is False:\n",
    "        return txt\n",
    "    else:\n",
    "        return txt, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "N2uWm6CTC8kT"
   },
   "outputs": [],
   "source": [
    "# @torch.jit.script\n",
    "# @torch.compile\n",
    "def do_train_step(xb, yb, device, state=None):\n",
    "    model.train()\n",
    "    if params['stateful'] is False:\n",
    "        logits, loss = model(xb, yb)\n",
    "    else:\n",
    "        # XXX continuous training date & state!\n",
    "        if state is None:\n",
    "            state = get_zero_state(xb.shape[0], params['sequence_len'], params['linear_yoke_size'], device)\n",
    "        logits, loss, state = model(xb, targets=yb, state=state)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if params['stateful'] is True:\n",
    "        return state.detach()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "id": "aZpMI7_iMdR6",
    "outputId": "1a9ef7e0-600b-4f95-ce3a-d8d48806cb0d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training, start at 2023-11-14 09:11:20...\n",
      "step 512: train loss 8.3049, time 1.925 sec/iter\n",
      "Sample at 2023-11-14 09:11:24:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What is the difference between good and evil? The difference \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\">What is the difference between </span><span style=\"background-color:#ebdef0;\">good and evil? </span><span style=\"background-color:#d8daef;\">The difference </span><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>S<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 768: train loss 8.2943, time 0.409 sec/iter.\n",
      "Sample at 2023-11-14 09:13:25:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How did everything come into existence? The origin \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#e2d7d5;\"> did everything </span><span style=\"background-color:#d8daef;\">come into existence</span>?<span style=\"background-color:#eadbd8;\"> The origin </span><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><span style=\"background-color:#d4e6f1;\">                                                                      </span><span style=\"background-color:#d4e6f1;\">                                                                      </span><span style=\"background-color:#d8daef;\">                            H</span>H2EEASA7TI<span style=\"background-color:#d4e6f1;\">                                                                      </span><span style=\"background-color:#d4e6f1;\">                                                                      </span><span style=\"background-color:#d4e6f1;\">                                                                      </span><span style=\"background-color:#e2d7d5;\">                                  N</span>TThe "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#eadbd8;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#d4e6f1;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: The Joyful Wisdom</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1024: train loss 8.1143, time 0.412 sec/iter\n",
      "Sample at 2023-11-14 09:15:16:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What was at the beginning of time? Time itself \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#d8daef;\">What was at</span><span style=\"background-color:#d8daef;\"> the beginning of t</span>ime? T<span style=\"background-color:#eadbd8;\">ime itself </span><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>), and ths<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: The Critique of Pure Reason</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1280: train loss 7.8417, time 0.413 sec/iter\n",
      "Sample at 2023-11-14 09:17:08:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How are physics, quantum-mechanics and consciousness related? The relation between \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#ebdef0;\"> are physi</span>cs, quantum-<span style=\"background-color:#d0ece7;\">mechanics and </span><span style=\"background-color:#d8daef;\">consciousness relate</span>d?<span style=\"background-color:#d8daef;\"> The relation between</span> <br><br>DEESpin<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><br><br>STHELOOUANACA<unk><unk><unk><unk><unk><unk>[45ESLEHIOLENGENDOCEISpin, and <unk><unk><unk><unk>HALTLLENTRLANE"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1536: train loss 7.6975, time 0.407 sec/iter\n",
      "Sample at 2023-11-14 09:19:00:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How to attain complete self-awareness? Complete \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "H<span style=\"background-color:#e2d7d5;\">ow to attain</span><span style=\"background-color:#d0ece7;\"> complete self-a</span>wareness?<span style=\"background-color:#eadbd8;\"> Complete </span>ELCDEMOERUCASTIFAEELFODEAFEHOFUTACCACHTMATFECOOFERFYODEOATLEMERAMAATDITOSONEALFMEEAUTOSETATENATEDIET"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: The Joyful Wisdom</span>, <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#eadbd8;\">Friedrich Nietzsche: The Will to Power, Books I and II</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1792: train loss 7.3469, time 0.409 sec/iter\n",
      "Sample at 2023-11-14 09:20:49:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What is the nature of reality? The nature \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\">What is the nature</span><span style=\"background-color:#d8daef;\"> of reality</span>?<span style=\"background-color:#d8daef;\"> The nature </span>; _CL<span style=\"background-color:#ebdef0;\">Schopenhaue</span><span style=\"background-color:#ebdef0;\">, and their e</span>asts<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2048: train loss 7.6176, time 0.418 sec/iter\n",
      "Sample at 2023-11-14 09:22:43:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How be a good human being? A human \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#eadbd8;\"> be a good </span><span style=\"background-color:#d8daef;\">human being</span>? A human ^parapef the om<span style=\"background-color:#ebdef0;\">y_ which a</span>nd the liime: but agner<span style=\"background-color:#d4efdf;\"> down and t</span>here<span style=\"background-color:#eadbd8;\">by necessar</span>ily axag<span style=\"background-color:#d8daef;\">oring their </span>gorn the so <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> priorihilfe. ME OFranc<span style=\"background-color:#ebdef0;\">e-determine</span> <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#ebdef0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d4efdf;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: The Critique of Pure Reason</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2304: train loss 7.2158, time 0.418 sec/iter\n",
      "Sample at 2023-11-14 09:24:36:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What is the difference between good and evil? The difference \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\">What is the difference between </span><span style=\"background-color:#ebdef0;\">good and evil? </span><span style=\"background-color:#d8daef;\">The difference </span><span style=\"background-color:#d8daef;\">which it be</span> bes<span style=\"background-color:#e2d7d5;\">e faithful </span>all by <span style=\"background-color:#d8daef;\">ur conceptions </span>(_S<span style=\"background-color:#eadbd8;\">towards all \"</span><span style=\"background-color:#d8daef;\">Metaphysic</span>cal<span style=\"background-color:#eadbd8;\"> other hand, I </span><span style=\"background-color:#f6ddcc;\">will become<br></span>the unpothet<span style=\"background-color:#eadbd8;\">icion of the </span><span style=\"background-color:#d8daef;\">Spirit of </span>es<span style=\"background-color:#d4e6f1;\"><br>all which </span><span style=\"background-color:#d8daef;\">contains th</span>.<br>ANPerhapsm A<span style=\"background-color:#ebdef0;\">rt of the e</span>isguis<span style=\"background-color:#e2d7d5;\">ir in the </span>_S<span style=\"background-color:#ebdef0;\">towards our </span>_: (ANIN OFichzsche<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: The Joyful Wisdom</span>, <span style=\"background-color:#eadbd8;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#f6ddcc;\">Friedrich Nietzsche: Human, All Too Human</span>, <span style=\"background-color:#d4e6f1;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#ebdef0;\">Friedrich Nietzsche: Beyond Good and Evil</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2560: train loss 6.7459, time 0.416 sec/iter\n",
      "Sample at 2023-11-14 09:26:28:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How did everything come into existence? The origin \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#e2d7d5;\"> did everything </span><span style=\"background-color:#d8daef;\">come into existence</span>?<span style=\"background-color:#eadbd8;\"> The origin </span>26.<br><br>[583978814557, 3344; 55734245255555, 1426; D45543551] DTEREDEN THED. LAGIII. 2; 128. VII. 1224, 3351513] MESSIOSESELI"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#eadbd8;\">Friedrich Nietzsche: The Will to Power, Books I and II</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2816: train loss 7.0522, time 0.418 sec/iter\n",
      "Sample at 2023-11-14 09:28:21:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What was at the beginning of time? Time itself \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#d8daef;\">What was at</span><span style=\"background-color:#d8daef;\"> the beginning of t</span>ime? T<span style=\"background-color:#eadbd8;\">ime itself </span>512.<br><br>  ANDOSICHERTHENOGENSOSNTHESOARSSTHYFADTHYODISDENALIDIDEODOMLSESANSSOLT CLDYOSTHISSDILSSHYDSIADDEND"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: The Critique of Pure Reason</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3072: train loss 6.9502, time 0.416 sec/iter\n",
      "Sample at 2023-11-14 09:30:12:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How are physics, quantum-mechanics and consciousness related? The relation between \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#ebdef0;\"> are physi</span>cs, quantum-<span style=\"background-color:#d0ece7;\">mechanics and </span><span style=\"background-color:#d8daef;\">consciousness relate</span>d?<span style=\"background-color:#d8daef;\"> The relation between</span> 349.<br><br>[599.<br><br> <span style=\"background-color:#d6dbdf;\"> Does not<br></span>\"<span style=\"background-color:#ecf3cf;\"><br><br>Scepticism</span>_ \"--The \"MYocke \"ELAOKEAOWPTATITINCECSAPREPHES OFETONOLTEETRTONNANTRMEEIETEALACETATHTUACTITM"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Thoughts Out of Season, Part 2</span>, <span style=\"background-color:#ecf3cf;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3328: train loss 6.3969, time 0.411 sec/iter\n",
      "Sample at 2023-11-14 09:32:02:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How to attain complete self-awareness? Complete \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "H<span style=\"background-color:#e2d7d5;\">ow to attain</span><span style=\"background-color:#d0ece7;\"> complete self-a</span>wareness?<span style=\"background-color:#eadbd8;\"> Complete </span>_<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: The Joyful Wisdom</span>, <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#eadbd8;\">Friedrich Nietzsche: The Will to Power, Books I and II</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3584: train loss 6.6114, time 0.405 sec/iter\n",
      "Sample at 2023-11-14 09:33:52:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What is the nature of reality? The nature \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\">What is the nature</span><span style=\"background-color:#d8daef;\"> of reality</span>?<span style=\"background-color:#d8daef;\"> The nature </span><span style=\"background-color:#fdebd0;\">^paragraph</span>ras<span style=\"background-color:#d8daef;\">n the whole the</span><span style=\"background-color:#d8daef;\">ir determinations </span>of-da<span style=\"background-color:#e2d7d5;\">y the arti</span>uch, (<span style=\"background-color:#e2d7d5;\">\"I should </span><span style=\"background-color:#eadbd8;\">in them to </span>the gl<span style=\"background-color:#ecf3cf;\">ow, which is </span><span style=\"background-color:#ecf3cf;\">which was wh</span>ich are (_\"Middl<span style=\"background-color:#d4e6f1;\">e Object, </span><span style=\"background-color:#d4e6f1;\">of the Moral </span><span style=\"background-color:#d8daef;\">which I am</span>s<span style=\"background-color:#d6eaf8;\">? And thus </span><span style=\"background-color:#d8daef;\">which reason</span><unk><unk><br><br>_<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> [692395-treat\" \"<unk><unk>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#fdebd0;\">Immanuel Kant: The Metaphysical Elements of Ethics</span>, <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: The Joyful Wisdom</span>, <span style=\"background-color:#eadbd8;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#ecf3cf;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span>, <span style=\"background-color:#d4e6f1;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#d6eaf8;\">Immanuel Kant: Perpetual Peace</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3840: train loss 6.6192, time 0.407 sec/iter\n",
      "Sample at 2023-11-14 09:35:42:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How be a good human being? A human \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#eadbd8;\"> be a good </span><span style=\"background-color:#d8daef;\">human being</span>? A human 5, 3, 86, 3.<br><br>_Fichtext? More <span style=\"background-color:#d4e6f1;\">_Pure Reason</span><span style=\"background-color:#e2d7d5;\">.<br><br><br>                                   </span><span style=\"background-color:#d4e6f1;\">                                                                      </span><span style=\"background-color:#d4e6f1;\">                                                                      </span><span style=\"background-color:#d4e6f1;\">                                                                      </span><span style=\"background-color:#d4e6f1;\">                                                                      </span><span style=\"background-color:#d4e6f1;\">                                                                      </span>38, Ver<span style=\"background-color:#d8daef;\">i                                                         </span>   "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d4e6f1;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: The Joyful Wisdom</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4096: train loss 6.2774, time 0.406 sec/iter\n",
      "Sample at 2023-11-14 09:37:30:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What is the difference between good and evil? The difference \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\">What is the difference between </span><span style=\"background-color:#ebdef0;\">good and evil? </span><span style=\"background-color:#d8daef;\">The difference </span><span style=\"background-color:#d4e6f1;\">not itself d</span>oes ha<span style=\"background-color:#e2d7d5;\">ve which w</span>e do not<span style=\"background-color:#ebdef0;\">mselves that </span><span style=\"background-color:#d8daef;\">other of the </span><span style=\"background-color:#ebdef0;\">essence of which t</span><span style=\"background-color:#e2d7d5;\">hey are fals</span><span style=\"background-color:#d8daef;\"> of the understand</span><span style=\"background-color:#d4e6f1;\">ad of the mor</span><span style=\"background-color:#d8daef;\">ale of the </span><span style=\"background-color:#d8daef;\">Notion of the in</span>n<span style=\"background-color:#ebdef0;\">er who would </span><span style=\"background-color:#d8daef;\">in Wagner the</span><span style=\"background-color:#eadbd8;\">y of the others, and</span><span style=\"background-color:#fae5d3;\"><br>the Greeks.<br><br></span><span style=\"background-color:#d8daef;\">All these </span><span style=\"background-color:#edebd0;\">reason, and a</span>nd the bse-da<span style=\"background-color:#e2d7d5;\">pt, and in</span><span style=\"background-color:#d8daef;\"> the conceptions, and</span><span style=\"background-color:#d4e6f1;\"> then from </span>_--<span style=\"background-color:#d8daef;\">and through its </span><span style=\"background-color:#d4e6f1;\">subjective pre</span>limic.<span style=\"background-color:#d6dbdf;\">\"historical p</span><span style=\"background-color:#d8daef;\">hilosophy to the </span><span style=\"background-color:#ebdef0;\">think of the </span>worl<span style=\"background-color:#d0ece7;\">nce of the others</span><span style=\"background-color:#ecf3cf;\"> a Greek s</span>uspiciopes, <span style=\"background-color:#ebdef0;\">_--and the </span><span style=\"background-color:#ecf3cf;\">One which has </span>of"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d4e6f1;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: The Critique of Pure Reason</span>, <span style=\"background-color:#fae5d3;\">Friedrich Nietzsche: We Philologists, Volume 8 of 18</span>, <span style=\"background-color:#edebd0;\">Immanuel Kant: Kant's Prolegomena</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Thoughts Out of Season, Part 2</span>, <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#ecf3cf;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4352: train loss 6.4602, time 0.409 sec/iter\n",
      "Sample at 2023-11-14 09:39:20:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How did everything come into existence? The origin \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#e2d7d5;\"> did everything </span><span style=\"background-color:#d8daef;\">come into existence</span>?<span style=\"background-color:#eadbd8;\"> The origin </span><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>_<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><br><br><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><span style=\"background-color:#ecf3cf;\">that is not i</span>s the<span style=\"background-color:#d8daef;\"> _seq._<br><br><br></span>6, _<unk><unk><unk><unk>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#eadbd8;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#ecf3cf;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4608: train loss 6.1587, time 0.412 sec/iter\n",
      "Sample at 2023-11-14 09:41:12:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What was at the beginning of time? Time itself \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#d8daef;\">What was at</span><span style=\"background-color:#d8daef;\"> the beginning of t</span>ime? T<span style=\"background-color:#eadbd8;\">ime itself </span>^paragraspeecying (<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>—<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>—mounto me"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: The Critique of Pure Reason</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4864: train loss 5.8026, time 0.408 sec/iter\n",
      "Sample at 2023-11-14 09:43:03:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How are physics, quantum-mechanics and consciousness related? The relation between \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#ebdef0;\"> are physi</span>cs, quantum-<span style=\"background-color:#d0ece7;\">mechanics and </span><span style=\"background-color:#d8daef;\">consciousness relate</span>d?<span style=\"background-color:#d8daef;\"> The relation between</span> 438088] Kant b<span style=\"background-color:#d8daef;\">acknowledge</span>: _The S<span style=\"background-color:#eadbd8;\">chological sy</span><span style=\"background-color:#eadbd8;\">stematic unity, </span>III. 38, 33885, 11584, _Diog. La<span style=\"background-color:#ecf3cf;\">tinus, which </span><span style=\"background-color:#d8daef;\">is<br>philosophy</span><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> <unk><unk><unk><unk><unk><unk><unk><unk>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span>, <span style=\"background-color:#eadbd8;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: The Critique of Pure Reason</span>, <span style=\"background-color:#ecf3cf;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5120: train loss 6.1033, time 0.408 sec/iter\n",
      "Sample at 2023-11-14 09:44:53:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How to attain complete self-awareness? Complete \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "H<span style=\"background-color:#e2d7d5;\">ow to attain</span><span style=\"background-color:#d0ece7;\"> complete self-a</span>wareness?<span style=\"background-color:#eadbd8;\"> Complete </span>\"_____\"--We may are ______\"<span style=\"background-color:#d8daef;\">___________________________________________</span>..._<span style=\"background-color:#d8daef;\"> ________________________</span>\"<span style=\"background-color:#d8daef;\">____________</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: The Joyful Wisdom</span>, <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#eadbd8;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved model to ./model/ngpt_COMP_neo_philosophers_v2_pt/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  5281/5376/1000000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstateful\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoint_state_training\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     35\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m get_torch_batch(td, params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], device, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mdo_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     state \u001b[38;5;241m=\u001b[39m get_zero_state(\u001b[38;5;241m1\u001b[39m, params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence_len\u001b[39m\u001b[38;5;124m'\u001b[39m], params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear_yoke_size\u001b[39m\u001b[38;5;124m'\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m, in \u001b[0;36mdo_train_step\u001b[0;34m(xb, yb, device, state)\u001b[0m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 14\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstateful\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/gith/domschl/HuggingFaceGuidedTourForMac/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/gith/domschl/HuggingFaceGuidedTourForMac/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/gith/domschl/HuggingFaceGuidedTourForMac/lib/python3.11/site-packages/torch/optim/adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    174\u001b[0m         group,\n\u001b[1;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m         state_steps,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/gith/domschl/HuggingFaceGuidedTourForMac/lib/python3.11/site-packages/torch/optim/adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 335\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gith/domschl/HuggingFaceGuidedTourForMac/lib/python3.11/site-packages/torch/optim/adamw.py:448\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    446\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 448\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n\u001b[1;32m    451\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n",
      "File \u001b[0;32m~/gith/domschl/HuggingFaceGuidedTourForMac/lib/python3.11/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dt0 = time.time()\n",
    "sdt = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"training, start at {sdt}...\")\n",
    "gen_id = 0\n",
    "iter_bench = 1\n",
    "current_loss = estimate_loss(device)\n",
    "if params['stateful'] is True:\n",
    "    gen_state = get_zero_state(1, params['sequence_len'], params['linear_yoke_size'], device=device)\n",
    "else:\n",
    "    gen_state = None\n",
    "inputs = [\"What is the difference between good and evil? The difference \", \"How did everything come into existence? The origin \", \"What was at the beginning of time? Time itself \", \"How are physics, quantum-mechanics and consciousness related? The relation between \", \"How to attain complete self-awareness? Complete \", \"What is the nature of reality? The nature \", \"How be a good human being? A human \"]\n",
    "for iter in range(start_iter, params['max_iterations']):\n",
    "    print(f\"\\rIteration: {iter+1:5d}/{((iter+1)//params['sample_every_n_iterations']+1)*params['sample_every_n_iterations']}/{params['max_iterations']}\", end=\"\", flush=True)\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if (iter + 1) % params['sample_every_n_iterations'] == 0 or iter == params['max_iterations'] - 1:\n",
    "        dt = time.time()\n",
    "        print(f\"\\rloss eval\", end=\"\", flush=True)\n",
    "        current_loss = estimate_loss(device)\n",
    "        print(\n",
    "            f\"step {iter+1}: train loss {current_loss:.4f}, time {(dt-dt0)/iter_bench:.3f} sec/iter\"\n",
    "        )\n",
    "        iter_bench = 1\n",
    "        sdt = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"Sample at {sdt}:\", flush=True)\n",
    "        for temperature in [0.75]:\n",
    "            print(f\"--------temperature: {temperature} ---------\")\n",
    "            prompt = inputs[gen_id%len(inputs)]\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            generate_sample(td=td, device=device, prompt=prompt, toks=params['sample_size'], state=gen_state, temperature=temperature, top_k=16)\n",
    "        print(\"-------------------------------------------\")\n",
    "        gen_id += 1\n",
    "        dt0 = time.time()\n",
    "\n",
    "    if params['stateful'] is False or params['joint_state_training'] == 0:\n",
    "        xb, yb = get_torch_batch(td, params['batch_size'], device, \"train\")\n",
    "        do_train_step(xb, yb, device=device)\n",
    "    else:\n",
    "        state = get_zero_state(1, params['sequence_len'], params['linear_yoke_size'], device=device)\n",
    "        state.requires_grad = False\n",
    "        for i in range(params['joint_state_training']):\n",
    "            print(f\"\\rIteration: {iter+1:5d}[{i+1}/{params['joint_state_training']}]/{((iter+1)//params['sample_every_n_iterations']+1)*params['sample_every_n_iterations']}/{params['max_iterations']}\", end=\"\", flush=True)\n",
    "            xb, yb = get_torch_subbatch(td, params['batch_size'], device, \"train\", i)\n",
    "            state = do_train_step(xb, yb, device=device, state=state)\n",
    "            # state = torch.cat((state, state[:, -1:, :]), dim=1)\n",
    "            # state[:, -1, :] = 0\n",
    "            state = torch.cat((state[:, :1, :], state), dim=1)\n",
    "            # state[:, 0, :] = 0\n",
    "            state = state [:, -params['sequence_len']:, :]\n",
    "            # state.detach() # requires_grad = False\n",
    "\n",
    "    start_iter = iter\n",
    "    iter_bench += 1\n",
    "    if (iter+1)%params['save_every_n_iterations'] == 0:\n",
    "        MJ.save_checkpoint(params, model, optimizer, iter, current_loss, file_path=model_file_path, log=log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "othN-Vnt5EZT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for t in [0.5, 1.5]:\n",
    "#     print(f\"------Temperature {t}--------\")\n",
    "#     generate_sample(td, device, prompt=\"How are consciousness and quantum mechanics related?\", toks=150, temperature=t, top_k=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UNG5wWhC8kU",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADm9ycuA2ik7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VmWbteSFQtfq",
    "yWE_ZZMKEARV"
   ],
   "gpuClass": "premium",
   "gpuType": "V100",
   "include_colab_link": true,
   "name": "torch_transformer_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
