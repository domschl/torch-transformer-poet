{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-transformer-poet/blob/main/torch_transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Torch-Transformer-Poet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DabS0VZ-1Zp0"
   },
   "source": [
    "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jtpy59Yq-Qfz"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # from: https://github.com/pytorch/pytorch/issues/107960  (libcuda not found)\n",
    "    !export LC_ALL=\"en_US.UTF-8\"\n",
    "    !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "    !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "    !ldconfig /usr/lib64-nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U5T4m6earb1e"
   },
   "outputs": [],
   "source": [
    "from ml_indie_tools.env_tools import MLEnv\n",
    "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
    "\n",
    "from ml_indie_tools.Calibre_Dataset import Calibre_Dataset\n",
    "from ml_indie_tools.Folder_Dataset import Folder_Dataset\n",
    "\n",
    "import ml_indie_tools.pytorch_meta_tools as MJ\n",
    "from ml_indie_tools.train_utils import TrainUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional experimental event server to record and propagate training progress, not (yet) recommended!\n",
    "# Functionality is ignored by default.\n",
    "try:\n",
    "    import indralib\n",
    "    indra_avail = True\n",
    "except Exception as e:\n",
    "    indra_avail = False\n",
    "if indra_avail is True:\n",
    "    print(\"Indralib is available, trying to connect to Indrajala server for training progress reports...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jVcwvURB5EZN"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.Logger(\"Main\")\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "A pytorch deep multi-head attention model for text generation following Andrej Karpathy's [video-lecture-ng](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py)\n",
    "\n",
    "This code can use either CPU, GPU, or Apple Silicon. Google Colab is supported too, select the corresponding Colab runtime (menu: **`Runtime / Change runtime type`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "llPw84PkEAP2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OS: Linux, Python: 3.13.2, Jupyter Notebook Pytorch: 2.6.0+cu124, GPU: NVIDIA GeForce RTX 4070 Ti (/  285W |      18MiB), CPU'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
    "\n",
    "ml_env = MLEnv(platform='pt', accelerator='fastest')\n",
    "ml_env.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Qg3ZPBmC8kO"
   },
   "source": [
    "## 1. Project configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "t-TP3Pnsrb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path (all projects) : . (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
      "Project path             : . (Changes to the file system happen only below this project path\n",
      "Model path (snapshots)   : ./model/tr_neo_philosophers_v3_pt (Model weights and snapshots are stored here)\n",
      "Data path (training data): ./data (Training data will be downloaded here)\n",
      "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
     ]
    }
   ],
   "source": [
    "# project_name = 'women_writers'\n",
    "# project_name='research'\n",
    "project_name='neo_philosophers'\n",
    "model_cpu = None\n",
    "model_name=f'tr_{project_name}_v3_pt'\n",
    "\n",
    "use_preprocessed_data = True                      # Use already tokenized data\n",
    "use_existing_model_from_checkpoint = False         # Try to load checkpoint of training\n",
    "use_torch_compile = True                           # Requires a modern graphics card with torch compile backend support\n",
    "skip_additional_texts = False                       # Don't look for other data sources in `additional_texts.json`\n",
    "\n",
    "if 'google.colab' in sys.modules:  # Google colab notebooks run on server that provide UTC time, we adapt logs to local time:\n",
    "    local_timezone = ZoneInfo('Europe/Berlin')\n",
    "else:\n",
    "    local_timezone = None\n",
    "\n",
    "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
    "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools\n",
    "#\n",
    "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
    "\n",
    "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device\n",
    "\n",
    "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
    "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
    "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
    "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
    "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  2.1 Text data from Project Gutenberg\n",
    "\n",
    "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training,\n",
    "encoding, batch generation, and formatted source display. It read some\n",
    "books from Project Gutenberg and supports creation of training batches.\n",
    "The output functions support highlighting to allow to compare generated\n",
    "texts with the actual sources to help to identify identical (memorized)\n",
    "parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BF8eyWnCrb1h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Loading tokenizer from ./data/neo_philosophers_tokens.json\n",
      "INFO:Datasets:Loading tokenizer done.\n"
     ]
    }
   ],
   "source": [
    "token_file = os.path.join(data_path,f\"{project_name}_tokens.json\")\n",
    "if use_preprocessed_data is True:\n",
    "    if os.path.exists(token_file):\n",
    "        td = Text_Dataset()\n",
    "        td.load_tokenizer(token_file)\n",
    "    else:\n",
    "        use_preprocessed_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "C66X7ynnrb1h"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False:\n",
    "    cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
    "    gd = Gutenberg_Dataset(cache_dir=cache_dir)\n",
    "\n",
    "    if project_name == 'women_writers':  # sample searches\n",
    "        search_spec= {\n",
    "            \"author\": [\"Emily BrontÃ«\", \"Jane Austen\", \"Virginia Woolf\"],\n",
    "            \"language\": [\"english\"]\n",
    "        }\n",
    "        book_list=gd.search(search_spec)\n",
    "    elif project_name == 'neo_philosophers':\n",
    "        search_spec = {\n",
    "            \"author\": [\"Immanuel Kant\", \"Friedrich Nietzsche\", \"Wilhelm Hegel\", \"Arthur Schopenhauer\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list=gd.search(search_spec)\n",
    "        search_spec = {\n",
    "            \"author\": [\"Plato\", \"Platon\"],\n",
    "            \"title\": [\"Timaeus\", \"Critias\", \"Symposium\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list+=gd.search(search_spec)\n",
    "        search_spec = {\n",
    "            \"title\": [\"Buddh\", \"Sutra\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list+=gd.search(search_spec)\n",
    "    else:\n",
    "        search_spec = {}\n",
    "        book_list = []\n",
    "\n",
    "    book_cnt = len(book_list)\n",
    "    print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
    "\n",
    "    if book_cnt > 0:\n",
    "        if book_cnt<80:\n",
    "            # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts,\n",
    "            # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
    "            book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)\n",
    "        else:\n",
    "            logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")\n",
    "\n",
    "        for i in range(len(book_list)):\n",
    "            if 'author' not in book_list[i]:\n",
    "                book_list[i]['author']='unknown'\n",
    "            print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")\n",
    "\n",
    "        if project_name == 'women_writers':\n",
    "            select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
    "            sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
    "        else:\n",
    "            sub_book_list = book_list\n",
    "\n",
    "        print(\"Using:\")\n",
    "        for i in range(len(sub_book_list)):\n",
    "            if 'author' not in sub_book_list[i]:\n",
    "                sub_book_list[i]['author']='unknown'\n",
    "            print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
    "\n",
    "        td = Text_Dataset(sub_book_list)\n",
    "    else:\n",
    "        td = Text_Dataset()()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxNIc7gL9UNg"
   },
   "source": [
    "## 2.2 Additional training material from folders or Calibre library\n",
    "\n",
    "This looks for a file `additional_texts.json` in the `project_path` as shown above.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"local_texts\": [[\"/some/directory/that/contains/texts\", [\".txt\", \".md\", \".org\", \".py\"]],\n",
    "  \"calibre\": [\"/home/myuser/Calibre Library\", []]\n",
    "}\n",
    "```\n",
    "\n",
    "If the folder(s) defined in `local_texts` contain text files with default endings `.txt`, `.md`, `.org`, or `.py` (can be configured), they are added to the training data. Folders are searched recursively.\n",
    "\n",
    "If the path defined in `calibre` contains a Calibre database, all text files (`.txt` only) within that library are added to the training data. The list argument can contain search-specs (see ml-indie-tools, calibre_dataset) to qualify which books to import, e.g. [{\"tags\": [\"philosophy\"]}] would import all books that are tagged with \"Philosophy\" within calibre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1NYdjlW65EZP"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False and skip_additional_texts is False:\n",
    "    additional = os.path.join(project_path, \"additional_texts.json\")\n",
    "    print(f\"Looking for description of additional sources in {additional}\")\n",
    "    if os.path.exists(additional) is True:\n",
    "        with open(additional, 'r') as f:\n",
    "            add_desc = json.load(f)\n",
    "            if 'local_texts' in add_desc:\n",
    "                fd = Folder_Dataset()\n",
    "                for text_path, qualifier in add_desc['local_texts']:\n",
    "                    if not isinstance(qualifier, list):\n",
    "                        qualifier = [qualifier]\n",
    "                    print(f\"Loading texts from {text_path} using extension restriction {qualifier}\")\n",
    "                    fd.load_index(text_path, use_aliases=False, min_file_size=2048, file_extensions=qualifier)\n",
    "                td.load_texts(fd.records[:10000])\n",
    "            if 'calibre' in add_desc:\n",
    "                cal_path, specs = add_desc['calibre']\n",
    "                if os.path.exists(cal_path):\n",
    "                    print(f\"Loading text from calibre at {cal_path}\")\n",
    "                    cd = Calibre_Dataset(cal_path)\n",
    "                    cd.load_index(max_file_size=100000000) \n",
    "                    if specs is not None and len(specs)!=0:\n",
    "                        ls = cd.search(specs)\n",
    "                        td.load_texts(ls)\n",
    "                    else:\n",
    "                        td.load_texts(cd.records[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSm4f9NSC8kQ"
   },
   "source": [
    "## 2.3 Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bsyBjqFyC8kQ"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False:\n",
    "    MAX_TOKENS = 32768  # This becomes vocab_size\n",
    "    MAX_NGRAM_LEN = 5   # Max length of a token\n",
    "    CHUNK_SIZE = 500000 # Split larger texts in chunks, if not None\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"Starting tokenizer with token length from 1..{MAX_NGRAM_LEN} with a max of {MAX_TOKENS} unique tokens,\")\n",
    "    print(\"this can take considerable time...\")\n",
    "\n",
    "    # Better tested NGRAM tokenizer:\n",
    "    # td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS) \n",
    "    # or alternative 'BYTEGRAM' (more experimental, can encode arbitrary UTF-8)\n",
    "    # td.init_tokenizer(tokenizer='bytegram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS, chunk_size=CHUNK_SIZE)\n",
    "    td.init_tokenizer(tokenizer='bytegram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS, chunk_size=CHUNK_SIZE)\n",
    "    td.save_tokenizer(token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: Good morning, this is a simple test sentence for tokenization(61) -> [5305, 111, 14443, 110, 1703, 116, 2759, 115, 15819, 109, 3184, 116, 25975, 101, 4283, 99, 1591, 32, 362, 107, 4337, 122, 444](23) OK, compressed size: 37.70%\n",
      "Tokenizer: Guten Morgen, dies is ein einfach Testsatz zur Aufteilung in Satzbestandteile(77) -> [31731, 110, 16080, 103, 2087, 105, 11319, 32, 22115, 105, 16381, 99, 347, 84, 11107, 97, 15893, 122, 1043, 65, 1868, 116, 25161, 103, 22582, 97, 2730, 98, 6441, 110, 24565, 101, 3106](33) OK, compressed size: 42.86%\n",
      "Tokenizer: à½¦à½ºà½à½¦à¼à½à½²à½à¼à½à½£à¼à½à½¦à½¼à¼à½¢à¾à¾±à½´à½à¼(22) -> [224, 189, 166, 224, 189, 186, 224, 189, 152, 224, 189, 166, 224, 188, 139, 224, 189, 137, 224, 189, 178, 224, 189, 145, 224, 188, 139, 224, 189, 132, 224, 189, 163, 224, 188, 139, 224, 189, 150, 224, 189, 166, 224, 189, 188, 224, 188, 139, 224, 189, 162, 224, 190, 146, 224, 190, 177, 224, 189, 180, 224, 189, 145, 224, 188, 139](66) OK, compressed size: 300.00%\n"
     ]
    }
   ],
   "source": [
    "tok_tests = [\"Good morning, this is a simple test sentence for tokenization\", \"Guten Morgen, dies is ein einfach Testsatz zur Aufteilung in Satzbestandteile\", \"à½¦à½ºà½à½¦à¼à½à½²à½à¼à½à½£à¼à½à½¦à½¼à¼à½¢à¾à¾±à½´à½à¼\"]\n",
    "for test in tok_tests:\n",
    "    enc = td.encode(test)\n",
    "    dec = td.decode(enc)\n",
    "    if dec != test:\n",
    "        print(f\"Tokenizer failed for: {test} -> {dec}\")\n",
    "    else:\n",
    "        r = len(enc)/len(test)*100.0\n",
    "        print(f\"Tokenizer: {test}({len(test)}) -> {enc}({len(enc)}) OK, compressed size: {r:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG03WA_yC8kR"
   },
   "source": [
    "## 3. Model metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UPMwIn2gC8kR"
   },
   "outputs": [],
   "source": [
    "params = None\n",
    "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'current_loss',\n",
    "                 'sample_every_n_iterations', 'sample_size', 'save_every_n_iterations', 'max_iterations']\n",
    "attn_layers = 4\n",
    "dims = 512\n",
    "sequence_length = 192\n",
    "\n",
    "params = { # Multi-head self-attention\n",
    "        'meta_name_template': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
    "\n",
    "        'mhsa_layers': attn_layers,\n",
    "        'heads': 16,\n",
    "        'vocab_size': td.get_unique_token_count(),\n",
    "        'sequence_len': sequence_length,\n",
    "        'dropout': 0.1,\n",
    "        'embedding_size': dims,\n",
    "        'test_iterations': 100,  # number of epocs for loss estimation\n",
    "        'use_recur': False,\n",
    "        'share_weights': False,\n",
    "\n",
    "        'batch_size': 48,      \n",
    "        'learning_rate': 4e-4,   # None: Set in dependence of graphics hw\n",
    "        'lr_schedule': True,\n",
    "        'lr_min': 2e-5,\n",
    "        'lr_max': 2e-4,\n",
    "        'warmup': 4000,\n",
    "        'decay': 100000,\n",
    "        'grad_clip': 0.8,\n",
    "\n",
    "        'sample_every_n_iterations': 1024,\n",
    "        'sample_size': 192,\n",
    "        'save_every_n_iterations': 4096,\n",
    "\n",
    "        'max_iterations': 100000000  # maximum number of training iterations\n",
    "    }\n",
    "\n",
    "model_file_path = MJ.get_model_filename(model_path)\n",
    "if use_existing_model_from_checkpoint is True:\n",
    "    params = MJ.load_model_metadata_from_checkpoint(params, updatable_keys, model_file_path, device=device, log=log) # torch.device('cpu'))\n",
    "if params == None or use_existing_model_from_checkpoint is False:\n",
    "    use_existing_model_from_checkpoint = False\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U1R4yDlC8kR"
   },
   "source": [
    "## 4. Batch handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "f7_tc2Lirb1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14796641 records\n"
     ]
    }
   ],
   "source": [
    "joint_training=0\n",
    "td.init_getitem(sample_type='encoded', sample_length=params['sequence_len']+1+joint_training, content_stepping=1)\n",
    "num_records = len(td)\n",
    "print(f\"{num_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zZbbsNm0cOeW"
   },
   "outputs": [],
   "source": [
    "def get_sample_sub_batch(sample_batch, batch_size, sub_index=0):\n",
    "    joint_training=0\n",
    "    for i in range(batch_size):\n",
    "        Xi = sample_batch[sub_index:-1-joint_training+sub_index]\n",
    "        yi = sample_batch[sub_index+1:]\n",
    "        if i==0:\n",
    "            # smpX=np.array(Xi, dtype=np.float32)\n",
    "            smpX=np.array(Xi, dtype=np.int32)\n",
    "            smpy=np.array(yi, dtype=np.int32)\n",
    "        else:\n",
    "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
    "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
    "    return np.array(smpX), np.array(smpy)\n",
    "\n",
    "def get_sample_batch(td, batch_size):\n",
    "    sample_batch = td.get_random_item()\n",
    "    return get_sample_sub_batch(sample_batch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jY3hUuhQYzdT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches = 308263\n"
     ]
    }
   ],
   "source": [
    "num_batches = num_records // params['batch_size']\n",
    "print(f\"num_batches = {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 192), (2, 192))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_sample_batch(td, 2)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bgVHUkbhdK9y"
   },
   "outputs": [],
   "source": [
    "sample_data = None\n",
    "\n",
    "def get_torch_subbatch(td, batch_size, device, split=None, sub_index=0):\n",
    "    global sample_data\n",
    "    if sub_index==0:\n",
    "        sample_data = td.get_random_item()\n",
    "    x, y = get_sample_sub_batch(sample_data, batch_size, sub_index)\n",
    "    tx = torch.tensor(x, dtype=torch.long).to(device)\n",
    "    tx.requires_grad = False\n",
    "    ty = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    ty.requires_grad = False\n",
    "    return tx, ty\n",
    "\n",
    "def get_torch_batch(td, batch_size, device, split=None):\n",
    "    x, y = get_sample_batch(td, batch_size)\n",
    "    tx = torch.tensor(x, dtype=torch.long).to(device)\n",
    "    tx.requires_grad = False\n",
    "    ty = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    ty.requires_grad = False\n",
    "    return tx, ty\n",
    "\n",
    "def get_zero_state(batch_size, sequence_len, hidden_size, device):\n",
    "    zstate = torch.zeros(batch_size, sequence_len, hidden_size, device=device)\n",
    "    zstate.requires_grad = False\n",
    "    return zstate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pvbi6kjXC8kS"
   },
   "source": [
    "## 5. Loss and training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000, device=None):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model, device=device)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingGrok(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Precompute positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, batch_size, d_model]\n",
    "        seq_len = x.size(0)\n",
    "        pe = self.pe[:, :seq_len, :].expand(-1, x.size(1), -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROK-3 implementation of latent recurrence!\n",
    "\n",
    "# Reusing previous block definitions with minor tweaks\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        if attn_mask is None and x.size(0) > 1:\n",
    "            seq_len = x.size(0)\n",
    "            attn_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "            attn_mask = attn_mask.to(x.device)  # [seq_len, seq_len], upper triangle = True (masked)\n",
    "        \n",
    "        attn_output, _ = self.self_attn(x, x, x, \n",
    "                                      attn_mask=attn_mask,\n",
    "                                      key_padding_mask=key_padding_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        return x\n",
    "\n",
    "class LatentRecurrentBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, num_steps=3, dropout=0.1, use_lstm=True):\n",
    "        super(LatentRecurrentBlock, self).__init__()\n",
    "        self.num_steps = num_steps\n",
    "        self.use_lstm = use_lstm\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        if use_lstm is True:\n",
    "            self.recurrent = nn.LSTM(  # Swap GRU for LSTM\n",
    "                input_size=d_model,\n",
    "                hidden_size=d_model,\n",
    "                num_layers=1,\n",
    "                batch_first=True,\n",
    "                bidirectional=False\n",
    "            )\n",
    "        else:\n",
    "            self.recurrent = nn.GRU(\n",
    "                input_size=d_model,\n",
    "                hidden_size=d_model,\n",
    "                num_layers=1,\n",
    "                batch_first=True,\n",
    "                bidirectional=False\n",
    "            )\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        attn_output, _ = self.self_attn(x, x, x, \n",
    "                                      attn_mask=attn_mask,\n",
    "                                      key_padding_mask=key_padding_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        # residual = x\n",
    "        batch_size = x.size(1)\n",
    "        latent = x.transpose(0, 1).contiguous()  # [batch, seq_len, d_model]\n",
    "        latent = latent.view(batch_size * x.size(0), 1, x.size(2))  # [batch*seq, 1, d_model]\n",
    "        if self.use_lstm is False:\n",
    "            h0 = torch.zeros(1, batch_size * x.size(0), x.size(2), device=x.device)\n",
    "            for _ in range(self.num_steps):\n",
    "                latent, h0 = self.recurrent(latent, h0)\n",
    "            latent = latent.view(batch_size, x.size(0), x.size(2)).transpose(0, 1)\n",
    "        else:\n",
    "            h0 = torch.zeros(1, latent.size(0), x.size(2), device=x.device)\n",
    "            c0 = torch.zeros(1, latent.size(0), x.size(2), device=x.device)  # Add cell state\n",
    "            for _ in range(self.num_steps):\n",
    "                latent, (h0, c0) = self.recurrent(latent, (h0, c0))  # LSTM outputs hidden + cell\n",
    "            latent = latent.view(x.size(1), x.size(0), -1).transpose(0, 1)\n",
    "            # latent = residual + latent\n",
    "        ff_output = self.ff(latent)\n",
    "        output = self.norm2(latent + self.dropout2(ff_output))\n",
    "        return output\n",
    "\n",
    "class LatentRecurrentDepthModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, sequence_len, d_ff, \n",
    "                 n1_prelude, n2_recurrent, n3_coda, num_steps=3, dropout=0.1, use_lstm=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary (for embedding and projection).\n",
    "            d_model (int): Transformer hidden size.\n",
    "            nhead (int): Number of attention heads.\n",
    "            d_ff (int): Feedforward hidden size.\n",
    "            n1_prelude, n2_recurrent, n3_coda (int): Number of blocks per stage.\n",
    "            num_steps (int): Recurrent steps per LRD block.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(LatentRecurrentDepthModel, self).__init__()\n",
    "\n",
    "        self.sequence_len = sequence_len  # for generate\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncodingGrok(d_model, max_len=sequence_len)\n",
    "        # self.pos_encoding = PositionalEncoding(d_model) # , max_len=sequence_len)\n",
    "        \n",
    "        # Prelude blocks\n",
    "        self.prelude = nn.ModuleList([\n",
    "            TransformerBlock(d_model, nhead, d_ff, dropout)\n",
    "            for _ in range(n1_prelude)\n",
    "        ])\n",
    "        \n",
    "        # Latent Recurrent blocks\n",
    "        if n2_recurrent > 0:\n",
    "            self.recurrent = nn.ModuleList([\n",
    "                LatentRecurrentBlock(d_model, nhead, d_ff, num_steps, dropout, use_lstm)\n",
    "                for _ in range(n2_recurrent)\n",
    "            ])\n",
    "        else:\n",
    "            self.recurrent = None\n",
    "        \n",
    "        # Coda blocks\n",
    "        self.coda = nn.ModuleList([\n",
    "            TransformerBlock(d_model, nhead, d_ff, dropout)\n",
    "            for _ in range(n3_coda)\n",
    "        ])\n",
    "        \n",
    "        # Final projection layer (e.g., to vocab size for generation)\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Token IDs [batch_size, seq_len].\n",
    "            attn_mask (torch.Tensor, optional): Attention mask [seq_len, seq_len].\n",
    "            key_padding_mask (torch.Tensor, optional): Padding mask [batch_size, seq_len].\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits [batch_size, seq_len, vocab_size].\n",
    "        \"\"\"\n",
    "        # Embed input tokens\n",
    "        x = self.embedding(input_ids) * math.sqrt(self.d_model) /2.0  # [batch_size, seq_len, d_model]\n",
    "        # x = self.pos_encoding(x)\n",
    "        x = x.transpose(0, 1)  # [seq_len, batch_size, d_model] for transformer\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Prelude: Entry to latent space\n",
    "        for block in self.prelude:\n",
    "            x = block(x, attn_mask, key_padding_mask)\n",
    "        \n",
    "        # Recurrent: Refine latents\n",
    "        if self.recurrent is not None:\n",
    "            for block in self.recurrent:\n",
    "                x = block(x, attn_mask, key_padding_mask)\n",
    "        \n",
    "        # Coda: Exit from latent space\n",
    "        for block in self.coda:\n",
    "            x = block(x, attn_mask, key_padding_mask)\n",
    "        \n",
    "        # Project to output space\n",
    "        x = x.transpose(0, 1)  # [batch_size, seq_len, d_model]\n",
    "        output = self.proj(x)  # [batch_size, seq_len, vocab_size]\n",
    "        return output\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate new tokens given a context\n",
    "\n",
    "        Note: for apple MPS, top_k is limited max 16 vor older torchs! ((01/2023) implementation limitation)\n",
    "        See: https://github.com/pytorch/pytorch/issues/78915\n",
    "        Solved in: https://github.com/pytorch/pytorch/pull/94639 (03/2023)\n",
    "\n",
    "        :param idx: the context (B,T) tensor of indices\n",
    "        :param max_new_tokens: the maximum number of tokens to generate\n",
    "        :param temperature: the temperature to use for sampling\n",
    "        :param top_k: the number of top tokens to consider\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last sequence_len tokens\n",
    "            idx_cond = idx[:, -self.sequence_len :]\n",
    "            # print(idx_cond.shape)\n",
    "            # get the predictions\n",
    "            logits = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply temperature\n",
    "            if temperature != 1.0 and temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def generate_from_prompt(self, model, tokenizer, prompt=\"The\", max_len=50):\n",
    "        model.eval()\n",
    "        input_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_len):\n",
    "                logits = model(input_ids)\n",
    "                next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "                input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=1)\n",
    "        return tokenizer.decode(input_ids[0].tolist())\n",
    "\n",
    "    def generate_with_beam(self, model, tokenizer, prompt=\"The\", max_len=50, temperature=1.0, top_k=30, beam_width=3):\n",
    "    # def generate(model, tokenizer, prompt=\"The\", max_len=50, temperature=1.0, top_k=30, beam_width=3):\n",
    "        \"\"\"\n",
    "        Beam search generation with static abort condition.\n",
    "        \n",
    "        Args:\n",
    "            model: LatentRecurrentDepthModel\n",
    "            tokenizer: Your custom/botok tokenizer (no [EOS])\n",
    "            prompt (str): Starting text\n",
    "            max_len (int): Max output length\n",
    "            temperature (float): Softmax temperature\n",
    "            top_k (int): Sample from top k tokens\n",
    "            beam_width (int): Number of beams\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        device = next(model.parameters()).device\n",
    "        input_ids = torch.tensor([tokenizer.encode(prompt)], device=device)  # [1, seq_len]\n",
    "        beams = [(input_ids, 0.0)]  # (sequence, log_prob)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in range(max_len):\n",
    "                new_beams = []\n",
    "                for seq, score in beams:\n",
    "                    # Forward pass\n",
    "                    logits = model(seq)  # [1, seq_len, vocab_size]\n",
    "                    next_logits = logits[0, -1, :] / temperature\n",
    "                    \n",
    "                    # Top-k sampling\n",
    "                    top_k_logits, top_k_indices = torch.topk(next_logits, top_k)\n",
    "                    probs = F.softmax(top_k_logits, dim=-1)\n",
    "                    \n",
    "                    # Sample beam_width candidates\n",
    "                    next_tokens = torch.multinomial(probs, num_samples=beam_width)\n",
    "                    for i in range(beam_width):\n",
    "                        token_id = top_k_indices[next_tokens[i]].unsqueeze(0).unsqueeze(0)  # [1, 1]\n",
    "                        log_prob = torch.log(probs[next_tokens[i]]).item()\n",
    "                        new_seq = torch.cat([seq, token_id], dim=1)\n",
    "                        # Repetition penalty\n",
    "                        penalty = 1.0 if new_seq[0, -1].item() not in new_seq[0, -5:-1] else 0.9\n",
    "                        new_beams.append((new_seq, score + log_prob * penalty))\n",
    "                \n",
    "                # Sort and prune beams\n",
    "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "                \n",
    "                # Static abort: all beams at max_len or repeating last 5 tokens\n",
    "                all_max_len = all(len(seq[0]) >= max_len for seq, _ in beams)\n",
    "                all_repeating = all(\n",
    "                    len(seq[0]) > 5 and seq[0, -5:].tolist() == [seq[0, -1].item()] * 5 \n",
    "                    for seq, _ in beams\n",
    "                )\n",
    "                if all_max_len or all_repeating:\n",
    "                    break\n",
    "        \n",
    "        best_seq, _ = beams[0]\n",
    "        return tokenizer.decode(best_seq[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.GRU, nn.LSTM)):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param, gain=1.0)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pdaulm1VdK9z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model...\n",
      "Compiling...\n",
      "Compile ok.\n",
      "OptimizedModule(\n",
      "  (_orig_mod): LatentRecurrentDepthModel(\n",
      "    (embedding): Embedding(32768, 512)\n",
      "    (pos_encoding): PositionalEncodingGrok()\n",
      "    (prelude): ModuleList(\n",
      "      (0-2): 3 x TransformerBlock(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (recurrent): ModuleList(\n",
      "      (0): LatentRecurrentBlock(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (recurrent): LSTM(512, 512, batch_first=True)\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (coda): ModuleList(\n",
      "      (0-1): 2 x TransformerBlock(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=512, out_features=32768, bias=True)\n",
      "  )\n",
      ")\n",
      "54.602752 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"creating model...\")\n",
    "try:\n",
    "    # Colab + torch 2 -> lots of garbage.\n",
    "    if model is not None:\n",
    "        del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "groks = True\n",
    "special_init = True\n",
    "\n",
    "if groks is False:\n",
    "    model = MultiHeadSelfAttentionWithMemory(vocab_size=params['vocab_size'], embedding_size=params['embedding_size'],\n",
    "                                           sequence_len=params['sequence_len'],\n",
    "                                           num_heads=params['heads'], num_layers=params['mhsa_layers'], dropout=params['dropout'],\n",
    "                                           use_recur=params['use_recur'], share_weights=params['share_weights'], device=device)\n",
    "else:\n",
    "    model = LatentRecurrentDepthModel(\n",
    "        vocab_size=params['vocab_size'],\n",
    "        d_model=params['embedding_size'], nhead=params['heads'], d_ff=params['embedding_size']*4,\n",
    "        sequence_len=params['sequence_len'],\n",
    "        n1_prelude=3, n2_recurrent=1, n3_coda=2, num_steps=3\n",
    "    )\n",
    "    if special_init is True:\n",
    "        model.apply(init_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "model = model.to(device)\n",
    "if use_existing_model_from_checkpoint is True:\n",
    "    params_load = MJ.load_checkpoint(params, model, optimizer, file_path=model_file_path, updatable_keys=updatable_keys, device=device, log=log) # torch.device(\"cpu\"))\n",
    "    if params_load is not None:\n",
    "        params = params_load\n",
    "model = model.to(device)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "if use_torch_compile is True:\n",
    "    if device == 'cuda':\n",
    "        print(\"Compiling...\")\n",
    "        model = torch.compile(model)\n",
    "        print(\"Compile ok.\")\n",
    "        try:\n",
    "            torch.set_float32_matmul_precision('high')\n",
    "        except:\n",
    "            print(\"Seems no tensor cores for that.\")\n",
    "    # elif str(device) == 'mps':\n",
    "    #     print(\"Compiling...\")\n",
    "    #     model = torch.compile(model)\n",
    "    #     print(\"Compile ok.\")\n",
    "\n",
    "if 'current_epoch' in params:\n",
    "    ep = params['current_epoch']\n",
    "else:\n",
    "    ep=0\n",
    "if 'current_loss' in params:\n",
    "    ls = params['current_loss']\n",
    "else:\n",
    "    ls=0\n",
    "\n",
    "if ep==0 and ls==0:\n",
    "    start_iter = 0\n",
    "else:\n",
    "    start_iter = ep\n",
    "    current_loss = ls\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "class MultiHeadSelfAttentionWithMemory(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_size,\n",
    "        sequence_len,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        dropout=0.1,\n",
    "        use_recur=False,\n",
    "        share_weights=True,\n",
    "        device=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if device is None:\n",
    "            raise ValueError(\n",
    "                \"Device is None at MultiHeadSelfAttentionWithMemory\"\n",
    "            )\n",
    "        self.device = device\n",
    "        self.sequence_len = sequence_len\n",
    "        self.use_recur = use_recur\n",
    "        context_sub_layers = num_layers // 2\n",
    "        self.context_sub_layers = context_sub_layers\n",
    "        dims = embedding_size\n",
    "        self.dims = dims\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.position_embedding_table = nn.Embedding(\n",
    "            sequence_len, embedding_size, device=device\n",
    "        )\n",
    "\n",
    "        if share_weights is True:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "            self.out_proj = nn.Linear(embedding_size, vocab_size, bias=False)\n",
    "            self.embedding.weight = self.out_proj.weight  # torch.nn.Parameter(self.out_proj.weight.transpose(1,0))\n",
    "            self.out_proj = self.out_proj.to(device)\n",
    "            self.embdding = self.embedding.to(device)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_size, device=device)\n",
    "            self.out_proj = nn.Linear(embedding_size, vocab_size, device=device)\n",
    "\n",
    "\n",
    "        if use_recur is True:\n",
    "            self.rec=nn.RNN(dims, dims, num_layers=3, batch_first=True, device=device)\n",
    "            self.lq=nn.Linear(dims, dims, device=device)\n",
    "            self.lk=nn.Linear(dims, dims, device=device)\n",
    "            self.lv=nn.Linear(dims, dims, device=device)\n",
    "            self.lnorm=nn.BatchNorm1d(dims, device=device)\n",
    "            self.sm = nn.Softmax(dim=2)\n",
    "            self.proj1 = nn.Linear(dims, dims, device=device)\n",
    "            self.sm2 = nn.Softmax(dim=2)\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=dims, nhead=num_heads, dim_feedforward=dims*4, dropout=dropout, batch_first=True, device=device)\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=context_sub_layers)\n",
    "            encoder_layer2 = nn.TransformerEncoderLayer(d_model=dims, nhead=num_heads, dim_feedforward=dims*4, dropout=dropout, batch_first=True, device=device)\n",
    "            self.transformer2 = nn.TransformerEncoder(encoder_layer2, num_layers=num_layers - context_sub_layers)\n",
    "        else:\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=dims, nhead=num_heads, activation=\"gelu\", dim_feedforward=dims*4, dropout=dropout, batch_first=True, device=device)\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_proj.bias.data.zero_()\n",
    "        self.out_proj.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, D = idx.shape\n",
    "        tok_emb = self.embedding(idx)\n",
    "        # idx and targets are both (B,D) tensor of integers\n",
    "        # tok_emb = self.token_embedding_table(idx)  # (B,D,C)\n",
    "\n",
    "        # XXX: move to init, make not trainable:\n",
    "        if self.device is None:\n",
    "            pos_emb = self.position_embedding_table(torch.arange(self.sequence_len))\n",
    "        else:\n",
    "            pos_emb = self.position_embedding_table(\n",
    "                torch.arange(D, device=self.device)\n",
    "            )  # (D,C)\n",
    "\n",
    "        x = tok_emb + pos_emb  # (B,D,C)\n",
    "        \n",
    "        # x = self.pos_encoder(x) \n",
    "        x_mask = nn.Transformer.generate_square_subsequent_mask(D).to(self.device)\n",
    "        x = self.transformer(x, x_mask)\n",
    "\n",
    "        if self.use_recur is True:\n",
    "            skip = x\n",
    "            x = self.rec(x)[0] + x\n",
    "            xk = self.lk(x).permute((0,2,1))\n",
    "            xv = self.lv(x)\n",
    "            xq = self.lq(x)\n",
    "            xqk = torch.matmul(xq, xk)\n",
    "            sm = self.sm(xqk)/math.sqrt(D)\n",
    "            att = torch.matmul(sm, xv)\n",
    "            x = self.lnorm(att)\n",
    "            x = self.sm2(self.proj1(x))\n",
    "            x = x + skip    \n",
    "            x = self.transformer2(x, x_mask)\n",
    "            \n",
    "        logits = self.out_proj(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate new tokens given a context\n",
    "\n",
    "        Note: for apple MPS, top_k is limited max 16 vor older torchs! ((01/2023) implementation limitation)\n",
    "        See: https://github.com/pytorch/pytorch/issues/78915\n",
    "        Solved in: https://github.com/pytorch/pytorch/pull/94639 (03/2023)\n",
    "\n",
    "        :param idx: the context (B,T) tensor of indices\n",
    "        :param max_new_tokens: the maximum number of tokens to generate\n",
    "        :param temperature: the temperature to use for sampling\n",
    "        :param top_k: the number of top tokens to consider\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last sequence_len tokens\n",
    "            idx_cond = idx[:, -self.sequence_len :]\n",
    "            # print(idx_cond.shape)\n",
    "            # get the predictions\n",
    "            logits = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply temperature\n",
    "            if temperature != 1.0 and temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QnMCWf5AZn1-"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(device):\n",
    "    # XXX: this does take data for train and val from SAME pool!\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(params['test_iterations'])\n",
    "        for k in range(params['test_iterations']):\n",
    "            # if k % (params['test_iterations']/10 + 1) == 0:\n",
    "            #     print(\".\", end=\"\", flush=True)\n",
    "            X, Y = get_torch_batch(td, params['batch_size'], device, split)\n",
    "            logits = model(X)\n",
    "            loss = get_loss(logits, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    print(\"\\r\", end=\"\", flush=True)\n",
    "    mloss = (out['train']+out['val'])/2.0\n",
    "    return mloss\n",
    "\n",
    "def generate_sample(td, device, prompt=' ', toks=100, state=None, temperature=1.0, top_k=None, pad=True, with_beam=True):\n",
    "    if with_beam is True:\n",
    "        txt = model.generate_with_beam(model,td,prompt,toks, temperature=temperature, top_k=top_k, beam_width=5)\n",
    "    else:\n",
    "        model.eval()\n",
    "        if pad is True:\n",
    "            while len(prompt)<params['sequence_len']*4:\n",
    "                if len(prompt)==params['sequence_len']*4-1:\n",
    "                    prompt = '\\n' + prompt\n",
    "                else:\n",
    "                    prompt = ' ' + prompt\n",
    "        context = torch.tensor([td.encode(prompt)]).to(device)\n",
    "        answer = model.generate(context, max_new_tokens=toks, temperature=temperature, top_k=top_k)\n",
    "        txt = td.decode(answer[0].tolist())\n",
    "    # Identify memorisation of text by highlighting verbatim quotes from sources\n",
    "    # that are longer than 10 chars. HTML colorcoded output for source identification:\n",
    "    td.source_highlight(txt, min_quote_size=10, dark_mode=False, display_ref_anchor=False)\n",
    "    model.train()\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "N2uWm6CTC8kT"
   },
   "outputs": [],
   "source": [
    "# @torch.jit.script\n",
    "# @torch.compile\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def get_loss(logits, yb):\n",
    "    output_flat = logits.reshape(-1, params['vocab_size'])\n",
    "    # output_flat = logits.view(-1, params['vocab_size'])\n",
    "    # print(output_flat.shape)\n",
    "    ybr = yb.reshape(-1)\n",
    "    # print(ybr.shape)\n",
    "    loss = criterion(output_flat, ybr)\n",
    "    return loss\n",
    "    \n",
    "def do_train_step(xb, yb, device, state=None):\n",
    "    model.train()\n",
    "    logits = model(xb)\n",
    "    loss = get_loss(logits, yb)\n",
    "    \n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), params['grad_clip']).cpu()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_tu_session():\n",
    "    if indra_avail is True:\n",
    "        with open('indra_creds.json', 'r') as f:\n",
    "            creds = json.load(f)\n",
    "            tu = TrainUtils(indra_server_profile_name='default', username=creds['username'], password=creds['password'])\n",
    "    else:\n",
    "        tu = TrainUtils()\n",
    "    tu.train_session_start(model_name=model_name, model_description=\"Torch-poet tests\", model_version=1, model_params=params, indra_subdomain=\"torch_poet/first_tests/1\", status_string_size=110)\n",
    "    return tu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(optim, n_iter, warmup, max_lr, decay, min_lr):\n",
    "    if n_iter<warmup and warmup>0:\n",
    "        lr = (n_iter+1)/warmup*max_lr\n",
    "    elif n_iter<warmup+decay and decay>0:\n",
    "        i = n_iter-warmup\n",
    "        lr = (decay-i)/decay*(max_lr-min_lr)+min_lr\n",
    "    else:\n",
    "        lr = min_lr\n",
    "\n",
    "    for g in optim.param_groups:\n",
    "        g['lr'] = lr\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "aZpMI7_iMdR6",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_utils):\n",
    "    global start_iter\n",
    "    dt0 = time.time()\n",
    "    sdt = datetime.datetime.now(tz=local_timezone).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"training, start at {sdt}...\")\n",
    "    gen_id = 0\n",
    "    last_print=0\n",
    "    iter_bench = 1\n",
    "    tu = train_utils\n",
    "    lr = params['learning_rate']\n",
    "    # current_loss = estimate_loss(device)\n",
    "    inputs = [\"What is the difference between good and evil? The difference \", \"How did everything come into existence? The origin \", \"What was at the beginning of time? Time itself \", \"How are physics, quantum-mechanics and consciousness related? The relation between \", \"How to attain complete self-awareness? Complete \", \"What is the nature of reality? The nature \", \"How be a good human being? A human \"]\n",
    "    for iter in range(start_iter, params['max_iterations']):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if (iter + 1) % params['sample_every_n_iterations'] == 0 or iter == params['max_iterations'] - 1:\n",
    "            dt = time.time()\n",
    "            print(f\"\\rloss eval\", end=\"\", flush=True)\n",
    "            current_loss = estimate_loss(device)\n",
    "            print(\n",
    "                f\"step {iter+1}: train loss {current_loss:.4f}, time {(dt-dt0)/iter_bench:.3f} sec/iter                       \"\n",
    "            )\n",
    "            iter_bench = 1\n",
    "            sdt = datetime.datetime.now(tz=local_timezone).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"Sample at {sdt}:\", flush=True)\n",
    "            for temperature in [0.75, 1.0, 1.1]:\n",
    "                print(f\"--------temperature: {temperature} ---------\")\n",
    "                prompt = inputs[gen_id%len(inputs)]\n",
    "                # print(f\"Prompt: {prompt}\")\n",
    "                # generate_sample(td=td, device=device, prompt=prompt, toks=params['sample_size'], temperature=temperature, top_k=10, with_beam=False)\n",
    "                print(f\"Prompt: {prompt}\")\n",
    "                generate_sample(td=td, device=device, prompt=prompt, toks=params['sample_size'], temperature=temperature, top_k=10, with_beam=True)\n",
    "            print(\"-------------------------------------------\")\n",
    "            gen_id += 1\n",
    "            dt0 = time.time()\n",
    "    \n",
    "        if params['lr_schedule'] is True:\n",
    "            lr = lr_schedule(optimizer, iter, params['warmup'], params['lr_max'], params['decay'], params['lr_min'])\n",
    "    \n",
    "        xb, yb = get_torch_batch(td, params['batch_size'], device, \"train\")\n",
    "        cur_loss, cur_norm = do_train_step(xb, yb, device=device)\n",
    "\n",
    "        \n",
    "        nt = time.time()\n",
    "        if (nt-last_print)>1:\n",
    "            rec = {\n",
    "                'epoch': iter/num_batches,\n",
    "                'batch': iter%params['sample_every_n_iterations'],\n",
    "                'num_batches': params['sample_every_n_iterations'],\n",
    "                'loss': cur_loss,\n",
    "                'learning_rate': lr,\n",
    "                'gradient_norm': cur_norm.item(),\n",
    "            }\n",
    "            status_string, record = train_utils.train_state(rec)\n",
    "            print(status_string, end=\"\\r\")\n",
    "            last_print=nt\n",
    "        \n",
    "        start_iter = iter\n",
    "        iter_bench += 1\n",
    "        if (iter+1)%params['save_every_n_iterations'] == 0:\n",
    "            MJ.save_checkpoint(params, model, optimizer, iter, current_loss, file_path=model_file_path, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training, start at 2025-02-28 16:33:37...\n",
      "step 1024: train loss 6.9965, time 0.145 sec/iter                       00051 grad_norm: 1.750 Sec/It: 0.143  \n",
      "Sample at 2025-02-28 16:36:16:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What is the difference between good and evil? The difference \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#edebd0;\">What is the difference between </span><span style=\"background-color:#edebd0;\">good and evil? </span><span style=\"background-color:#ebdef0;\">The difference </span> <span style=\"background-color:#edebd0;\"> the         </span><span style=\"background-color:#edebd0;\">the         </span><span style=\"background-color:#ebdef0;\">the                 </span><span style=\"background-color:#edebd0;\">                                  the</span><span style=\"background-color:#edebd0;\">                             the </span>  the   the   the   <span style=\"background-color:#ebdef0;\"> the                 </span><span style=\"background-color:#edebd0;\">                                    the</span><span style=\"background-color:#f6ddcc;\">         the </span>  the   the   the  <span style=\"background-color:#ebdef0;\"> the                 </span><span style=\"background-color:#edebd0;\">                                         the</span><span style=\"background-color:#edebd0;\">                                 the</span>   the   the   the  <span style=\"background-color:#edebd0;\"> the         </span><span style=\"background-color:#edebd0;\">the         </span><span style=\"background-color:#edebd0;\">the         </span><span style=\"background-color:#edebd0;\">the         </span><span style=\"background-color:#ebdef0;\">the                 </span><span style=\"background-color:#edebd0;\">                                    the</span><span style=\"background-color:#f6ddcc;\">         the </span> <span style=\"background-color:#ebdef0;\"> the                 </span>   "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#edebd0;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 2 of 3)</span>, <span style=\"background-color:#ebdef0;\">Immanuel Kant: The Metaphysical Elements of Ethics</span>, <span style=\"background-color:#f6ddcc;\">Arthur Schopenhauer: On the Fourfold Root of the Principle of Sufficient Reason and On the Will in Nature: Two Essays (revised edition)</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.0 ---------\n",
      "Prompt: What is the difference between good and evil? The difference \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#edebd0;\">What is the difference between </span><span style=\"background-color:#edebd0;\">good and evil? </span><span style=\"background-color:#ebdef0;\">The difference </span><span style=\"background-color:#f6ddcc;\">        the </span><span style=\"background-color:#edebd0;\">              the </span><span style=\"background-color:#f6ddcc;\">        the </span><span style=\"background-color:#d8daef;\">                                                             </span><span style=\"background-color:#ebdef0;\">the                 </span><span style=\"background-color:#edebd0;\">           the </span><span style=\"background-color:#edebd0;\">                                  the</span>    the   the    the   <span style=\"background-color:#edebd0;\"> the          </span><span style=\"background-color:#edebd0;\">the         </span>the    the  <span style=\"background-color:#ebdef0;\"> the                 </span><span style=\"background-color:#edebd0;\">                          the </span><span style=\"background-color:#edebd0;\">                     the </span><span style=\"background-color:#d8daef;\">                                                        </span><span style=\"background-color:#ebdef0;\">the                 </span><span style=\"background-color:#edebd0;\">                                           the</span><span style=\"background-color:#d8daef;\">                     </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#f6ddcc;\">Arthur Schopenhauer: On the Fourfold Root of the Principle of Sufficient Reason and On the Will in Nature: Two Essays (revised edition)</span>, <span style=\"background-color:#edebd0;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 2 of 3)</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span>, <span style=\"background-color:#ebdef0;\">Immanuel Kant: The Metaphysical Elements of Ethics</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.1 ---------\n",
      "Prompt: What is the difference between good and evil? The difference \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#edebd0;\">What is the difference between </span><span style=\"background-color:#edebd0;\">good and evil? </span><span style=\"background-color:#ebdef0;\">The difference </span><span style=\"background-color:#d8daef;\">t                </span>the  <span style=\"background-color:#ebdef0;\"> the               </span>the    the e the    the     the   the   the   the   the    the   the    the  <span style=\"background-color:#edebd0;\"> the         </span>the   the  <span style=\"background-color:#ebdef0;\"> the                 </span><span style=\"background-color:#edebd0;\">                        the </span><span style=\"background-color:#d8daef;\">                                                                 </span><span style=\"background-color:#edebd0;\">the         </span>the  <span style=\"background-color:#ebdef0;\"> the                 </span>    <span style=\"background-color:#ebdef0;\"> the               </span>the    the   the   <span style=\"background-color:#ebdef0;\"> the               </span><span style=\"background-color:#ebdef0;\">the                </span><span style=\"background-color:#edebd0;\">the         </span>the  <span style=\"background-color:#ebdef0;\"> the                </span><span style=\"background-color:#edebd0;\">the          </span>the    the  <span style=\"background-color:#ebdef0;\"> the                 </span><span style=\"background-color:#d8daef;\">          </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span>, <span style=\"background-color:#ebdef0;\">Immanuel Kant: The Metaphysical Elements of Ethics</span>, <span style=\"background-color:#edebd0;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 2 of 3)</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "step 2048: train loss 6.4830, time 0.142 sec/iter                       00102 grad_norm: 2.428 Sec/It: 0.143  \n",
      "Sample at 2025-02-28 16:39:04:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How did everything come into existence? The origin \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#d0ece7;\"> did everything </span><span style=\"background-color:#ebdef0;\">come into existence</span>?<span style=\"background-color:#ecf3cf;\"> The origin </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                                                            </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.0 ---------\n",
      "Prompt: How did everything come into existence? The origin \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#d0ece7;\"> did everything </span><span style=\"background-color:#ebdef0;\">come into existence</span>?<span style=\"background-color:#ecf3cf;\"> The origin </span><span style=\"background-color:#edebd0;\">            which </span><span style=\"background-color:#e5e8e8;\">      which </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                              </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#edebd0;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 2 of 3)</span>, <span style=\"background-color:#e5e8e8;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.1 ---------\n",
      "Prompt: How did everything come into existence? The origin \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#d0ece7;\"> did everything </span><span style=\"background-color:#ebdef0;\">come into existence</span>?<span style=\"background-color:#ecf3cf;\"> The origin </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                                                        </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "step 3072: train loss 6.2291, time 0.142 sec/iter                       00153 grad_norm: 2.195 Sec/It: 0.143  \n",
      "Sample at 2025-02-28 16:41:45:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What was at the beginning of time? Time itself \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\">What was at</span><span style=\"background-color:#ebdef0;\"> the beginning of t</span>ime?<span style=\"background-color:#d8daef;\"> Time itself </span><span style=\"background-color:#d4efdf;\">of the whi</span><span style=\"background-color:#e2d7d5;\">ch                                                       </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                       </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d8daef;\">Lafcadio Hearn: Gleanings in Buddha-Fields</span>, <span style=\"background-color:#d4efdf;\">Friedrich Nietzsche: The Joyful Wisdom</span>, <span style=\"background-color:#e2d7d5;\">Immanuel Kant: Kant's gesammelte Schriften</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.0 ---------\n",
      "Prompt: What was at the beginning of time? Time itself \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\">What was at</span><span style=\"background-color:#ebdef0;\"> the beginning of t</span>ime?<span style=\"background-color:#d8daef;\"> Time itself i</span>n<span style=\"background-color:#d6eaf8;\"> the which </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                                                </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d8daef;\">Lafcadio Hearn: Gleanings in Buddha-Fields</span>, <span style=\"background-color:#d6eaf8;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.1 ---------\n",
      "Prompt: What was at the beginning of time? Time itself \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\">What was at</span><span style=\"background-color:#ebdef0;\"> the beginning of t</span>ime?<span style=\"background-color:#d8daef;\"> Time itself </span><span style=\"background-color:#d4efdf;\">of the whi</span>ch<span style=\"background-color:#edebd0;\"> which                                 </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">          </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d8daef;\">Lafcadio Hearn: Gleanings in Buddha-Fields</span>, <span style=\"background-color:#d4efdf;\">Friedrich Nietzsche: The Joyful Wisdom</span>, <span style=\"background-color:#edebd0;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 2 of 3)</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "step 4096: train loss 5.9733, time 0.142 sec/iter                       00200 grad_norm: 4.342 Sec/It: 0.143  \n",
      "Sample at 2025-02-28 16:44:27:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How are physics, quantum-mechanics and consciousness related? The relation between \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#ebdef0;\"> are physic</span><span style=\"background-color:#ebdef0;\">s, quantum</span>-<span style=\"background-color:#d6dbdf;\">mechanics and </span><span style=\"background-color:#ebdef0;\">consciousness relate</span>d?<span style=\"background-color:#edebd0;\"> The relation between</span> existeing thefore ing th<span style=\"background-color:#ebdef0;\">efore though</span><span style=\"background-color:#ebdef0;\"> which whi</span><span style=\"background-color:#d4e6f1;\">ch which w</span><span style=\"background-color:#eadbd8;\">hich which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#edebd0;\">      which w</span><span style=\"background-color:#eadbd8;\">hich which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#edebd0;\">which       </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#e5e8e8;\">      which </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                                            </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Arthur Schopenhauer: The Basis of Morality</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span>, <span style=\"background-color:#d4e6f1;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#eadbd8;\">unknown: The Vedanta-Sutras with the Commentary by Sankaracarya</span>, <span style=\"background-color:#edebd0;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 2 of 3)</span>, <span style=\"background-color:#e5e8e8;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.0 ---------\n",
      "Prompt: How are physics, quantum-mechanics and consciousness related? The relation between \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#ebdef0;\"> are physic</span><span style=\"background-color:#ebdef0;\">s, quantum</span>-<span style=\"background-color:#d6dbdf;\">mechanics and </span><span style=\"background-color:#ebdef0;\">consciousness relate</span>d?<span style=\"background-color:#edebd0;\"> The relation between</span> existeing th<span style=\"background-color:#eadbd8;\">every existe</span>existeexisteexisteexiste<span style=\"background-color:#edebd0;\">ed to existe</span><span style=\"background-color:#d0ece7;\">efore exist</span>e<span style=\"background-color:#d0ece7;\">efore exist</span>e<span style=\"background-color:#d0ece7;\">efore exist</span>e<span style=\"background-color:#ebdef0;\">efore though</span><span style=\"background-color:#edebd0;\"> which       </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#edebd0;\">which       </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#edebd0;\">      which w</span><span style=\"background-color:#eadbd8;\">hich which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#edebd0;\">which       </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#edebd0;\">which       </span><span style=\"background-color:#edebd0;\">which       </span><span style=\"background-color:#edebd0;\">which                               </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Arthur Schopenhauer: The Basis of Morality</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: The Critique of Practical Reason</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d0ece7;\">Immanuel Kant: Kant's Prolegomena</span>, <span style=\"background-color:#edebd0;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 2 of 3)</span>, <span style=\"background-color:#eadbd8;\">unknown: The Vedanta-Sutras with the Commentary by Sankaracarya</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.1 ---------\n",
      "Prompt: How are physics, quantum-mechanics and consciousness related? The relation between \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#ebdef0;\"> are physic</span><span style=\"background-color:#ebdef0;\">s, quantum</span>-<span style=\"background-color:#d6dbdf;\">mechanics and </span><span style=\"background-color:#ebdef0;\">consciousness relate</span>d?<span style=\"background-color:#edebd0;\"> The relation between</span> existeing thexi<span style=\"background-color:#ecf3cf;\">ste which </span><span style=\"background-color:#edebd0;\">            which </span><span style=\"background-color:#edebd0;\">      which w</span><span style=\"background-color:#eadbd8;\">hich which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#edebd0;\">      which w</span><span style=\"background-color:#eadbd8;\">hich which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#e5e8e8;\">      which </span><span style=\"background-color:#e5e8e8;\">      which </span><span style=\"background-color:#d8daef;\">                                                </span><span style=\"background-color:#edebd0;\">which       </span><span style=\"background-color:#edebd0;\">which                                 </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                                                      </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Arthur Schopenhauer: The Basis of Morality</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#edebd0;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 2 of 3)</span>, <span style=\"background-color:#eadbd8;\">unknown: The Vedanta-Sutras with the Commentary by Sankaracarya</span>, <span style=\"background-color:#e5e8e8;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "step 5120: train loss 5.7695, time 0.143 sec/iter                       00198 grad_norm: 5.157 Sec/It: 0.143  \n",
      "Sample at 2025-02-28 16:47:11:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How to attain complete self-awareness? Complete \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "H<span style=\"background-color:#fdebd0;\">ow to attain </span><span style=\"background-color:#d6dbdf;\">complete self-a</span>wareness?<span style=\"background-color:#ecf3cf;\"> Complete </span>of the of th<span style=\"background-color:#f6ddcc;\">eason, of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#ecf3cf;\">rtain of the</span><span style=\"background-color:#d0ece7;\">ason, that </span>of the of th<span style=\"background-color:#f6ddcc;\">eason, of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#d0ece7;\">ason, that </span><span style=\"background-color:#ecf3cf;\">of the tha</span><span style=\"background-color:#ebdef0;\">t of the th</span><span style=\"background-color:#d4e6f1;\">at of the o</span>f th<span style=\"background-color:#f6ddcc;\">eason, of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#ecf3cf;\">rtain of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#d0ece7;\">ason, that </span>of the of th<span style=\"background-color:#f6ddcc;\">eason, of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span>d the of th<span style=\"background-color:#f6ddcc;\">eason, of the</span><span style=\"background-color:#ecf3cf;\">rtain of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#ecf3cf;\">rtain of the</span><span style=\"background-color:#ecf3cf;\">rtain of the</span><span style=\"background-color:#ecf3cf;\">rtain of the</span><span style=\"background-color:#f6ddcc;\">ason, of the</span><span style=\"background-color:#d0ece7;\">ason, that </span><span style=\"background-color:#edebd0;\">of the        </span>     "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#fdebd0;\">Friedrich Nietzsche: On the Future of our Educational Institutions</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#f6ddcc;\">Arthur Schopenhauer: On the Fourfold Root of the Principle of Sufficient Reason and On the Will in Nature: Two Essays (revised edition)</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d0ece7;\">Immanuel Kant: Kant's Prolegomena</span>, <span style=\"background-color:#d4e6f1;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#edebd0;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 2 of 3)</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.0 ---------\n",
      "Prompt: How to attain complete self-awareness? Complete \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "H<span style=\"background-color:#fdebd0;\">ow to attain </span><span style=\"background-color:#d6dbdf;\">complete self-a</span>wareness?<span style=\"background-color:#f6ddcc;\"> Complete in </span><span style=\"background-color:#ebdef0;\">the                 </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                                      </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#fdebd0;\">Friedrich Nietzsche: On the Future of our Educational Institutions</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#f6ddcc;\">Arthur Schopenhauer: On the Fourfold Root of the Principle of Sufficient Reason and On the Will in Nature: Two Essays (revised edition)</span>, <span style=\"background-color:#ebdef0;\">Immanuel Kant: The Metaphysical Elements of Ethics</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.1 ---------\n",
      "Prompt: How to attain complete self-awareness? Complete \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "H<span style=\"background-color:#fdebd0;\">ow to attain </span><span style=\"background-color:#d6dbdf;\">complete self-a</span>wareness?<span style=\"background-color:#ecf3cf;\"> Complete </span><span style=\"background-color:#edebd0;\">of the        </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                                               </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#fdebd0;\">Friedrich Nietzsche: On the Future of our Educational Institutions</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#edebd0;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 2 of 3)</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "step 6144: train loss 5.6477, time 0.142 sec/iter                       00196 grad_norm: 4.871 Sec/It: 0.143  \n",
      "Sample at 2025-02-28 16:49:54:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What is the nature of reality? The nature \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#d6eaf8;\">What is the nature of </span>re<span style=\"background-color:#ecf3cf;\">ality? The </span><span style=\"background-color:#e5e8e8;\">nature                                                     </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                    </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d6eaf8;\">Daisetz Teitaro Suzuki: Outlines of Mahayana Buddhism</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#e5e8e8;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.0 ---------\n",
      "Prompt: What is the nature of reality? The nature \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#d6eaf8;\">What is the nature of </span>re<span style=\"background-color:#ecf3cf;\">ality? The </span><span style=\"background-color:#d4efdf;\">nature which </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                                                                  </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d6eaf8;\">Daisetz Teitaro Suzuki: Outlines of Mahayana Buddhism</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#d4efdf;\">Friedrich Nietzsche: The Joyful Wisdom</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.1 ---------\n",
      "Prompt: What is the nature of reality? The nature \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#d6eaf8;\">What is the nature of </span>re<span style=\"background-color:#ecf3cf;\">ality? The </span><span style=\"background-color:#e5e8e8;\">nature                                                     </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                    </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d6eaf8;\">Daisetz Teitaro Suzuki: Outlines of Mahayana Buddhism</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#e5e8e8;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "step 7168: train loss 5.5089, time 0.142 sec/iter                       00194 grad_norm: 4.024 Sec/It: 0.142  \n",
      "Sample at 2025-02-28 16:52:36:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How be a good human being? A human \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#ecf3cf;\"> be a good </span><span style=\"background-color:#ebdef0;\">human being</span>? A<span style=\"background-color:#ebdef0;\"> human in the</span><span style=\"background-color:#edebd0;\"> of the same o</span>f th<span style=\"background-color:#ecf3cf;\">ertain of the</span><span style=\"background-color:#ecf3cf;\">rtain of the </span>s<span style=\"background-color:#ebdef0;\">ame of the</span>d wit<span style=\"background-color:#e5e8e8;\">hich in the t</span><span style=\"background-color:#d6eaf8;\">o the that</span> tion otion otion otion otion otion otion otion otion otion otion otion othe s<span style=\"background-color:#ebdef0;\">ation othe</span> sation otion otion otion otion otion otion otion otion otion otion othe sation otion otion otion othe sation otion otion otion otion otion otion othe s<span style=\"background-color:#ebdef0;\">ation othe</span> sation otion otion otion otion otion otion otion otion otion otion othe sation otion otion otion otion otion otion oto t<span style=\"background-color:#ebdef0;\">he of the </span>the tion otion otion otion otion otion otion otion otion otion ot"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#ebdef0;\">Charles Eliot: Hinduism And Buddhism, Vol. 2 of 3</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#e5e8e8;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#d6eaf8;\">Daisetz Teitaro Suzuki: Outlines of Mahayana Buddhism</span>, <span style=\"background-color:#ebdef0;\">Arthur Schopenhauer: The Basis of Morality</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.0 ---------\n",
      "Prompt: How be a good human being? A human \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#ecf3cf;\"> be a good </span><span style=\"background-color:#ebdef0;\">human being</span>? A <span style=\"background-color:#eadbd8;\">human the </span>sato t<span style=\"background-color:#e5e8e8;\">he of the same </span><span style=\"background-color:#edebd0;\">of the same o</span><span style=\"background-color:#edebd0;\">f the same o</span>f the of thich i to thed withich i that tion otion otion othe sation otion otion otion otion oto t<span style=\"background-color:#e5e8e8;\">he of the same </span><span style=\"background-color:#edebd0;\">of the same o</span>f th<span style=\"background-color:#ecf3cf;\">ertain of the</span><span style=\"background-color:#ecf3cf;\">rtain of the </span>s<span style=\"background-color:#ebdef0;\">ame of the</span>d withat th<span style=\"background-color:#ecf3cf;\">ertain of the</span><span style=\"background-color:#ecf3cf;\">rtain of the </span>of thich i of th<span style=\"background-color:#ecf3cf;\">ertain of the</span><span style=\"background-color:#ecf3cf;\">rtain of the </span>of thich <span style=\"background-color:#d8daef;\">i that of the </span><span style=\"background-color:#d6eaf8;\">that of the th</span><span style=\"background-color:#d6eaf8;\">at of the th</span><span style=\"background-color:#d4e6f1;\">at of the o</span><span style=\"background-color:#edebd0;\">f the same o</span>f th<span style=\"background-color:#d4e6f1;\">e of that the</span><span style=\"background-color:#ecf3cf;\">rtain of the </span><span style=\"background-color:#eadbd8;\">of that the o</span>f thich i<span style=\"background-color:#d4efdf;\"> of the of</span> thich i<span style=\"background-color:#edebd0;\"> of the same o</span>f th<span style=\"background-color:#d4e6f1;\">e of that the</span><span style=\"background-color:#d4e6f1;\"> of that the</span><span style=\"background-color:#d4e6f1;\"> of that the</span><span style=\"background-color:#d4e6f1;\"> of that the</span> of thich i the sof the of the"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#eadbd8;\">unknown: The Vedanta-Sutras with the Commentary by Sankaracarya</span>, <span style=\"background-color:#e5e8e8;\">Vatsyayana: The Kama Sutra of Vatsyayana</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d8daef;\">Charles Eliot: Hinduism and Buddhism, An Historical Sketch, Vol. 3 of 3</span>, <span style=\"background-color:#d6eaf8;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span>, <span style=\"background-color:#d4e6f1;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#d4e6f1;\">Friedrich Nietzsche: Thoughts Out of Season, Part 2</span>, <span style=\"background-color:#d4efdf;\">Friedrich Nietzsche: The Joyful Wisdom</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.1 ---------\n",
      "Prompt: How be a good human being? A human \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#ecf3cf;\"> be a good </span><span style=\"background-color:#ebdef0;\">human being</span>? A<span style=\"background-color:#ebdef0;\"> human in the</span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                                                                   </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#ebdef0;\">Charles Eliot: Hinduism And Buddhism, Vol. 2 of 3</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "step 8192: train loss 5.4827, time 0.142 sec/iter                       00192 grad_norm: 4.934 Sec/It: 0.143  \n",
      "Sample at 2025-02-28 16:55:29:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What is the difference between good and evil? The difference \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#edebd0;\">What is the difference between </span><span style=\"background-color:#edebd0;\">good and evil? </span><span style=\"background-color:#ebdef0;\">The difference </span><span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>ther<span style=\"background-color:#d6eaf8;\">fect which </span>of th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#ebdef0;\">eption of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#d4efdf;\">fect which w</span><span style=\"background-color:#ebdef0;\">hich of the</span><span style=\"background-color:#ebdef0;\">ption of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>d <span style=\"background-color:#d0ece7;\">by of the o</span>f th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#ebdef0;\">eption of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#d6eaf8;\">fect which </span>of ther<span style=\"background-color:#d4efdf;\">fect which w</span><span style=\"background-color:#ebdef0;\">hich of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#d4efdf;\">fect which w</span><span style=\"background-color:#ebdef0;\">hich of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#d6eaf8;\">fect which </span>of ther<span style=\"background-color:#d6eaf8;\">fect which </span>of th<span style=\"background-color:#d4e6f1;\">erfect of </span>the"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d4e6f1;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#d6eaf8;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span>, <span style=\"background-color:#d4efdf;\">Immanuel Kant: The Critique of Pure Reason</span>, <span style=\"background-color:#d0ece7;\">Immanuel Kant: Kant's Prolegomena</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.0 ---------\n",
      "Prompt: What is the difference between good and evil? The difference \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#edebd0;\">What is the difference between </span><span style=\"background-color:#edebd0;\">good and evil? </span><span style=\"background-color:#e2d7d5;\">The difference of the wh</span><span style=\"background-color:#eadbd8;\">ich which </span>of th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>ther<span style=\"background-color:#d6eaf8;\">fect which </span>of th<span style=\"background-color:#d4e6f1;\">erfect of </span>ther<span style=\"background-color:#d6eaf8;\">fect which </span>of th<span style=\"background-color:#d4e6f1;\">erfect of </span>ther<span style=\"background-color:#d4efdf;\">fect which w</span><span style=\"background-color:#ebdef0;\">hich of the</span>r<span style=\"background-color:#d6eaf8;\">fect which </span>of th<span style=\"background-color:#d4e6f1;\">erfect of </span>ther<span style=\"background-color:#d4efdf;\">fect which w</span><span style=\"background-color:#eadbd8;\">hich which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span>of ther<span style=\"background-color:#d6eaf8;\">fect which </span>of th<span style=\"background-color:#ebdef0;\">eption of the</span>r<span style=\"background-color:#d4e6f1;\">fect which the</span><span style=\"background-color:#fdebd0;\">ir which o</span>f thed <span style=\"background-color:#d0ece7;\">by of the o</span>f th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#edebd0;\">erfect that </span>to the of th<span style=\"background-color:#d4e6f1;\">erfect of </span>ther<span style=\"background-color:#d6eaf8;\">fect which </span>of th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>ther<span style=\"background-color:#d6eaf8;\">fect which </span>of th<span style=\"background-color:#ebdef0;\">erfect the</span>ir of th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>ther<span style=\"background-color:#d6eaf8;\">fect which </span>of th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>ther<span style=\"background-color:#d6eaf8;\">fect which </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#e2d7d5;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 3 of 3)</span>, <span style=\"background-color:#eadbd8;\">unknown: The Vedanta-Sutras with the Commentary by Sankaracarya</span>, <span style=\"background-color:#d4e6f1;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#d6eaf8;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span>, <span style=\"background-color:#d4efdf;\">Immanuel Kant: The Critique of Pure Reason</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#fdebd0;\">Friedrich Nietzsche: On the Future of our Educational Institutions</span>, <span style=\"background-color:#d0ece7;\">Immanuel Kant: Kant's Prolegomena</span>, <span style=\"background-color:#edebd0;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 2 of 3)</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.1 ---------\n",
      "Prompt: What is the difference between good and evil? The difference \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#edebd0;\">What is the difference between </span><span style=\"background-color:#edebd0;\">good and evil? </span><span style=\"background-color:#ebdef0;\">The difference l</span>re<span style=\"background-color:#d4efdf;\">ady of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#d4efdf;\">fect which w</span><span style=\"background-color:#eadbd8;\">hich which </span>of th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#e2d7d5;\">erfect is the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#ebdef0;\">fect of the</span>r<span style=\"background-color:#d4efdf;\">fect which w</span><span style=\"background-color:#eadbd8;\">hich which </span>of th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>ther<span style=\"background-color:#d4efdf;\">fect which w</span><span style=\"background-color:#eadbd8;\">hich which </span><span style=\"background-color:#ebdef0;\">which of the</span>r<span style=\"background-color:#d4efdf;\">fect which w</span><span style=\"background-color:#eadbd8;\">hich which </span>of th<span style=\"background-color:#d4e6f1;\">erfect of </span>th<span style=\"background-color:#d4e6f1;\">erfect of </span>therf<span style=\"background-color:#d4e6f1;\">ect                                           </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                                      </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#ebdef0;\">Charles Eliot: Hinduism And Buddhism, Vol. 2 of 3</span>, <span style=\"background-color:#d4efdf;\">Friedrich Nietzsche: The Joyful Wisdom</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d4efdf;\">Immanuel Kant: The Critique of Pure Reason</span>, <span style=\"background-color:#eadbd8;\">unknown: The Vedanta-Sutras with the Commentary by Sankaracarya</span>, <span style=\"background-color:#d4e6f1;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#e2d7d5;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 3 of 3)</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "step 9216: train loss 5.3657, time 0.143 sec/iter                       00191 grad_norm: 3.284 Sec/It: 0.143  \n",
      "Sample at 2025-02-28 16:58:17:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How did everything come into existence? The origin \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#d0ece7;\"> did everything </span><span style=\"background-color:#ebdef0;\">come into existence</span>?<span style=\"background-color:#ecf3cf;\"> The origin of the</span>d t<span style=\"background-color:#ebdef0;\">he of the </span>and <span style=\"background-color:#ecf3cf;\">ation of the of</span> t<span style=\"background-color:#d8daef;\">hed the of</span> <span style=\"background-color:#e2d7d5;\">the the at</span><span style=\"background-color:#ebdef0;\">ion of the of</span><span style=\"background-color:#ebdef0;\"> the that </span>of the and <span style=\"background-color:#ecf3cf;\">ation of the of</span> t<span style=\"background-color:#d8daef;\">hed the of</span><span style=\"background-color:#ebdef0;\"> the that </span>of t<span style=\"background-color:#d8daef;\">hed the of</span> the and <span style=\"background-color:#ecf3cf;\">ation of the of</span> t<span style=\"background-color:#d6eaf8;\">he and the</span><span style=\"background-color:#e2d7d5;\">re of the of</span><span style=\"background-color:#ebdef0;\"> the that </span><span style=\"background-color:#ebdef0;\">of the the</span> <span style=\"background-color:#ecf3cf;\">ation of the of</span> t<span style=\"background-color:#ebdef0;\">he of the </span>ot<span style=\"background-color:#ebdef0;\">he of the </span>oth<span style=\"background-color:#e5e8e8;\">e that of the th</span>at of t<span style=\"background-color:#d8daef;\">hed the of</span> t<span style=\"background-color:#d6eaf8;\">he and the</span><span style=\"background-color:#e2d7d5;\">re of the of</span><span style=\"background-color:#ebdef0;\"> the that </span>of the and <span style=\"background-color:#ebdef0;\">ation of the th</span><span style=\"background-color:#ecf3cf;\">at there o</span>f the of t<span style=\"background-color:#d8daef;\">hed the of</span> t<span style=\"background-color:#d8daef;\">hed the of</span><span style=\"background-color:#ebdef0;\"> the that </span><span style=\"background-color:#ebdef0;\">which of the o</span><span style=\"background-color:#ebdef0;\">f the othe</span> and <span style=\"background-color:#ecf3cf;\">ation of the of</span><span style=\"background-color:#ebdef0;\"> the that </span><span style=\"background-color:#ebdef0;\">of the the</span> <span style=\"background-color:#d4e6f1;\">ation of the whi</span>ch of t<span style=\"background-color:#d8daef;\">hed the of</span> t<span style=\"background-color:#ebdef0;\">he of the </span>othe of th"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#ecf3cf;\">Arthur Lillie: The Influence of Buddhism on Primitive Christianity</span>, <span style=\"background-color:#ebdef0;\">Arthur Schopenhauer: The Basis of Morality</span>, <span style=\"background-color:#ecf3cf;\">unknown: The Vedanta-Sutras with the Commentary by Ramanuja, attributed to Badarayana</span>, <span style=\"background-color:#d8daef;\">Charles Eliot: Hinduism and Buddhism, An Historical Sketch, Vol. 3 of 3</span>, <span style=\"background-color:#e2d7d5;\">Charles Eliot: Hinduism and Buddhism, Vol. 1 of 3</span>, <span style=\"background-color:#ebdef0;\">Charles Eliot: Hinduism And Buddhism, Vol. 2 of 3</span>, <span style=\"background-color:#ebdef0;\">Immanuel Kant: The Metaphysical Elements of Ethics</span>, <span style=\"background-color:#d6eaf8;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span>, <span style=\"background-color:#e5e8e8;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#d4e6f1;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.0 ---------\n",
      "Prompt: How did everything come into existence? The origin \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#d0ece7;\"> did everything </span><span style=\"background-color:#ebdef0;\">come into existence</span>?<span style=\"background-color:#ecf3cf;\"> The origin </span><span style=\"background-color:#ebdef0;\">which of the</span>d t<span style=\"background-color:#ebdef0;\">he of the </span><span style=\"background-color:#ebdef0;\">that of the</span>d t<span style=\"background-color:#ebdef0;\">he of the </span><span style=\"background-color:#ebdef0;\">of the othe</span><span style=\"background-color:#ebdef0;\"> of the othe</span><span style=\"background-color:#ebdef0;\"> that of the</span>d the of t<span style=\"background-color:#d8daef;\">hed the of</span> t<span style=\"background-color:#eadbd8;\">hed the wh</span><span style=\"background-color:#eadbd8;\">ich which </span><span style=\"background-color:#ecf3cf;\">of the tha</span><span style=\"background-color:#ebdef0;\">t of the th</span>at of t<span style=\"background-color:#d8daef;\">hed the of</span> t<span style=\"background-color:#d8daef;\">hed the of</span><span style=\"background-color:#ebdef0;\"> the that </span><span style=\"background-color:#ecf3cf;\">of the tha</span><span style=\"background-color:#e2d7d5;\">t of the of</span> the <span style=\"background-color:#d6eaf8;\">othed the </span>of t<span style=\"background-color:#d8daef;\">hed the of</span> t<span style=\"background-color:#d8daef;\">hed the of</span> t<span style=\"background-color:#d8daef;\">hed the of</span> t<span style=\"background-color:#d8daef;\">hed the of</span> t<span style=\"background-color:#ebdef0;\">he of the </span>ot<span style=\"background-color:#ebdef0;\">he of the </span>oth<span style=\"background-color:#e5e8e8;\">e that of the th</span><span style=\"background-color:#ebdef0;\">at of the an</span><span style=\"background-color:#ebdef0;\">d there of </span>t<span style=\"background-color:#ebdef0;\">he of the </span>t<span style=\"background-color:#ebdef0;\">he from the </span>ot<span style=\"background-color:#ebdef0;\">he of the </span>ot<span style=\"background-color:#ebdef0;\">he of the </span>oth<span style=\"background-color:#ebdef0;\">e that of the</span>d t<span style=\"background-color:#ebdef0;\">he of the </span><span style=\"background-color:#ecf3cf;\">and to the o</span>f the t<span style=\"background-color:#ebdef0;\">he from the </span>othe w<span style=\"background-color:#ebdef0;\">ith of the</span>d t<span style=\"background-color:#ebdef0;\">he of the </span><span style=\"background-color:#ebdef0;\">of the othe</span><span style=\"background-color:#eadbd8;\"> that of the the</span><span style=\"background-color:#ebdef0;\"> from the othe</span> and t"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#ebdef0;\">Arthur Schopenhauer: The Basis of Morality</span>, <span style=\"background-color:#d8daef;\">Charles Eliot: Hinduism and Buddhism, An Historical Sketch, Vol. 3 of 3</span>, <span style=\"background-color:#eadbd8;\">unknown: The Vedanta-Sutras with the Commentary by Sankaracarya</span>, <span style=\"background-color:#ebdef0;\">Immanuel Kant: The Metaphysical Elements of Ethics</span>, <span style=\"background-color:#e2d7d5;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 3 of 3)</span>, <span style=\"background-color:#d6eaf8;\">Sir Monier Monier-Williams: Buddhism, In its Connexion With Brahmanism and Hinduism, and In Its Contrast with Christianity</span>, <span style=\"background-color:#e5e8e8;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#ebdef0;\">V. FausbÃ¶ll: Buddhist birth stories; or, Jataka tales, Vol. 1</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: The Critique of Practical Reason</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.1 ---------\n",
      "Prompt: How did everything come into existence? The origin \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#d0ece7;\"> did everything </span><span style=\"background-color:#ebdef0;\">come into existence</span>?<span style=\"background-color:#ecf3cf;\"> The origin </span><span style=\"background-color:#e5e8e8;\">that of the wh</span><span style=\"background-color:#eadbd8;\">ich which </span><span style=\"background-color:#ebdef0;\">which of the</span>d the of t<span style=\"background-color:#d8daef;\">hed the of</span> t<span style=\"background-color:#ebdef0;\">he of the </span><span style=\"background-color:#d6eaf8;\">othed the </span><span style=\"background-color:#ebdef0;\">which of the</span>d the of th<span style=\"background-color:#ebdef0;\">ection which w</span><span style=\"background-color:#eadbd8;\">hich which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#ebdef0;\">which of the w</span><span style=\"background-color:#e2d7d5;\">ith of the t</span><span style=\"background-color:#d6eaf8;\">hat of the th</span><span style=\"background-color:#e2d7d5;\">at and the </span>ot<span style=\"background-color:#ebdef0;\">he of the </span>ot<span style=\"background-color:#ebdef0;\">he of the </span><span style=\"background-color:#d6eaf8;\">othed the </span><span style=\"background-color:#ebdef0;\">which of the th</span><span style=\"background-color:#d4e6f1;\">at of the o</span><span style=\"background-color:#ebdef0;\">f the othe</span>d the of t<span style=\"background-color:#d8daef;\">hed the of</span> t<span style=\"background-color:#d8daef;\">hed the of</span> t<span style=\"background-color:#eadbd8;\">hed the wh</span><span style=\"background-color:#ebdef0;\">ich of the o</span><span style=\"background-color:#ebdef0;\">f the othe</span><span style=\"background-color:#d4efdf;\"> that which of </span>th<span style=\"background-color:#e5e8e8;\">e that of the th</span><span style=\"background-color:#d4e6f1;\">at of the o</span><span style=\"background-color:#ebdef0;\">f the othe</span>d<span style=\"background-color:#d6eaf8;\"> the which </span>of t<span style=\"background-color:#d8daef;\">hed the of</span> t<span style=\"background-color:#d8daef;\">hed the of</span><span style=\"background-color:#ebdef0;\"> the that </span><span style=\"background-color:#ecf3cf;\">of the tha</span><span style=\"background-color:#f6ddcc;\">t which whi</span><span style=\"background-color:#d6eaf8;\">ch of the th</span><span style=\"background-color:#d6eaf8;\">at of the th</span>at of th<span style=\"background-color:#ebdef0;\">ection of the</span>d the of t<span style=\"background-color:#d8daef;\">hed the of</span> the of th"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#e5e8e8;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#eadbd8;\">unknown: The Vedanta-Sutras with the Commentary by Sankaracarya</span>, <span style=\"background-color:#d8daef;\">Charles Eliot: Hinduism and Buddhism, An Historical Sketch, Vol. 3 of 3</span>, <span style=\"background-color:#ebdef0;\">Arthur Schopenhauer: The Basis of Morality</span>, <span style=\"background-color:#d6eaf8;\">Sir Monier Monier-Williams: Buddhism, In its Connexion With Brahmanism and Hinduism, and In Its Contrast with Christianity</span>, <span style=\"background-color:#ebdef0;\">Charles Eliot: Hinduism And Buddhism, Vol. 2 of 3</span>, <span style=\"background-color:#e2d7d5;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 3 of 3)</span>, <span style=\"background-color:#d6eaf8;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span>, <span style=\"background-color:#ebdef0;\">V. FausbÃ¶ll: Buddhist birth stories; or, Jataka tales, Vol. 1</span>, <span style=\"background-color:#d4e6f1;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#d4efdf;\">Immanuel Kant: The Critique of Pure Reason</span>, <span style=\"background-color:#ebdef0;\">Immanuel Kant: The Metaphysical Elements of Ethics</span>, <span style=\"background-color:#f6ddcc;\">Archibald Scott: Buddhism and Christianity</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "step 10240: train loss 5.2779, time 0.142 sec/iter                       0189 grad_norm: 3.103 Sec/It: 0.143  \n",
      "Sample at 2025-02-28 17:01:05:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What was at the beginning of time? Time itself \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\">What was at</span><span style=\"background-color:#ebdef0;\"> the beginning of t</span>ime?<span style=\"background-color:#d8daef;\"> Time itself </span>gleich<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#e2d7d5;\">rnunft in de</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht durch g</span>leich<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>rselbe<span style=\"background-color:#eadbd8;\">nicht durch g</span>leich<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#d4e6e1;\">nicht denken </span>in de<span style=\"background-color:#eadbd8;\">nicht gleich</span>rselbe<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>rselbe<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>rselbe<span style=\"background-color:#eadbd8;\">nicht der<br></span>gleich<span style=\"background-color:#eadbd8;\">nicht gleich</span>n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d8daef;\">Lafcadio Hearn: Gleanings in Buddha-Fields</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#e2d7d5;\">Immanuel Kant: Kant's gesammelte Schriften</span>, <span style=\"background-color:#d4e6e1;\">Immanuel Kant: Kritik der reinen Vernunft (2nd Edition)</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.0 ---------\n",
      "Prompt: What was at the beginning of time? Time itself \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\">What was at</span><span style=\"background-color:#ebdef0;\"> the beginning of t</span>ime?<span style=\"background-color:#d8daef;\"> Time itself </span>t eine<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#d4e6e1;\">nicht denken </span>zu<br>hÃ¶chs<span style=\"background-color:#d4e6e1;\">t einen ein</span>en eine<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>rselbe<span style=\"background-color:#d8daef;\">nicht durch d</span>ur<span style=\"background-color:#d8daef;\">ch durch d</span><span style=\"background-color:#fdebd0;\">urch daÃ d</span><span style=\"background-color:#eadbd8;\">urch gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>rselbe<span style=\"background-color:#eadbd8;\">nicht daÃ </span>gleich<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>n eine<span style=\"background-color:#eadbd8;\">nicht gleich</span>n eine<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#e2d7d5;\">rnunft in de</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>rselbe<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht durch g</span>leich<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>n eine<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>rselbe<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d8daef;\">Lafcadio Hearn: Gleanings in Buddha-Fields</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d4e6e1;\">Immanuel Kant: Kritik der reinen Vernunft (2nd Edition)</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span>, <span style=\"background-color:#fdebd0;\">Immanuel Kant: Kant's gesammelte Schriften</span>, <span style=\"background-color:#e2d7d5;\">Immanuel Kant: Kant's gesammelte Schriften</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.1 ---------\n",
      "Prompt: What was at the beginning of time? Time itself \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\">What was at</span><span style=\"background-color:#ebdef0;\"> the beginning of t</span>ime?<span style=\"background-color:#d8daef;\"> Time itself </span>t eine<span style=\"background-color:#d4e6e1;\">nicht denken </span>und u<span style=\"background-color:#d8daef;\">nicht durch d</span><span style=\"background-color:#eadbd8;\">urch gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>r eine<span style=\"background-color:#eadbd8;\">nicht gleich</span>nicht ch<span style=\"background-color:#eadbd8;\">tet in den</span><span style=\"background-color:#eadbd8;\">icht durch g</span>leich<span style=\"background-color:#eadbd8;\">nicht durch g</span>leich<span style=\"background-color:#eadbd8;\">nicht gleich</span>rselbe<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#e2d7d5;\">rnunft in de</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>rselbe<span style=\"background-color:#eadbd8;\">nicht durch g</span>leich<span style=\"background-color:#eadbd8;\">nicht gleich</span>n eine<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">rselben eine</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">rselben eine</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#d8daef;\">rnunft und </span>u<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>rselbe<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>rselbe<span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span><span style=\"background-color:#eadbd8;\">nicht gleich</span>n einen"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d8daef;\">Lafcadio Hearn: Gleanings in Buddha-Fields</span>, <span style=\"background-color:#d4e6e1;\">Immanuel Kant: Kritik der reinen Vernunft (2nd Edition)</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#eadbd8;\">Karl Eugen Neumann: Die Reden Gotamo Buddhos</span>, <span style=\"background-color:#e2d7d5;\">Immanuel Kant: Kant's gesammelte Schriften</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "step 11264: train loss 5.2029, time 0.142 sec/iter                       0187 grad_norm: 3.922 Sec/It: 0.142  \n",
      "Sample at 2025-02-28 17:03:52:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How are physics, quantum-mechanics and consciousness related? The relation between \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#ebdef0;\"> are physic</span><span style=\"background-color:#ebdef0;\">s, quantum</span>-<span style=\"background-color:#d6dbdf;\">mechanics and </span><span style=\"background-color:#ebdef0;\">consciousness relate</span>d?<span style=\"background-color:#edebd0;\"> The relation between</span> W<span style=\"background-color:#eadbd8;\">ill          </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span>   "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Arthur Schopenhauer: The Basis of Morality</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span>, <span style=\"background-color:#eadbd8;\">Karl Eugen Neumann: Die Reden Gotamo Buddhos</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.0 ---------\n",
      "Prompt: How are physics, quantum-mechanics and consciousness related? The relation between \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#ebdef0;\"> are physic</span><span style=\"background-color:#ebdef0;\">s, quantum</span>-<span style=\"background-color:#d6dbdf;\">mechanics and </span><span style=\"background-color:#ebdef0;\">consciousness relate</span>d?<span style=\"background-color:#edebd0;\"> The relation between</span> MÃ¶nchexistider Erder Erder Erder Erder Er<span style=\"background-color:#eadbd8;\">dieser         </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#d8daef;\">                                       </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Arthur Schopenhauer: The Basis of Morality</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Der Wille zur Macht</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.1 ---------\n",
      "Prompt: How are physics, quantum-mechanics and consciousness related? The relation between \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#ebdef0;\"> are physic</span><span style=\"background-color:#ebdef0;\">s, quantum</span>-<span style=\"background-color:#d6dbdf;\">mechanics and </span><span style=\"background-color:#ebdef0;\">consciousness relate</span>d?<span style=\"background-color:#edebd0;\"> The relation between</span> Wohlen<span style=\"background-color:#edebd0;\">, so             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                             </span><span style=\"background-color:#eadbd8;\">                                                                       </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Arthur Schopenhauer: The Basis of Morality</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span>, <span style=\"background-color:#edebd0;\">Karl Eugen Neumann: Die Reden Gotamo Buddhos</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: Die Religion innerhalb der Grenzen der bloÃen Vernunft</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "step 12288: train loss 5.1483, time 0.142 sec/iter                       0185 grad_norm: 4.009 Sec/It: 0.143  \n",
      "Sample at 2025-02-28 17:06:34:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How to attain complete self-awareness? Complete \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "H<span style=\"background-color:#fdebd0;\">ow to attain </span><span style=\"background-color:#d6dbdf;\">complete self-a</span>wareness?<span style=\"background-color:#ecf3cf;\"> Complete and </span><span style=\"background-color:#d0ece7;\">the same that t</span><span style=\"background-color:#d6eaf8;\">o the that</span><span style=\"background-color:#d6eaf8;\"> to the that</span><span style=\"background-color:#d6eaf8;\"> to the that</span><span style=\"background-color:#d6eaf8;\"> to the that</span><span style=\"background-color:#d6eaf8;\"> to the that</span><span style=\"background-color:#d6eaf8;\"> to the that</span><span style=\"background-color:#e2d7d5;\"> to the of</span><span style=\"background-color:#eadbd8;\"> the same that t</span><span style=\"background-color:#d6eaf8;\">o the that</span><span style=\"background-color:#d6eaf8;\"> to the that</span><span style=\"background-color:#d6eaf8;\"> to the that</span><span style=\"background-color:#d6eaf8;\"> to the that</span><span style=\"background-color:#ecf3cf;\"> to the to</span><span style=\"background-color:#ebdef0;\"> therefore sa</span><span style=\"background-color:#ecf3cf;\">me that to </span>th<span style=\"background-color:#edebd0;\">e that to the </span><span style=\"background-color:#edebd0;\">that to the th</span><span style=\"background-color:#edebd0;\">at to the th</span><span style=\"background-color:#edebd0;\">at to the th</span><span style=\"background-color:#edebd0;\">at to the th</span><span style=\"background-color:#ebdef0;\">at to the of</span><span style=\"background-color:#ebdef0;\"> therefore sa</span><span style=\"background-color:#ecf3cf;\">me that to </span>th<span style=\"background-color:#edebd0;\">e that to the </span>t<span style=\"background-color:#ecf3cf;\">o therefore</span>reforereforereforereforereforereforereforerefor<span style=\"background-color:#ebdef0;\">erefore sa</span><span style=\"background-color:#ecf3cf;\">me that to </span>th<span style=\"background-color:#edebd0;\">e that to the </span><span style=\"background-color:#edebd0;\">that to the t</span><span style=\"background-color:#ecf3cf;\">o therefore</span>reforerefor<span style=\"background-color:#ebdef0;\">erefore sa</span><span style=\"background-color:#ecf3cf;\">me that to </span>th<span style=\"background-color:#edebd0;\">e that to the </span><span style=\"background-color:#d0ece7;\">that to the o</span><span style=\"background-color:#ebdef0;\">f therefore</span>reforereforerefore"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#fdebd0;\">Friedrich Nietzsche: On the Future of our Educational Institutions</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#d6eaf8;\">Daisetz Teitaro Suzuki: Outlines of Mahayana Buddhism</span>, <span style=\"background-color:#e2d7d5;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 3 of 3)</span>, <span style=\"background-color:#eadbd8;\">unknown: The Vedanta-Sutras with the Commentary by Sankaracarya</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#ebdef0;\">V. FausbÃ¶ll: Buddhist birth stories; or, Jataka tales, Vol. 1</span>, <span style=\"background-color:#d0ece7;\">Immanuel Kant: Kant's Prolegomena</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.0 ---------\n",
      "Prompt: How to attain complete self-awareness? Complete \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "H<span style=\"background-color:#fdebd0;\">ow to attain </span><span style=\"background-color:#d6dbdf;\">complete self-a</span>wareness?<span style=\"background-color:#ecf3cf;\"> Complete a</span>t the t<span style=\"background-color:#ecf3cf;\">o therefore</span>refor<span style=\"background-color:#ebdef0;\">erefore sa</span><span style=\"background-color:#ecf3cf;\">me that to </span>th<span style=\"background-color:#edebd0;\">e that to the </span><span style=\"background-color:#d0ece7;\">that to the o</span><span style=\"background-color:#ebdef0;\">f therefore</span>refor<span style=\"background-color:#edebd0;\">e same of </span><span style=\"background-color:#d0ece7;\">the same that t</span><span style=\"background-color:#d6eaf8;\">o the that</span><span style=\"background-color:#d6eaf8;\"> to the that</span><span style=\"background-color:#e2d7d5;\"> to the of</span><span style=\"background-color:#ebdef0;\"> therefore</span>reforereforereforereforereforereforereforereforerefor<span style=\"background-color:#ebdef0;\">erefore sa</span><span style=\"background-color:#d0ece7;\">me to the same w</span><span style=\"background-color:#eadbd8;\">hich which </span><span style=\"background-color:#edebd0;\">of the same o</span><span style=\"background-color:#edebd0;\">f the same o</span><span style=\"background-color:#edebd0;\">f the same o</span><span style=\"background-color:#edebd0;\">f the same o</span><span style=\"background-color:#ebdef0;\">f therefore</span>refor<span style=\"background-color:#ebdef0;\">erefore sa</span><span style=\"background-color:#d4e6f1;\">me of the same </span><span style=\"background-color:#edebd0;\">of the same o</span><span style=\"background-color:#ebdef0;\">f therefore</span>refor<span style=\"background-color:#ebdef0;\">erefore sa</span><span style=\"background-color:#d0ece7;\">me to the same </span><span style=\"background-color:#f6ddcc;\">that to the w</span><span style=\"background-color:#ebdef0;\">hich of the</span>reforereforereforerefor<span style=\"background-color:#ebdef0;\">erefore sa</span><span style=\"background-color:#d4e6f1;\">me of the same </span><span style=\"background-color:#eadbd8;\">of the same to</span><span style=\"background-color:#eadbd8;\"> the same that t</span><span style=\"background-color:#d6eaf8;\">o the that</span><span style=\"background-color:#d6eaf8;\"> to the that</span> to the "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#fdebd0;\">Friedrich Nietzsche: On the Future of our Educational Institutions</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span>, <span style=\"background-color:#d0ece7;\">Immanuel Kant: Kant's Prolegomena</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#d6eaf8;\">Daisetz Teitaro Suzuki: Outlines of Mahayana Buddhism</span>, <span style=\"background-color:#e2d7d5;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 3 of 3)</span>, <span style=\"background-color:#d0ece7;\">unknown: The Yoga Sutras of Patanjali by Charles Johnston</span>, <span style=\"background-color:#eadbd8;\">unknown: The Vedanta-Sutras with the Commentary by Sankaracarya</span>, <span style=\"background-color:#d4e6f1;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#f6ddcc;\">Archibald Scott: Buddhism and Christianity</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.1 ---------\n",
      "Prompt: How to attain complete self-awareness? Complete \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "H<span style=\"background-color:#fdebd0;\">ow to attain </span><span style=\"background-color:#d6dbdf;\">complete self-a</span>wareness?<span style=\"background-color:#ecf3cf;\"> Complete a</span>t<span style=\"background-color:#d6eaf8;\"> the which </span>o<span style=\"background-color:#ebdef0;\">f therefore</span>refor<span style=\"background-color:#ebdef0;\">erefore sa</span><span style=\"background-color:#d4e6f1;\">me of the same </span>t<span style=\"background-color:#d6eaf8;\">o therefore sa</span><span style=\"background-color:#d4e6f1;\">me of the same </span><span style=\"background-color:#edebd0;\">of the same o</span><span style=\"background-color:#ebdef0;\">f therefore</span>reforereforerefor<span style=\"background-color:#ebdef0;\">erefore sa</span><span style=\"background-color:#d0ece7;\">me to the same </span><span style=\"background-color:#edebd0;\">to the same o</span><span style=\"background-color:#edebd0;\">f the same o</span><span style=\"background-color:#ebdef0;\">f therefore</span>refor<span style=\"background-color:#ebdef0;\">erefore sa</span><span style=\"background-color:#d4e6f1;\">me of the same </span><span style=\"background-color:#edebd0;\">of the same o</span><span style=\"background-color:#ebdef0;\">f the same th</span><span style=\"background-color:#edebd0;\">at to the th</span><span style=\"background-color:#edebd0;\">at to the th</span><span style=\"background-color:#ebdef0;\">at to the of</span><span style=\"background-color:#ebdef0;\"> therefore</span>refor<span style=\"background-color:#d4e6f1;\">erefore of thi</span><span style=\"background-color:#ebdef0;\">mself the p</span>hilosof t<span style=\"background-color:#e5e8e8;\">he of the same </span><span style=\"background-color:#d0ece7;\">that to the o</span><span style=\"background-color:#ebdef0;\">f therefore</span>refor<span style=\"background-color:#edebd0;\">e same of </span><span style=\"background-color:#edebd0;\">the same of </span><span style=\"background-color:#edebd0;\">the same of </span><span style=\"background-color:#edebd0;\">the same of </span>therefor<span style=\"background-color:#ebdef0;\">erefore sa</span><span style=\"background-color:#ebdef0;\">me which w</span><span style=\"background-color:#ebdef0;\">hich of the</span>refor<span style=\"background-color:#d4e6f1;\">erefore of thi</span><span style=\"background-color:#ebdef0;\">mself the p</span>hilos<span style=\"background-color:#d4efdf;\">of the whi</span>ch o<span style=\"background-color:#ebdef0;\">f therefore</span>refore"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#fdebd0;\">Friedrich Nietzsche: On the Future of our Educational Institutions</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#d6eaf8;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d4e6f1;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d0ece7;\">unknown: The Yoga Sutras of Patanjali by Charles Johnston</span>, <span style=\"background-color:#ebdef0;\">V. FausbÃ¶ll: Buddhist birth stories; or, Jataka tales, Vol. 1</span>, <span style=\"background-color:#d4e6f1;\">Daisetz Teitaro Suzuki: Essays in Zen Buddhism</span>, <span style=\"background-color:#e5e8e8;\">Vatsyayana: The Kama Sutra of Vatsyayana</span>, <span style=\"background-color:#d0ece7;\">Immanuel Kant: Kant's Prolegomena</span>, <span style=\"background-color:#d4efdf;\">Friedrich Nietzsche: The Joyful Wisdom</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "step 13312: train loss 5.1006, time 0.143 sec/iter                       0183 grad_norm: 3.168 Sec/It: 0.142  \n",
      "Sample at 2025-02-28 17:09:22:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What is the nature of reality? The nature \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#d6eaf8;\">What is the nature of </span>re<span style=\"background-color:#ecf3cf;\">ality? The </span><span style=\"background-color:#d4e6f1;\">nature which w</span><span style=\"background-color:#ebdef0;\">hich of the</span><span style=\"background-color:#edebd0;\">nce of the c</span>cor<span style=\"background-color:#eadbd8;\">din the co</span><span style=\"background-color:#ebdef0;\">nception of the conc</span><span style=\"background-color:#ebdef0;\">eption of the conc</span><span style=\"background-color:#ebdef0;\">eption of the c</span><span style=\"background-color:#d0ece7;\">cording to ti</span><span style=\"background-color:#e2d7d5;\">on of the conception of the </span><span style=\"background-color:#ebdef0;\">conception of the conc</span><span style=\"background-color:#ebdef0;\">eption of the c</span><span style=\"background-color:#d0ece7;\">cording to ti</span><span style=\"background-color:#e2d7d5;\">on of the conception of the </span><span style=\"background-color:#ebdef0;\">conception of the conc</span><span style=\"background-color:#ebdef0;\">eption of the c</span>epti<span style=\"background-color:#e2d7d5;\">ong to termin</span>in<span style=\"background-color:#ebdef0;\"> the that </span><span style=\"background-color:#eadbd8;\">tion of the conception of the </span><span style=\"background-color:#ebdef0;\">conception of the conc</span><span style=\"background-color:#ebdef0;\">eption of the conc</span><span style=\"background-color:#ebdef0;\">eption of the c</span><span style=\"background-color:#d0ece7;\">cording to ti</span><span style=\"background-color:#e2d7d5;\">on of the conception of the </span><span style=\"background-color:#ebdef0;\">conception of the conc</span><span style=\"background-color:#ebdef0;\">eption of the c</span><span style=\"background-color:#f6ddcc;\">cording to te</span>rmin<span style=\"background-color:#ebdef0;\">itself which w</span><span style=\"background-color:#eadbd8;\">hich which </span><span style=\"background-color:#eadbd8;\">which which </span>of thexperiexperiexperiexperiexperie"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d6eaf8;\">Daisetz Teitaro Suzuki: Outlines of Mahayana Buddhism</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#d4e6f1;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#eadbd8;\">unknown: The Vedanta-Sutras with the Commentary by Sankaracarya</span>, <span style=\"background-color:#d0ece7;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 1 of 3)</span>, <span style=\"background-color:#e2d7d5;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 3 of 3)</span>, <span style=\"background-color:#e2d7d5;\">Charles Eliot: Hinduism and Buddhism, Vol. 1 of 3</span>, <span style=\"background-color:#ebdef0;\">Immanuel Kant: The Metaphysical Elements of Ethics</span>, <span style=\"background-color:#f6ddcc;\">Arthur Schopenhauer: On the Fourfold Root of the Principle of Sufficient Reason and On the Will in Nature: Two Essays (revised edition)</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.0 ---------\n",
      "Prompt: What is the nature of reality? The nature \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#d6eaf8;\">What is the nature of </span>re<span style=\"background-color:#ecf3cf;\">ality? The </span>nat<span style=\"background-color:#eadbd8;\">ure nature</span>s<span style=\"background-color:#d6eaf8;\"> of through</span>e conng to theref<span style=\"background-color:#ebdef0;\">of the the</span> c<span style=\"background-color:#ebdef0;\">of the the</span> c<span style=\"background-color:#ebdef0;\">of the the</span> cof the of thexperi<span style=\"background-color:#fae5d3;\">experience of the c</span><span style=\"background-color:#ebdef0;\">onception of the conc</span><span style=\"background-color:#ebdef0;\">eption of the conc</span><span style=\"background-color:#ebdef0;\">eption of the t</span><span style=\"background-color:#ebdef0;\">ion of the concept</span>heref<span style=\"background-color:#d4efdf;\">of the whi</span><span style=\"background-color:#d4e6f1;\">ch which w</span><span style=\"background-color:#eadbd8;\">hich which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span>of thexperiexperi<span style=\"background-color:#fae5d3;\">experience of the c</span><span style=\"background-color:#d0ece7;\">cording to ti</span><span style=\"background-color:#e2d7d5;\">on of the conception of the </span><span style=\"background-color:#ebdef0;\">conception of the c</span><span style=\"background-color:#d0ece7;\">cording to ti</span><span style=\"background-color:#ebdef0;\">on of the c</span><span style=\"background-color:#ebdef0;\">cording to the</span>ref<span style=\"background-color:#d4efdf;\">of the whi</span><span style=\"background-color:#d4e6f1;\">ch which w</span><span style=\"background-color:#eadbd8;\">hich which </span><span style=\"background-color:#ebdef0;\">which of the</span>xperiexperiexperiexperiexperiexperi<span style=\"background-color:#fae5d3;\">experience of the c</span><span style=\"background-color:#d0ece7;\">cording to ti</span><span style=\"background-color:#ebdef0;\">on of the concept</span>heref<span style=\"background-color:#d4efdf;\">of the whi</span><span style=\"background-color:#d4e6f1;\">ch which w</span>hich "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d6eaf8;\">Daisetz Teitaro Suzuki: Outlines of Mahayana Buddhism</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#eadbd8;\">unknown: The Vedanta-Sutras with the Commentary by Sankaracarya</span>, <span style=\"background-color:#d6eaf8;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#fae5d3;\">Immanuel Kant: Perpetual Peace</span>, <span style=\"background-color:#d4efdf;\">Friedrich Nietzsche: The Joyful Wisdom</span>, <span style=\"background-color:#d4e6f1;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#d0ece7;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 1 of 3)</span>, <span style=\"background-color:#e2d7d5;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 3 of 3)</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.1 ---------\n",
      "Prompt: What is the nature of reality? The nature \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#d6eaf8;\">What is the nature of </span>re<span style=\"background-color:#ecf3cf;\">ality? The </span><span style=\"background-color:#ebdef0;\">nature of the</span>d the of thexperiexp<span style=\"background-color:#d4e6f1;\">eried the </span>of t<span style=\"background-color:#eadbd8;\">hed the wh</span><span style=\"background-color:#eadbd8;\">ich which </span>of th<span style=\"background-color:#fae5d3;\">experience of the c</span>cor<span style=\"background-color:#eadbd8;\">din the co</span><span style=\"background-color:#d4efdf;\">nception of the ca</span>n<span style=\"background-color:#fae5d3;\">not which </span><span style=\"background-color:#ebdef0;\">which of the</span><span style=\"background-color:#fae5d3;\">xperience of the c</span><span style=\"background-color:#ebdef0;\">onception of the conc</span><span style=\"background-color:#ebdef0;\">eption of the conc</span>eptheref<span style=\"background-color:#d4efdf;\">of the whi</span>ch naturexperiexperi<span style=\"background-color:#f6ddcc;\">e of thought which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#ebdef0;\">which the co</span><span style=\"background-color:#ebdef0;\">f the nature</span><span style=\"background-color:#e2d7d5;\">nce of the conception of the </span><span style=\"background-color:#ebdef0;\">conception of the conc</span><span style=\"background-color:#ebdef0;\">eption of the conc</span><span style=\"background-color:#d4efdf;\">eption of the ca</span>n<span style=\"background-color:#fae5d3;\">not which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span><span style=\"background-color:#eadbd8;\">which which </span>of th<span style=\"background-color:#fae5d3;\">experience of the c</span><span style=\"background-color:#ebdef0;\">onception of the c</span><span style=\"background-color:#d0ece7;\">cording to ti</span><span style=\"background-color:#d6eaf8;\">on of the ce</span>ptiong to <span style=\"background-color:#eadbd8;\">tion of the conception of the </span>concept"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d6eaf8;\">Daisetz Teitaro Suzuki: Outlines of Mahayana Buddhism</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d4e6f1;\">Daisetz Teitaro Suzuki: Essays in Zen Buddhism</span>, <span style=\"background-color:#eadbd8;\">unknown: The Vedanta-Sutras with the Commentary by Sankaracarya</span>, <span style=\"background-color:#fae5d3;\">Immanuel Kant: Perpetual Peace</span>, <span style=\"background-color:#d4efdf;\">Immanuel Kant: The Critique of Pure Reason</span>, <span style=\"background-color:#d4efdf;\">Friedrich Nietzsche: The Joyful Wisdom</span>, <span style=\"background-color:#f6ddcc;\">Arthur Schopenhauer: On the Fourfold Root of the Principle of Sufficient Reason and On the Will in Nature: Two Essays (revised edition)</span>, <span style=\"background-color:#e2d7d5;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 3 of 3)</span>, <span style=\"background-color:#d0ece7;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 1 of 3)</span>, <span style=\"background-color:#d6eaf8;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Ep: 0.05 Bat: 759/1024       â¦âââââââââââââââ     â¦ loss: 5.0482 lr: 0.000182 grad_norm: 5.996 Sec/It: 0.143  "
     ]
    }
   ],
   "source": [
    "tu = start_tu_session()\n",
    "try:\n",
    "    train(train_utils = tu)\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\nTraining interrupted.\")\n",
    "tu.train_session_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "othN-Vnt5EZT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for t in [0.5, 1.5]:\n",
    "#     print(f\"------Temperature {t}--------\")\n",
    "#     generate_sample(td, device, prompt=\"How are consciousness and quantum mechanics related?\", toks=150, temperature=t, top_k=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-code below, unfinished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UNG5wWhC8kU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "enc_texts = []\n",
    "for i in range(500):\n",
    "    e = td[i*50000][:256]\n",
    "    tx = torch.tensor([e]).to(device)\n",
    "    enc_texts.append(tx)\n",
    "    texts.append(td.decode(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_texts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADm9ycuA2ik7"
   },
   "outputs": [],
   "source": [
    "emb_text = []\n",
    "cont_text = []\n",
    "for et in enc_texts:\n",
    "    emb_text.append(model.embedding(et))\n",
    "    cont_text.append(model.context(et))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_text[0].shape, cont_text[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_vec = []\n",
    "cont_vec = []\n",
    "for i in range(len(emb_text)):\n",
    "    emb_vec.append(emb_text[i][0].sum(axis=0))\n",
    "    cont_vec.append(cont_text[i][0].sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_vec(a, b):\n",
    "    al = torch.sqrt(torch.dot(a,a))\n",
    "    bl = torch.sqrt(torch.dot(b,b))\n",
    "    an = a/al\n",
    "    bn = b/bl\n",
    "    return torch.dot(an,bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0):\n",
    "    best = -10.0\n",
    "    ind = -1\n",
    "    for j in range(len(emb_vec)):\n",
    "        if i==j:\n",
    "            continue\n",
    "        cos_val = cos_vec(emb_vec[i], emb_vec[j])\n",
    "        if cos_val > best:\n",
    "            best = cos_val\n",
    "            ind = j\n",
    "    # print(f\"{texts[i][:20]} ->{best}: {texts[ind][:20]}\")\n",
    "print()        \n",
    "for i in range(200):\n",
    "    best = 0\n",
    "    ind = -1\n",
    "    for j in range(len(emb_vec)):\n",
    "        if i==j:\n",
    "            continue\n",
    "        cos_val = cos_vec(cont_vec[i], cont_vec[j])\n",
    "        if cos_val > best:\n",
    "            best = cos_val\n",
    "            ind = j\n",
    "    print(f\"{texts[i]} \\n\\n->{best}:\\n\\n {texts[ind]}\")\n",
    "    print(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td[200002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VmWbteSFQtfq",
    "yWE_ZZMKEARV"
   ],
   "gpuClass": "premium",
   "gpuType": "V100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "torch_transformer_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
