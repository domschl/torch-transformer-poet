{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-transformer-poet/blob/main/torch_transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Torch-Transformer-Poet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DabS0VZ-1Zp0"
   },
   "source": [
    "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jtpy59Yq-Qfz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml-indie-tools in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (0.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ml-indie-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U5T4m6earb1e"
   },
   "outputs": [],
   "source": [
    "from ml_indie_tools.env_tools import MLEnv\n",
    "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
    "\n",
    "from ml_indie_tools.pytorch_custom_layers import MultiHeadSelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "A pytorch deep multi-head attention model for text generation following Andrej Karpathy's [video-lecture-ng](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py)\n",
    "\n",
    "This code can use either CPU, GPU, or Apple Silicon. Google Colab is supported too, select the corresponding Colab runtime (menu: **`Runtime / Change runtime type`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "llPw84PkEAP2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OS: Darwin, Python: 3.10.8 (Conda), Jupyter Notebook Pytorch: 2.0.0.dev20230124, GPU: MPS Metal accelerator (system memory)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
    "\n",
    "ml_env = MLEnv(platform='pt', accelerator='fastest')\n",
    "ml_env.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "t-TP3Pnsrb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path (all projects) : . (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
      "Project path             : . (Changes to the file system happen only below this project path\n",
      "Model path (snapshots)   : ./model/ngpt_philosophers_v1_pt (Model weights and snapshots are stored here)\n",
      "Data path (training data): ./data (Training data will be downloaded here)\n",
      "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
     ]
    }
   ],
   "source": [
    "# project_name = 'women_writers'\n",
    "project_name='philosophers'\n",
    "model_name=f'ngpt_{project_name}_v1_pt'\n",
    "\n",
    "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
    "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools \n",
    "#\n",
    "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
    "\n",
    "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
    "\n",
    "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
    "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
    "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
    "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
    "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  1. Text library\n",
    "\n",
    "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
    "encoding, batch generation, and formatted source display. It read some \n",
    "books from Project Gutenberg and supports creation of training batches. \n",
    "The output functions support highlighting to allow to compare generated \n",
    "texts with the actual sources to help to identify identical (memorized) \n",
    "parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BF8eyWnCrb1h"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
    "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C66X7ynnrb1h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 matching books found with search {'author': ['Immanuel Kant', 'Friedrich Nietzsche', 'Wilhelm Hegel'], 'language': ['english']}.\n"
     ]
    }
   ],
   "source": [
    "if project_name == 'women_writers':  # sample searches\n",
    "    search_spec= {\n",
    "        \"author\": [\"Emily BrontÃ«\", \"Jane Austen\", \"Virginia Woolf\"], \n",
    "        \"language\": [\"english\"]\n",
    "    }\n",
    "elif project_name == 'philosophers':\n",
    "    search_spec = {\n",
    "        \"author\": [\"Immanuel Kant\", \"Friedrich Nietzsche\", \"Wilhelm Hegel\"],\n",
    "        \"language\": [\"english\"]\n",
    "    }\n",
    "    \n",
    "book_list=gd.search(search_spec)\n",
    "book_cnt = len(book_list)\n",
    "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
    "if book_cnt<40:\n",
    "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
    "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
    "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
    "else:\n",
    "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MH6_7IU3upOd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The History of Philosophy: Volume 3 of 3 - Georg Wilhelm Hegel, 58169\n",
      "1: The Will to Power, Books III and IV - Friedrich Nietzsche, 52915\n",
      "2: The Will to Power, Books I and II - Friedrich Nietzsche, 52914\n",
      "3: The Joyful Wisdom - Friedrich Nietzsche, 52881\n",
      "4: Kant's Prolegomena - Immanuel Kant, 52821\n",
      "5: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3 - Georg Wilhelm Hegel, 51636\n",
      "6: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3 - Georg Wilhelm Hegel, 51635\n",
      "7: Early Greek Philosophy & Other Essays - Friedrich Nietzsche, 51548\n",
      "8: Perpetual Peace - Immanuel Kant, 50922\n",
      "9: Kant's Critique of Judgement - Immanuel Kant, 48433\n",
      "10: Thoughts Out of Season, Part 2 - Friedrich Nietzsche, 38226\n",
      "11: Human, All Too Human - Friedrich Nietzsche, 38145\n",
      "12: We Philologists, Volume 8 of 18 - Friedrich Nietzsche, 18267\n",
      "13: The Metaphysical Elements of Ethics - Immanuel Kant, 5684\n",
      "14: The Critique of Practical Reason - Immanuel Kant, 5683\n",
      "15: Fundamental Principles of the Metaphysic of Morals - Immanuel Kant, 5682\n",
      "16: Thoughts out of Season, Part One - Friedrich Nietzsche, 5652\n",
      "17: Beyond Good and Evil - Friedrich Nietzsche, 4363\n",
      "18: The Critique of Pure Reason - Immanuel Kant, 4280\n",
      "19: Thus Spake Zarathustra - Friedrich Nietzsche, 1998\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(book_list)):\n",
    "    print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2jBH3Z15rb1h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Loaded 20 texts\n",
      "INFO:Datasets:Extracting ngrams of length 1..20 from text_list, selecting 25000 most used ngrams.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "1: The History of Philosophy: Volume 3 of 3 - Georg Wilhelm Hegel\n",
      "2: The Will to Power, Books III and IV - Friedrich Nietzsche\n",
      "3: The Will to Power, Books I and II - Friedrich Nietzsche\n",
      "4: The Joyful Wisdom - Friedrich Nietzsche\n",
      "5: Kant's Prolegomena - Immanuel Kant\n",
      "6: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3 - Georg Wilhelm Hegel\n",
      "7: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3 - Georg Wilhelm Hegel\n",
      "8: Early Greek Philosophy & Other Essays - Friedrich Nietzsche\n",
      "9: Perpetual Peace - Immanuel Kant\n",
      "10: Kant's Critique of Judgement - Immanuel Kant\n",
      "11: Thoughts Out of Season, Part 2 - Friedrich Nietzsche\n",
      "12: Human, All Too Human - Friedrich Nietzsche\n",
      "13: We Philologists, Volume 8 of 18 - Friedrich Nietzsche\n",
      "14: The Metaphysical Elements of Ethics - Immanuel Kant\n",
      "15: The Critique of Practical Reason - Immanuel Kant\n",
      "16: Fundamental Principles of the Metaphysic of Morals - Immanuel Kant\n",
      "17: Thoughts out of Season, Part One - Friedrich Nietzsche\n",
      "18: Beyond Good and Evil - Friedrich Nietzsche\n",
      "19: The Critique of Pure Reason - Immanuel Kant\n",
      "20: Thus Spake Zarathustra - Friedrich Nietzsche\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Encoding text corpora as ngrams.\n",
      "INFO:Datasets:Encoding text The History of Philosophy: Volume 3 of 3...\n",
      "INFO:Datasets:Encoding text The Will to Power, Books III and IV...\n",
      "INFO:Datasets:Encoding text The Will to Power, Books I and II...\n",
      "INFO:Datasets:Encoding text The Joyful Wisdom...\n",
      "INFO:Datasets:Encoding text Kant's Prolegomena...\n",
      "INFO:Datasets:Encoding text Hegel's Lectures on the History of Philosophy: Vol. 2 of 3...\n",
      "INFO:Datasets:Encoding text Hegel's Lectures on the History of Philosophy: Vol. 1 of 3...\n",
      "INFO:Datasets:Encoding text Early Greek Philosophy & Other Essays...\n",
      "INFO:Datasets:Encoding text Perpetual Peace...\n",
      "INFO:Datasets:Encoding text Kant's Critique of Judgement...\n",
      "INFO:Datasets:Encoding text Thoughts Out of Season, Part 2...\n",
      "INFO:Datasets:Encoding text Human, All Too Human...\n",
      "INFO:Datasets:Encoding text We Philologists, Volume 8 of 18...\n",
      "INFO:Datasets:Encoding text The Metaphysical Elements of Ethics...\n",
      "INFO:Datasets:Encoding text The Critique of Practical Reason...\n",
      "INFO:Datasets:Encoding text Fundamental Principles of the Metaphysic of Morals...\n",
      "INFO:Datasets:Encoding text Thoughts out of Season, Part One...\n",
      "INFO:Datasets:Encoding text Beyond Good and Evil...\n",
      "INFO:Datasets:Encoding text The Critique of Pure Reason...\n",
      "INFO:Datasets:Encoding text Thus Spake Zarathustra...\n",
      "INFO:Datasets:Encoding text corpora as ngrams done.\n"
     ]
    }
   ],
   "source": [
    "MAX_TOKENS = 25000  # This becomes vocab_size\n",
    "MAX_NGRAM_LEN = 20   # Max length of a token\n",
    "\n",
    "if project_name == 'women_writers':\n",
    "    select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
    "    sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
    "else:\n",
    "    sub_book_list = book_list\n",
    "    \n",
    "print(\"Using:\")\n",
    "for i in range(len(sub_book_list)):\n",
    "    print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
    "\n",
    "textlib_dataset = None  # Forces re-caching\n",
    "td = Text_Dataset(sub_book_list)\n",
    "td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "f7_tc2Lirb1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247562 records\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LEN = 64\n",
    "# SUB_PROBABILITY = 0.15  # like BERT\n",
    "\n",
    "td.init_getitem(sample_type='encoded', sample_length=SEQUENCE_LEN+1, content_stepping=1)\n",
    "\n",
    "num_records = len(td)\n",
    "\n",
    "print(f\"{num_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zZbbsNm0cOeW"
   },
   "outputs": [],
   "source": [
    "def get_sample_batch(td, batch_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    # y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    # return x, y\n",
    "    for i in range(batch_size):\n",
    "        data = td.get_random_item()\n",
    "        Xi = data[:-1]\n",
    "        yi = data[1:]\n",
    "        if i==0:\n",
    "            # smpX=np.array(Xi, dtype=np.float32)\n",
    "            smpX=np.array(Xi, dtype=np.int32)\n",
    "            smpy=np.array(yi, dtype=np.int32)\n",
    "        else:\n",
    "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
    "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
    "    return np.array(smpX), np.array(smpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TI3Fx6bNuR9A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0](l=64): X=>however, would still be _ambiguous_: it might be a\n",
      "movement either of _increase_ or _decline_ in Life.\n",
      "\n",
      "_B._\n",
      "\n",
      "The belief in \"progress\"--in lower spheres of intelligence, appears as\n",
      "increasing life: but this is self-deception;\n",
      "\n",
      "    in higher sphere<,\n",
      "y=>would still be _ambiguous_: it might be a\n",
      "movement either of _increase_ or _decline_ in Life.\n",
      "\n",
      "_B._\n",
      "\n",
      "The belief in \"progress\"--in lower spheres of intelligence, appears as\n",
      "increasing life: but this is self-deception;\n",
      "\n",
      "    in higher spheres of i<\n",
      "[1](l=64): X=>yself, which is\n",
      "the relation of me to myself; what is in me, that I know. The supreme\n",
      "principle, as immediate and not derived, must be certain on its own\n",
      "account; that is, a determination of the ego only, for it is only from\n",
      "the ego that I cannot abstract.[383] Fichte thus begins, like Descartes,\n",
      "with âI think, therefore I <,\n",
      "y=>, which is\n",
      "the relation of me to myself; what is in me, that I know. The supreme\n",
      "principle, as immediate and not derived, must be certain on its own\n",
      "account; that is, a determination of the ego only, for it is only from\n",
      "the ego that I cannot abstract.[383] Fichte thus begins, like Descartes,\n",
      "with âI think, therefore I am<\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = get_sample_batch(td, 2)\n",
    "for i in range(len(test_x)):\n",
    "    xi=[int(x) for x in test_x[i]]\n",
    "    print(f\"[{i}](l={len(xi)}): X=>{td.decode(xi)}<,\\ny=>{td.decode(test_y[i])}<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qnMxRkkmcOeX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 64), (2, 64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30hi0UPtEAQG"
   },
   "source": [
    "## 2. data for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jn_LcJ6g9Mzy"
   },
   "outputs": [],
   "source": [
    "def expand_name_template(template, params):\n",
    "    exp=copy.copy(template)\n",
    "    for key in params:\n",
    "        src=\"{\"+key+\"}\"\n",
    "        dst=f\"{params[key]}\"\n",
    "        exp=exp.replace(src,dst).replace('[','(').replace(']',')')\n",
    "    return exp\n",
    "\n",
    "def save_model_metadata(epoch, suffix='std'):\n",
    "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
    "    params['current_epoch'] = epoch\n",
    "    try:\n",
    "        with open(meta_file, 'w') as f:\n",
    "            f.write(json.dumps(params))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to store model metadata at {model_path}: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def read_model_metadata(suffix=\"std\"):\n",
    "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
    "    try:\n",
    "        with open(meta_file, 'r') as f:\n",
    "            meta = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot access project meta-data at {meta_file}: {e}, starting anew.\")\n",
    "        return None\n",
    "    return meta\n",
    "\n",
    "def is_metadata_compatible(params, meta):\n",
    "    is_valid=True\n",
    "    keys=set(list(params.keys())+list(meta.keys()))\n",
    "    for key in keys:\n",
    "        if key in updatable_keys:\n",
    "            continue\n",
    "        if key not in meta:\n",
    "            print(f\"Key {key} not available in last checkpoint model_meta, params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
    "            is_valid = False\n",
    "        elif key not in params:\n",
    "            print(f\"Key {key} not available in params, last checkpoint model_meta[{key}]: {meta[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
    "            is_valid = False\n",
    "        elif meta[key]!=params[key]:\n",
    "            print(f\"Last checkpoint model_meta[{key}]: {meta[key]} != params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
    "            is_valid = False\n",
    "    if is_valid is False:\n",
    "        print(\"Aborting import.\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "znpIUA3ig3gO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot access project meta-data at ./model/ngpt_philosophers_v1_pt/model_meta_2x2x{units}x25000.json: [Errno 2] No such file or directory: './model/ngpt_philosophers_v1_pt/model_meta_2x2x{units}x25000.json', starting anew.\n",
      "Starting new model\n",
      "{'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}', 'mhsa_layers': 2, 'heads': 2, 'causal': True, 'dropout': 0.1, 'vocab_size': 25000, 'sequence_len': 64, 'embedding_size': 64, 'test_iterations': 10, 'batch_size': 256, 'learning_rate': 0.001, 'sample_every_n_iterations': 250, 'max_iterations': 1000000}\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = td.get_unique_token_count()  # vocabulary-size\n",
    "\n",
    "attn_layers = 2;\n",
    "\n",
    "params = { # Multi-head self-attention\n",
    "    'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
    "\n",
    "    'mhsa_layers': attn_layers, \n",
    "    'heads': 2,\n",
    "    'causal': True,  # Use causal self-attention\n",
    "    'dropout': 0.1,       # no dropout: 0.0\n",
    "    'vocab_size': vocabulary_size,\n",
    "    'sequence_len': SEQUENCE_LEN,\n",
    "    'embedding_size': 64, \n",
    "    'test_iterations': 10,  # number of epocs for loss estimation\n",
    "\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 0.001,\n",
    "    'sample_every_n_iterations': 250,\n",
    "    \n",
    "    'max_iterations': 1000000  # maximum number of training iterations\n",
    "}\n",
    "\n",
    "# if len(params['heads'])!=params['mhsa_layers']: # or len(params['units'])!=params['mhsa_layers']:\n",
    "#     print(\"ERROR: lenght of 'heads' and 'units' must be equal to mhsa_layers!\")\n",
    "    \n",
    "model_suffix = expand_name_template(params['name'], params)\n",
    "# Put 'important' params in checkpoint-pathname to separate model-data:\n",
    "checkpoint_dir = os.path.join(model_path, f\"training_checkpoints_{model_suffix}\")\n",
    "if os.path.exists(checkpoint_dir) is False:\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# When comparing if training-data is compatible with new params set, \n",
    "# the following keys are updatable, they can be changed while continuing\n",
    "# to use existing checkpoints and continue training with those values\n",
    "# changed:\n",
    "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'dropout', \n",
    "             'sample_every_n_epochs']\n",
    "\n",
    "# These values are taking from saved checkpoint:\n",
    "keep_keys=['current_epoch']\n",
    "\n",
    "continue_last = True\n",
    "if continue_last is False:\n",
    "    print(\"NOT continuing based on existing training! New start.\")\n",
    "\n",
    "meta = read_model_metadata(suffix=model_suffix)\n",
    "if meta is not None and is_metadata_compatible(params, meta) is True and continue_last is True:\n",
    "    for key in keep_keys:\n",
    "        if key in meta:\n",
    "            params[key]=meta[key]\n",
    "    if params is not None:\n",
    "        print(f\"Continuing last session from epoch {params['current_epoch']}\")\n",
    "    else:\n",
    "        print(f\"No previous data, starting new model\")\n",
    "else:\n",
    "    print(\"Starting new model\")\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jY3hUuhQYzdT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches = 8779\n"
     ]
    }
   ],
   "source": [
    "num_batches = num_records // params['batch_size']\n",
    "print(f\"num_batches = {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bgVHUkbhdK9y"
   },
   "outputs": [],
   "source": [
    "def get_torch_batch(td, batch_size, device, split=None):\n",
    "    x, y = get_sample_batch(td, batch_size)\n",
    "    return torch.tensor(x, dtype=torch.long).to(device), torch.tensor(y, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JTte4VvUdK9z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2823,  7934, 17043,    91, 16858, 22369, 14219, 18864, 13275,  1431,\n",
       "          10427,  4859,  3778, 13493,  2856, 24014, 18003,  3152,  4337, 12427,\n",
       "            891,    90,  5973, 13275, 23137, 16332,  1209, 10900, 13474,   436,\n",
       "             91, 12800,  3990,  7771, 21903,  3386, 18686, 23532, 20090,  6958,\n",
       "           1192,  1814,  4318, 22573,  6256, 10395, 13021, 11039,  1142,  8756,\n",
       "           6892, 15298,  2303, 12886,  7604,  3601,    20, 13034,   808,  9205,\n",
       "           4240,   351,    36,  4337],\n",
       "         [12401,  4448,  4975,  7682, 11245,    43,   594, 11325,    47,    73,\n",
       "           3440,  4975, 22209,  8849, 17590, 13217,   785, 17373,  5697, 22810,\n",
       "           7373, 15593, 22123,  2553, 12106, 20907, 11766,  2902,  3961,  7153,\n",
       "            344, 11656,    90,  1770, 18111, 14263,   323,   354,  1046,  2611,\n",
       "            399,  7910, 12188,  5363,   345,  5753, 16036,  2902, 10266, 12188,\n",
       "           5363,   345,  5753,  2697, 23789,  5895, 17552,   661,  1747, 19811,\n",
       "             28,  3200,  2244,  7171]]),\n",
       " tensor([[ 7934, 17043,    91, 16858, 22369, 14219, 18864, 13275,  1431, 10427,\n",
       "           4859,  3778, 13493,  2856, 24014, 18003,  3152,  4337, 12427,   891,\n",
       "             90,  5973, 13275, 23137, 16332,  1209, 10900, 13474,   436,    91,\n",
       "          12800,  3990,  7771, 21903,  3386, 18686, 23532, 20090,  6958,  1192,\n",
       "           1814,  4318, 22573,  6256, 10395, 13021, 11039,  1142,  8756,  6892,\n",
       "          15298,  2303, 12886,  7604,  3601,    20, 13034,   808,  9205,  4240,\n",
       "            351,    36,  4337, 12427],\n",
       "         [ 4448,  4975,  7682, 11245,    43,   594, 11325,    47,    73,  3440,\n",
       "           4975, 22209,  8849, 17590, 13217,   785, 17373,  5697, 22810,  7373,\n",
       "          15593, 22123,  2553, 12106, 20907, 11766,  2902,  3961,  7153,   344,\n",
       "          11656,    90,  1770, 18111, 14263,   323,   354,  1046,  2611,   399,\n",
       "           7910, 12188,  5363,   345,  5753, 16036,  2902, 10266, 12188,  5363,\n",
       "            345,  5753,  2697, 23789,  5895, 17552,   661,  1747, 19811,    28,\n",
       "           3200,  2244,  7171,  4395]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_torch_batch(td, 2, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "QnMCWf5AZn1-"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(device):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(params['test_iterations'])\n",
    "        for k in range(params['test_iterations']):\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            X, Y = get_torch_batch(td, params['batch_size'], device, split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    print(\"\\r\", end=\"\", flush=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_sample(td, device, toks=100, temperature=1.0):\n",
    "    # generate from the model\n",
    "    # context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    context = torch.tensor([td.encode(' ')]).to(device)\n",
    "    txt = td.decode(model.generate(context, max_new_tokens=toks, temperature=temperature)[0].tolist())\n",
    "    td.source_highlight(txt, min_quote_size=10, dark_mode=False, display_ref_anchor=False)\n",
    "    return txt\n",
    "    # open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n",
    "\n",
    "\n",
    "# @torch.compile\n",
    "def do_train_step(xb, yb):\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9hEziJ0odK9z"
   },
   "outputs": [],
   "source": [
    "# XXX!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pdaulm1VdK9z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model...\n",
      "3.328808 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"creating model...\")\n",
    "model_cpu = MultiHeadSelfAttention(params['vocab_size'], params['embedding_size'], \n",
    "                                   params['sequence_len'], params['dropout'], \n",
    "                                   params['heads'], params['mhsa_layers'], params['causal'], device)\n",
    "model = model_cpu.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "start_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdaulm1VdK9z",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "step 5250: train loss 5.3341, val loss 5.3299, time 0.518 sec/iter\n",
      "Sample: --------temperature: 1.1 ---------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " âO C<span style=\"background-color:#e2d7d5;\">old thought</span><span style=\"background-color:#d8daef;\"> into light</span>s, be insures\"!â<br>and young<span style=\"background-color:#d6eaf8;\"> treat him</span><span style=\"background-color:#edebd0;\">, it would<br></span><span style=\"background-color:#d8daef;\">dialectic a</span>ccust<span style=\"background-color:#e2d7d5;\"><br>earth, and </span>har able,â ask<span style=\"background-color:#d8daef;\">ed with himself</span><span style=\"background-color:#e2d7d5;\"><br>them there</span>by too -sto<span style=\"background-color:#d6dbdf;\">tic, but bli</span>y<span style=\"background-color:#d0ece7;\">s of Justice</span> (commround"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: The Joyful Wisdom</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d6eaf8;\">Immanuel Kant: Perpetual Peace</span>, <span style=\"background-color:#edebd0;\">Immanuel Kant: Kant's Prolegomena</span>, <span style=\"background-color:#d6dbdf;\">Friedrich Nietzsche: Thoughts Out of Season, Part 2</span>, <span style=\"background-color:#d0ece7;\">Friedrich Nietzsche: Early Greek Philosophy & Other Essays</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.2 ---------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " bivii<span style=\"background-color:#d8daef;\">.                    </span>I<span style=\"background-color:#ebdef0;\"> have great </span>familed its s<span style=\"background-color:#e2d7d5;\">lety and s</span>culkilled uniteth by my break <span style=\"background-color:#ecf3cf;\">ââa system of </span>evil<span style=\"background-color:#ebdef0;\"> almost no</span>-ey<span style=\"background-color:#ebdef0;\">es--that is to say, the </span>fass<span style=\"background-color:#eadbd8;\">er, by the a</span>bove st<span style=\"background-color:#d8daef;\">actness of the </span><span style=\"background-color:#d8daef;\">fashion,<br>o</span><span style=\"background-color:#d8daef;\">n the contrary, no </span>till. Wh"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#ebdef0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#e2d7d5;\">Friedrich Nietzsche: The Joyful Wisdom</span>, <span style=\"background-color:#ecf3cf;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: The Critique of Pure Reason</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "step 5500: train loss 5.3130, val loss 5.3079, time 0.652 sec/iter\n",
      "Sample: --------temperature: 1.1 ---------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " ^paragratc.--in exaltum<span style=\"background-color:#ebdef0;\"> after a st</span>al in _<span style=\"background-color:#d8daef;\">gument of </span>dising,<span style=\"background-color:#d8daef;\"> has himself </span><span style=\"background-color:#ebdef0;\">below the </span>emo<span style=\"background-color:#e5e8e8;\">ration feel</span>ing<span style=\"background-color:#d8daef;\">s of knowledge, and the</span>se cease the Goman<span style=\"background-color:#d4efdf;\"> as educat</span>ion<span style=\"background-color:#f6ddcc;\">: feeling a</span>s<span style=\"background-color:#ebdef0;\"> a striving </span><span style=\"background-color:#ecf3cf;\">organs, as </span>en perby mestorections "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d8daef;\">Friedrich Nietzsche: Thoughts out of Season, Part One</span>, <span style=\"background-color:#e5e8e8;\">Immanuel Kant: The Critique of Practical Reason</span>, <span style=\"background-color:#d4efdf;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#f6ddcc;\">Friedrich Nietzsche: Human, All Too Human</span>, <span style=\"background-color:#ecf3cf;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------temperature: 1.2 ---------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Î¼Î­Î½ ÏÎ¬ÏÏÎ¯Î±Î½Î·Ïá¼ÏÎµÎ¯Î¼Î±sâ<span style=\"background-color:#d8daef;\">a system<br>of </span>*Pan<span style=\"background-color:#eadbd8;\">ty as the pr</span>i<span style=\"background-color:#ebdef0;\">think of this </span><span style=\"background-color:#eadbd8;\">just as Ho</span><span style=\"background-color:#d8daef;\">wever, because </span><span style=\"background-color:#ecf3cf;\">in respect of m</span>i<span style=\"background-color:#d8daef;\">nd conception of </span>an or B<span style=\"background-color:#eadbd8;\">eal connection </span>on occur Theore<span style=\"background-color:#ebdef0;\">_unconscious</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d8daef;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#eadbd8;\">Immanuel Kant: The Critique of Pure Reason</span>, <span style=\"background-color:#ebdef0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#ecf3cf;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Iteration:  5554/5750/1000000"
     ]
    }
   ],
   "source": [
    "dt0 = time.time()\n",
    "print(\"training...\")\n",
    "for iter in range(start_iter, params['max_iterations']):\n",
    "    print(f\"\\rIteration: {iter+1:5d}/{((iter+1)//params['sample_every_n_iterations']+1)*params['sample_every_n_iterations']}/{params['max_iterations']}\", end=\"\", flush=True)\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if (iter + 1) % params['sample_every_n_iterations'] == 0 or iter == params['max_iterations'] - 1:\n",
    "        dt = time.time()\n",
    "        print(f\"\\rloss eval\", end=\"\", flush=True)\n",
    "        losses = estimate_loss(device)\n",
    "        print(\n",
    "            f\"step {iter+1}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time {(dt-dt0)/params['sample_every_n_iterations']:.3f} sec/iter\"\n",
    "        )\n",
    "        print(\"Sample: \", end=\"\", flush=True)\n",
    "        for temperature in [1.1, 1.2]:\n",
    "            print(f\"--------temperature: {temperature} ---------\")\n",
    "            generate_sample(td, device, toks=50, temperature=temperature)\n",
    "        print(\"-------------------------------------------\")\n",
    "        dt0 = time.time()\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_torch_batch(td, params['batch_size'], device, \"train\")\n",
    "    # evaluate the loss\n",
    "    do_train_step(xb, yb)\n",
    "    start_iter = iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZozsv2RdK90"
   },
   "outputs": [],
   "source": [
    "txt = generate_sample(td, device, toks=100, temperature=1.2)\n",
    "print(txt)\n",
    "td.source_highlight(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VmWbteSFQtfq",
    "yWE_ZZMKEARV"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "torch_transformer_poet.ipynb",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
