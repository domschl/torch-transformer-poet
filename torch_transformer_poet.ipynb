{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-transformer-poet/blob/main/torch_transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Torch-Transformer-Poet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DabS0VZ-1Zp0"
   },
   "source": [
    "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtpy59Yq-Qfz",
    "outputId": "24357d38-bcf2-4588-8e78-dcb722f121a6"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install ml-indie-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U5T4m6earb1e"
   },
   "outputs": [],
   "source": [
    "from ml_indie_tools.env_tools import MLEnv\n",
    "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
    "\n",
    "from ml_indie_tools.Calibre_Dataset import Calibre_Dataset\n",
    "from ml_indie_tools.Folder_Dataset import Folder_Dataset\n",
    "\n",
    "import ml_indie_tools.pytorch_meta_tools as MJ\n",
    "from ml_indie_tools.train_utils import TrainUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "p3kgZd3AnAnu"
   },
   "outputs": [],
   "source": [
    "# Optional experimental event server to record and propagate training progress, not (yet) recommended!\n",
    "# Functionality is ignored by default.\n",
    "try:\n",
    "    import indralib\n",
    "    indra_avail = True\n",
    "except Exception as e:\n",
    "    indra_avail = False\n",
    "if indra_avail is True:\n",
    "    print(\"Indralib is available, trying to connect to Indrajala server for training progress reports...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jVcwvURB5EZN"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.Logger(\"Main\")\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "A pytorch deep multi-head attention model for text generation following Andrej Karpathy's [video-lecture-ng](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py)\n",
    "\n",
    "This code can use either CPU, GPU, or Apple Silicon. Google Colab is supported too, select the corresponding Colab runtime (menu: **`Runtime / Change runtime type`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "llPw84PkEAP2",
    "outputId": "556a1ede-c7b4-4168-ccb2-4a9f1e3c5a06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OS: Darwin, Python: 3.13.2, Jupyter Notebook Pytorch: 2.6.0, GPU: MPS Metal accelerator (system memory)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
    "\n",
    "ml_env = MLEnv(platform='pt', accelerator='fastest')\n",
    "ml_env.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Qg3ZPBmC8kO"
   },
   "source": [
    "## 1. Project configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-TP3Pnsrb1f",
    "outputId": "7437d18e-c770-4949-b958-9cdefa3901dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path (all projects) : . (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
      "Project path             : . (Changes to the file system happen only below this project path\n",
      "Model path (snapshots)   : ./model/tr_neo_philosophers_v3_pt (Model weights and snapshots are stored here)\n",
      "Data path (training data): ./data (Training data will be downloaded here)\n",
      "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
     ]
    }
   ],
   "source": [
    "# project_name = 'women_writers'\n",
    "# project_name='research'\n",
    "project_name='neo_philosophers'\n",
    "model_cpu = None\n",
    "model_name=f'tr_{project_name}_v3_pt'\n",
    "\n",
    "use_preprocessed_data = True                      # Use already tokenized data\n",
    "use_existing_model_from_checkpoint = False         # Try to load checkpoint of training\n",
    "use_torch_compile = True                           # Requires a modern graphics card with torch compile backend support\n",
    "skip_additional_texts = False                       # Don't look for other data sources in `additional_texts.json`\n",
    "\n",
    "if 'google.colab' in sys.modules:  # Google colab notebooks run on server that provide UTC time, we adapt logs to local time:\n",
    "    local_timezone = ZoneInfo('Europe/Berlin')\n",
    "else:\n",
    "    local_timezone = None\n",
    "\n",
    "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
    "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools\n",
    "#\n",
    "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
    "\n",
    "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device\n",
    "\n",
    "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
    "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
    "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
    "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
    "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  2.1 Text data from Project Gutenberg\n",
    "\n",
    "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training,\n",
    "encoding, batch generation, and formatted source display. It read some\n",
    "books from Project Gutenberg and supports creation of training batches.\n",
    "The output functions support highlighting to allow to compare generated\n",
    "texts with the actual sources to help to identify identical (memorized)\n",
    "parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BF8eyWnCrb1h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Loading tokenizer from ./data/neo_philosophers_tokens.json\n",
      "INFO:Datasets:Loading tokenizer done.\n"
     ]
    }
   ],
   "source": [
    "token_file = os.path.join(data_path,f\"{project_name}_tokens.json\")\n",
    "if use_preprocessed_data is True:\n",
    "    if os.path.exists(token_file):\n",
    "        td = Text_Dataset()\n",
    "        td.load_tokenizer(token_file)\n",
    "    else:\n",
    "        use_preprocessed_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "C66X7ynnrb1h"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False:\n",
    "    cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
    "    gd = Gutenberg_Dataset(cache_dir=cache_dir)\n",
    "\n",
    "    if project_name == 'women_writers':  # sample searches\n",
    "        search_spec= {\n",
    "            \"author\": [\"Emily BrontÃ«\", \"Jane Austen\", \"Virginia Woolf\"],\n",
    "            \"language\": [\"english\"]\n",
    "        }\n",
    "        book_list=gd.search(search_spec)\n",
    "    elif project_name == 'neo_philosophers':\n",
    "        search_spec = {\n",
    "            \"author\": [\"Immanuel Kant\", \"Friedrich Nietzsche\", \"Wilhelm Hegel\", \"Arthur Schopenhauer\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list=gd.search(search_spec)\n",
    "        search_spec = {\n",
    "            \"author\": [\"Plato\", \"Platon\"],\n",
    "            \"title\": [\"Timaeus\", \"Critias\", \"Symposium\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list+=gd.search(search_spec)\n",
    "        search_spec = {\n",
    "            \"title\": [\"Buddh\", \"Sutra\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list+=gd.search(search_spec)\n",
    "    else:\n",
    "        search_spec = {}\n",
    "        book_list = []\n",
    "\n",
    "    book_cnt = len(book_list)\n",
    "    print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
    "\n",
    "    if book_cnt > 0:\n",
    "        if book_cnt<80:\n",
    "            # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts,\n",
    "            # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
    "            book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)\n",
    "        else:\n",
    "            logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")\n",
    "\n",
    "        for i in range(len(book_list)):\n",
    "            if 'author' not in book_list[i]:\n",
    "                book_list[i]['author']='unknown'\n",
    "            print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")\n",
    "\n",
    "        if project_name == 'women_writers':\n",
    "            select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
    "            sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
    "        else:\n",
    "            sub_book_list = book_list\n",
    "\n",
    "        print(\"Using:\")\n",
    "        for i in range(len(sub_book_list)):\n",
    "            if 'author' not in sub_book_list[i]:\n",
    "                sub_book_list[i]['author']='unknown'\n",
    "            print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
    "\n",
    "        td = Text_Dataset(sub_book_list)\n",
    "    else:\n",
    "        td = Text_Dataset()()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxNIc7gL9UNg"
   },
   "source": [
    "## 2.2 Additional training material from folders or Calibre library\n",
    "\n",
    "This looks for a file `additional_texts.json` in the `project_path` as shown above.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"local_texts\": [[\"/some/directory/that/contains/texts\", [\".txt\", \".md\", \".org\", \".py\"]]],\n",
    "  \"calibre\": [\"/home/myuser/Calibre Library\", []]\n",
    "}\n",
    "```\n",
    "\n",
    "If the folder(s) defined in `local_texts` contain text files with default endings `.txt`, `.md`, `.org`, or `.py` (can be configured), they are added to the training data. Folders are searched recursively.\n",
    "\n",
    "If the path defined in `calibre` contains a Calibre database, all text files (`.txt` only) within that library are added to the training data. The list argument can contain search-specs (see ml-indie-tools, calibre_dataset) to qualify which books to import, e.g. [{\"tags\": [\"philosophy\"]}] would import all books that are tagged with \"Philosophy\" within calibre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1NYdjlW65EZP"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False and skip_additional_texts is False:\n",
    "    additional = os.path.join(project_path, \"additional_texts.json\")\n",
    "    print(f\"Looking for description of additional sources in {additional}\")\n",
    "    if os.path.exists(additional) is True:\n",
    "        with open(additional, 'r') as f:\n",
    "            add_desc = json.load(f)\n",
    "            if 'local_texts' in add_desc:\n",
    "                fd = Folder_Dataset()\n",
    "                for text_path, qualifier in add_desc['local_texts']:\n",
    "                    if not isinstance(qualifier, list):\n",
    "                        qualifier = [qualifier]\n",
    "                    print(f\"Loading texts from {text_path} using extension restriction {qualifier}\")\n",
    "                    fd.load_index(text_path, use_aliases=False, min_file_size=2048, file_extensions=qualifier)\n",
    "                td.load_texts(fd.records[:10000])\n",
    "            if 'calibre' in add_desc:\n",
    "                cal_path, specs = add_desc['calibre']\n",
    "                if os.path.exists(cal_path):\n",
    "                    print(f\"Loading text from calibre at {cal_path}\")\n",
    "                    cd = Calibre_Dataset(cal_path)\n",
    "                    cd.load_index(max_file_size=100000000)\n",
    "                    if specs is not None and len(specs)!=0:\n",
    "                        ls = cd.search(specs)\n",
    "                        td.load_texts(ls)\n",
    "                    else:\n",
    "                        td.load_texts(cd.records[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSm4f9NSC8kQ"
   },
   "source": [
    "## 2.3 Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bsyBjqFyC8kQ"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False:\n",
    "    MAX_TOKENS = 32768  # This becomes vocab_size\n",
    "    MAX_NGRAM_LEN = 5   # Max length of a token\n",
    "    CHUNK_SIZE = 500000 # Split larger texts in chunks, if not None\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"Starting tokenizer with token length from 1..{MAX_NGRAM_LEN} with a max of {MAX_TOKENS} unique tokens,\")\n",
    "    print(\"this can take considerable time...\")\n",
    "\n",
    "    # Better tested NGRAM tokenizer:\n",
    "    # td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS)\n",
    "    # or alternative 'BYTEGRAM' (more experimental, can encode arbitrary UTF-8)\n",
    "    # td.init_tokenizer(tokenizer='bytegram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS, chunk_size=CHUNK_SIZE)\n",
    "    td.init_tokenizer(tokenizer='bytegram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS, chunk_size=CHUNK_SIZE)\n",
    "    td.save_tokenizer(token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oImAp6-DnAnw",
    "outputId": "5698f93a-3e3c-4ce1-fb1b-f0cb3c51dda4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: Good morning, this is a simple test sentence for tokenization(61) -> [5505, 111, 18314, 110, 1806, 116, 3012, 115, 8985, 109, 25713, 101, 20947, 110, 8513, 32, 1675, 111, 2243, 105, 6663, 110](22) OK, compressed size: 36.07%\n",
      "Tokenizer: Guten Morgen, dies is ein einfach Testsatz zur Aufteilung in Satzbestandteile(77) -> [17441, 116, 31991, 114, 3903, 100, 15655, 115, 1682, 101, 24959, 99, 349, 84, 13338, 97, 15817, 122, 805, 65, 23813, 101, 484, 117, 2708, 32, 4708, 116, 122, 6077, 97, 32391, 101, 1898](34) OK, compressed size: 44.16%\n",
      "Tokenizer: à½¦à½ºà½à½¦à¼à½à½²à½à¼à½à½£à¼à½à½¦à½¼à¼à½¢à¾à¾±à½´à½à¼(22) -> [11060, 186, 5962, 166, 497, 137, 4740, 145, 497, 132, 7015, 139, 4897, 166, 10124, 139, 8666, 146, 4577, 180, 5980, 139](22) OK, compressed size: 100.00%\n",
      "Tokenizer: à½¦à¾à½¼à½à¼à½à½²à½à¼à½¦à¾à½²à½à¼à½¢à¾à½ºà½ à½²à¼à½¦à¾à½²à½à¼à½à½¼à¼à½à½à¼(31) -> [6416, 159, 3901, 132, 497, 137, 4740, 145, 497, 166, 3615, 153, 4740, 132, 497, 162, 3615, 151, 6103, 160, 3957, 139, 6416, 153, 4740, 132, 497, 148, 10124, 139, 21563, 147, 5954](33) OK, compressed size: 106.45%\n"
     ]
    }
   ],
   "source": [
    "tok_tests = [\"Good morning, this is a simple test sentence for tokenization\",\n",
    "             \"Guten Morgen, dies is ein einfach Testsatz zur Aufteilung in Satzbestandteile\",\n",
    "             \"à½¦à½ºà½à½¦à¼à½à½²à½à¼à½à½£à¼à½à½¦à½¼à¼à½¢à¾à¾±à½´à½à¼\",\n",
    "             \"à½¦à¾à½¼à½à¼à½à½²à½à¼à½¦à¾à½²à½à¼à½¢à¾à½ºà½ à½²à¼à½¦à¾à½²à½à¼à½à½¼à¼à½à½à¼\"]\n",
    "for test in tok_tests:\n",
    "    enc = td.encode(test)\n",
    "    dec = td.decode(enc)\n",
    "    if dec != test:\n",
    "        print(f\"Tokenizer failed for: {test} -> {dec}\")\n",
    "    else:\n",
    "        r = len(enc)/len(test)*100.0\n",
    "        print(f\"Tokenizer: {test}({len(test)}) -> {enc}({len(enc)}) OK, compressed size: {r:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG03WA_yC8kR"
   },
   "source": [
    "## 3. Model metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UPMwIn2gC8kR"
   },
   "outputs": [],
   "source": [
    "params = None\n",
    "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'current_loss',\n",
    "                 'sample_every_n_iterations', 'sample_size', 'save_every_n_iterations', 'max_iterations']\n",
    "model_dimensions = 128\n",
    "context_length = 64\n",
    "\n",
    "params = { # Multi-head self-attention\n",
    "        'meta_name_template': '{prelude_layers}-{recurrent_layers}/{recurrence_steps}-{coda_layers}x{heads}x{units}x{vocab_size}',\n",
    "\n",
    "        'prelude_layers': 2,\n",
    "        'recurrent_layers': 0,\n",
    "        'coda_layers': 2,\n",
    "        'recurrence_steps': 2,\n",
    "        'heads': 8,\n",
    "        'vocab_size': td.get_unique_token_count(),\n",
    "        'context_length': context_length,\n",
    "        'dropout': 0.1,\n",
    "        'non_linearity': nn.Mish,  # Default nn.ReLU\n",
    "        'model_dimensions': model_dimensions,\n",
    "        'test_iterations': 100,  # number of epocs for loss estimation\n",
    "\n",
    "        'batch_size': 256,\n",
    "    \n",
    "        'learning_rate': 4e-4,  # Only used, if lr_schedule is False\n",
    "        'lr_schedule': True,\n",
    "        'lr_min': 2e-4,\n",
    "        'lr_max': 8e-4,\n",
    "        'warmup': 2000,\n",
    "        'decay': 50000,\n",
    "    \n",
    "        'grad_clip': 0.8,\n",
    "\n",
    "        'sample_every_n_iterations': 4096,\n",
    "        'sample_size': 192,\n",
    "        'save_every_n_iterations': 4096,\n",
    "\n",
    "        'max_iterations': 100000000  # maximum number of training iterations\n",
    "    }\n",
    "\n",
    "model_file_path = MJ.get_model_filename(model_path)\n",
    "if use_existing_model_from_checkpoint is True:\n",
    "    params = MJ.loamodel_dimensions_metadata_from_checkpoint(params, updatable_keys, model_file_path, device=device, log=log) # torch.device('cpu'))\n",
    "if params == None or use_existing_model_from_checkpoint is False:\n",
    "    use_existing_model_from_checkpoint = False\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U1R4yDlC8kR"
   },
   "source": [
    "## 4. Batch handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7_tc2Lirb1i",
    "outputId": "7952e25a-eac4-4242-b4c9-c255027af935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4843433 records\n"
     ]
    }
   ],
   "source": [
    "joint_training=0\n",
    "td.init_getitem(sample_type='encoded', sample_length=params['context_length']+1+joint_training, content_stepping=64)\n",
    "num_records = len(td)\n",
    "print(f\"{num_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zZbbsNm0cOeW"
   },
   "outputs": [],
   "source": [
    "def get_sample_sub_batch(sample_batch, batch_size, sub_index=0):\n",
    "    joint_training=0\n",
    "    for i in range(batch_size):\n",
    "        Xi = sample_batch[sub_index:-1-joint_training+sub_index]\n",
    "        yi = sample_batch[sub_index+1:]\n",
    "        if i==0:\n",
    "            # smpX=np.array(Xi, dtype=np.float32)\n",
    "            smpX=np.array(Xi, dtype=np.int32)\n",
    "            smpy=np.array(yi, dtype=np.int32)\n",
    "        else:\n",
    "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
    "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
    "    return np.array(smpX), np.array(smpy)\n",
    "\n",
    "def get_sample_batch(td, batch_size):\n",
    "    sample_batch = td.get_random_item()\n",
    "    return get_sample_sub_batch(sample_batch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jY3hUuhQYzdT",
    "outputId": "f3d44867-b84a-45aa-9b8c-691f0653617f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches = 18919\n"
     ]
    }
   ],
   "source": [
    "num_batches = num_records // params['batch_size']\n",
    "print(f\"num_batches = {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5VbH5nxnAnx",
    "outputId": "a180872f-dd22-49f8-d771-ddf3f1e19450"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 64), (2, 64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_sample_batch(td, 2)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bgVHUkbhdK9y"
   },
   "outputs": [],
   "source": [
    "sample_data = None\n",
    "\n",
    "def get_torch_subbatch(td, batch_size, device, split=None, sub_index=0):\n",
    "    global sample_data\n",
    "    if sub_index==0:\n",
    "        sample_data = td.get_random_item()\n",
    "    x, y = get_sample_sub_batch(sample_data, batch_size, sub_index)\n",
    "    tx = torch.tensor(x, dtype=torch.long).to(device)\n",
    "    tx.requires_grad = False\n",
    "    ty = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    ty.requires_grad = False\n",
    "    return tx, ty\n",
    "\n",
    "def get_torch_batch(td, batch_size, device, split=None):\n",
    "    x, y = get_sample_batch(td, batch_size)\n",
    "    tx = torch.tensor(x, dtype=torch.long).to(device)\n",
    "    tx.requires_grad = False\n",
    "    ty = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    ty.requires_grad = False\n",
    "    return tx, ty\n",
    "\n",
    "def get_zero_state(batch_size, context_length, hidden_size, device):\n",
    "    zstate = torch.zeros(batch_size, context_length, hidden_size, device=device)\n",
    "    zstate.requires_grad = False\n",
    "    return zstate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pvbi6kjXC8kS"
   },
   "source": [
    "## 5. Loss and training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZIUoglD7nAnx"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dimensions, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Precompute positional encodings\n",
    "        pe = torch.zeros(max_len, model_dimensions)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dimensions, 2).float() * (-math.log(10000.0) / model_dimensions))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # [1, max_len, model_dimensions]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, batch_size, model_dimensions]\n",
    "        seq_len = x.size(0)\n",
    "        pe = self.pe[:, :seq_len, :].expand(-1, x.size(1), -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jOfyQjAInAnx"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, model_dimensions, heads, projection_dimension, dropout=0.1, non_linearity=nn.ReLU):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(model_dimensions, heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(model_dimensions)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(model_dimensions, projection_dimension),\n",
    "            non_linearity(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(projection_dimension, model_dimensions)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(model_dimensions)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        if attn_mask is None and x.size(0) > 1:\n",
    "            seq_len = x.size(0)\n",
    "            attn_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "            attn_mask = attn_mask.to(x.device)  # [seq_len, seq_len], upper triangle = True (masked)\n",
    "\n",
    "        attn_output, _ = self.self_attn(x, x, x,\n",
    "                                      attn_mask=attn_mask,\n",
    "                                      key_padding_mask=key_padding_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        return x\n",
    "\n",
    "class LatentRecurrentBlock(nn.Module):\n",
    "    def __init__(self, model_dimensions, heads, projection_dimension, recurrence_steps=3, dropout=0.1, non_linearity=nn.ReLU):\n",
    "        super(LatentRecurrentBlock, self).__init__()\n",
    "        self.recurrence_steps = recurrence_steps\n",
    "        self.self_attn = nn.MultiheadAttention(model_dimensions, heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(model_dimensions)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.recurrent = nn.LSTM(  # Swap GRU for LSTM\n",
    "            input_size=model_dimensions,\n",
    "            hidden_size=model_dimensions,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(model_dimensions, projection_dimension),\n",
    "            non_linearity(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(projection_dimension, model_dimensions)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(model_dimensions)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        attn_output, _ = self.self_attn(x, x, x,\n",
    "                                      attn_mask=attn_mask,\n",
    "                                      key_padding_mask=key_padding_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        residual = x\n",
    "        batch_size = x.size(1)\n",
    "        latent = x.transpose(0, 1).contiguous()  # [batch, seq_len, model_dimensions]\n",
    "        latent = latent.view(batch_size * x.size(0), 1, x.size(2))  # [batch*seq, 1, model_dimensions]\n",
    "        h0 = torch.zeros(1, latent.size(0), x.size(2), device=x.device)\n",
    "        c0 = torch.zeros(1, latent.size(0), x.size(2), device=x.device)  # Add cell state\n",
    "        for _ in range(self.recurrence_steps):\n",
    "            latent, (h0, c0) = self.recurrent(latent, (h0, c0))  # LSTM outputs hidden + cell\n",
    "        latent = latent.view(x.size(1), x.size(0), -1).transpose(0, 1)\n",
    "        latent = residual + latent\n",
    "        ff_output = self.ff(latent)\n",
    "        output = self.norm2(latent + self.dropout2(ff_output))\n",
    "        return output\n",
    "\n",
    "class LatentRecurrentDepthModel(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dimensions, heads, context_length, projection_dimension,\n",
    "                 n1_prelude, n2_recurrent, n3_coda, recurrence_steps=3, dropout=0.1, non_linearity=nn.ReLU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary (for embedding and projection).\n",
    "            model_dimensions (int): Transformer hidden size.\n",
    "            heads (int): Number of attention heads.\n",
    "            projection_dimension (int): Feedforward hidden size.\n",
    "            n1_prelude, n2_recurrent, n3_coda (int): Number of blocks per stage.\n",
    "            recurrence_steps (int): Recurrent steps per LRD block.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(LatentRecurrentDepthModel, self).__init__()\n",
    "\n",
    "        self.context_length = context_length  # for generate\n",
    "        self.model_dimensions = model_dimensions\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, model_dimensions)\n",
    "        self.pos_encoding = PositionalEncoding(model_dimensions, max_len=context_length)\n",
    "        # self.pos_encoding = PositionalEncoding(model_dimensions) # , max_len=context_length)\n",
    "\n",
    "        # Prelude blocks\n",
    "        self.prelude = nn.ModuleList([\n",
    "            TransformerBlock(model_dimensions, heads, projection_dimension, dropout, non_linearity)\n",
    "            for _ in range(n1_prelude)\n",
    "        ])\n",
    "\n",
    "        # Latent Recurrent blocks\n",
    "        if n2_recurrent > 0:\n",
    "            self.recurrent = nn.ModuleList([\n",
    "                LatentRecurrentBlock(model_dimensions, heads, projection_dimension, recurrence_steps, dropout, non_linearity)\n",
    "                for _ in range(n2_recurrent)\n",
    "            ])\n",
    "        else:\n",
    "            self.recurrent = None\n",
    "\n",
    "        # Coda blocks\n",
    "        self.coda = nn.ModuleList([\n",
    "            TransformerBlock(model_dimensions, heads, projection_dimension, dropout, non_linearity)\n",
    "            for _ in range(n3_coda)\n",
    "        ])\n",
    "\n",
    "        # Final projection layer (e.g., to vocab size for generation)\n",
    "        self.proj = nn.Linear(model_dimensions, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Token IDs [batch_size, seq_len].\n",
    "            attn_mask (torch.Tensor, optional): Attention mask [seq_len, seq_len].\n",
    "            key_padding_mask (torch.Tensor, optional): Padding mask [batch_size, seq_len].\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits [batch_size, seq_len, vocab_size].\n",
    "        \"\"\"\n",
    "        # Embed input tokens\n",
    "        x = self.embedding(input_ids) * math.sqrt(self.model_dimensions) # /2.0  # [batch_size, seq_len, model_dimensions]\n",
    "        # x = self.pos_encoding(x)\n",
    "        x = x.transpose(0, 1)  # [seq_len, batch_size, model_dimensions] for transformer\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # Prelude: Entry to latent space\n",
    "        for block in self.prelude:\n",
    "            x = block(x, attn_mask, key_padding_mask)\n",
    "\n",
    "        # Recurrent: Refine latents\n",
    "        if self.recurrent is not None:\n",
    "            for block in self.recurrent:\n",
    "                x = block(x, attn_mask, key_padding_mask)\n",
    "\n",
    "        # Coda: Exit from latent space\n",
    "        for block in self.coda:\n",
    "            x = block(x, attn_mask, key_padding_mask)\n",
    "\n",
    "        # Project to output space\n",
    "        x = x.transpose(0, 1)  # [batch_size, seq_len, model_dimensions]\n",
    "        output = self.proj(x)  # [batch_size, seq_len, vocab_size]\n",
    "        return output\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate new tokens given a context\n",
    "\n",
    "        Note: for apple MPS, top_k is limited max 16 vor older torchs! ((01/2023) implementation limitation)\n",
    "        See: https://github.com/pytorch/pytorch/issues/78915\n",
    "        Solved in: https://github.com/pytorch/pytorch/pull/94639 (03/2023)\n",
    "\n",
    "        :param idx: the context (B,T) tensor of indices\n",
    "        :param max_new_tokens: the maximum number of tokens to generate\n",
    "        :param temperature: the temperature to use for sampling\n",
    "        :param top_k: the number of top tokens to consider\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last context_length tokens\n",
    "            idx_cond = idx[:, -self.context_length :]\n",
    "            # print(idx_cond.shape)\n",
    "            # get the predictions\n",
    "            logits = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply temperature\n",
    "            if temperature != 1.0 and temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def generate_with_beam(self, model, tokenizer, prompt=\"The\", max_len=50, temperature=1.0, top_k=30, beam_width=3):\n",
    "    # def generate(model, tokenizer, prompt=\"The\", max_len=50, temperature=1.0, top_k=30, beam_width=3):\n",
    "        \"\"\"\n",
    "        Beam search generation with static abort condition.\n",
    "\n",
    "        Args:\n",
    "            model: LatentRecurrentDepthModel\n",
    "            tokenizer: Your custom/botok tokenizer (no [EOS])\n",
    "            prompt (str): Starting text\n",
    "            max_len (int): Max output length\n",
    "            temperature (float): Softmax temperature\n",
    "            top_k (int): Sample from top k tokens\n",
    "            beam_width (int): Number of beams\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        device = next(model.parameters()).device\n",
    "        input_ids = torch.tensor([tokenizer.encode(prompt)], device=device)  # [1, seq_len]\n",
    "        beams = [(input_ids, 0.0)]  # (sequence, log_prob)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for step in range(max_len):\n",
    "                new_beams = []\n",
    "                for seq, score in beams:\n",
    "                    # Forward pass\n",
    "                    logits = model(seq)  # [1, seq_len, vocab_size]\n",
    "                    next_logits = logits[0, -1, :] / temperature\n",
    "\n",
    "                    # Top-k sampling\n",
    "                    top_k_logits, top_k_indices = torch.topk(next_logits, top_k)\n",
    "\n",
    "                    # Repetition penality\n",
    "                    for i, token in enumerate(seq[0][-5:]):\n",
    "                        penalty = 1.0 + 0.2 * i\n",
    "                        top_k_logits[top_k_indices == token] /= penalty\n",
    "\n",
    "                    probs = F.softmax(top_k_logits, dim=-1)\n",
    "\n",
    "                    # Sample beam_width candidates\n",
    "                    next_tokens = torch.multinomial(probs, num_samples=beam_width)\n",
    "                    for i in range(beam_width):\n",
    "                        token_id = top_k_indices[next_tokens[i]].unsqueeze(0).unsqueeze(0)  # [1, 1]\n",
    "                        log_prob = torch.log(probs[next_tokens[i]]).item()\n",
    "                        new_seq = torch.cat([seq, token_id], dim=1)\n",
    "                        # Repetition penalty\n",
    "                        # penalty = 1.0 if new_seq[0, -1].item() not in new_seq[0, -5:-1] else 0.9\n",
    "                        new_beams.append((new_seq, score + log_prob * penalty))\n",
    "\n",
    "                # Sort and prune beams\n",
    "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "                # Static abort: all beams at max_len or repeating last 5 tokens\n",
    "                # all_max_len = all(len(seq[0]) >= max_len for seq, _ in beams)\n",
    "                # all_repeating = all(\n",
    "                #     len(seq[0]) > 5 and seq[0, -5:].tolist() == [seq[0, -1].item()] * 5\n",
    "                #     for seq, _ in beams\n",
    "                # )\n",
    "                # if all_max_len or all_repeating:\n",
    "                #     break\n",
    "                if all(len(seq[0]) >= max_len for seq, _ in beams):\n",
    "                    break\n",
    "\n",
    "        best_seq, _ = beams[0]\n",
    "        return tokenizer.decode(best_seq[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5UQagG_tnAnx"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.GRU, nn.LSTM)):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param, gain=1.0)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdaulm1VdK9z",
    "outputId": "4b6d2ba1-3d5f-41e9-e8cf-36a0a56b951c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model...\n",
      "LatentRecurrentDepthModel(\n",
      "  (embedding): Embedding(32768, 128)\n",
      "  (pos_encoding): PositionalEncoding()\n",
      "  (prelude): ModuleList(\n",
      "    (0-1): 2 x TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (1): Mish()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (coda): ModuleList(\n",
      "    (0-1): 2 x TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (1): Mish()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (proj): Linear(in_features=128, out_features=32768, bias=True)\n",
      ")\n",
      "9.214464 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"creating model...\")\n",
    "try:\n",
    "    # Colab + torch 2 -> lots of garbage.\n",
    "    if model is not None:\n",
    "        del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = LatentRecurrentDepthModel(\n",
    "    vocab_size=params['vocab_size'],\n",
    "    model_dimensions=params['model_dimensions'], heads=params['heads'], projection_dimension=params['model_dimensions']*4,\n",
    "    context_length=params['context_length'],\n",
    "    n1_prelude=params['prelude_layers'], n2_recurrent=params['recurrent_layers'], n3_coda=params['coda_layers'], \n",
    "    recurrence_steps=params['recurrence_steps'], dropout=params['dropout'], non_linearity=params['non_linearity']\n",
    ")\n",
    "model.apply(init_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "model = model.to(device)\n",
    "if use_existing_model_from_checkpoint is True:\n",
    "    params_load = MJ.load_checkpoint(params, model, optimizer, file_path=model_file_path, updatable_keys=updatable_keys, device=device, log=log) # torch.device(\"cpu\"))\n",
    "    if params_load is not None:\n",
    "        params = params_load\n",
    "model = model.to(device)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "if use_torch_compile is True:\n",
    "    if device == 'cuda':\n",
    "        print(\"Compiling...\")\n",
    "        model = torch.compile(model)\n",
    "        print(\"Compile ok.\")\n",
    "        try:\n",
    "            torch.set_float32_matmul_precision('high')\n",
    "        except:\n",
    "            print(\"Seems no tensor cores for that.\")\n",
    "    # elif str(device) == 'mps':\n",
    "    #     print(\"Compiling...\")\n",
    "    #     model = torch.compile(model)\n",
    "    #     print(\"Compile ok.\")\n",
    "\n",
    "if 'current_epoch' in params:\n",
    "    ep = params['current_epoch']\n",
    "else:\n",
    "    ep=0\n",
    "if 'current_loss' in params:\n",
    "    ls = params['current_loss']\n",
    "else:\n",
    "    ls=0\n",
    "\n",
    "if ep==0 and ls==0:\n",
    "    start_iter = 0\n",
    "else:\n",
    "    start_iter = ep\n",
    "    current_loss = ls\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "QnMCWf5AZn1-"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(device):\n",
    "    # XXX: this does take data for train and val from SAME pool!\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(params['test_iterations'])\n",
    "        for k in range(params['test_iterations']):\n",
    "            # if k % (params['test_iterations']/10 + 1) == 0:\n",
    "            #     print(\".\", end=\"\", flush=True)\n",
    "            X, Y = get_torch_batch(td, params['batch_size'], device, split)\n",
    "            logits = model(X)\n",
    "            loss = get_loss(logits, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    print(\"\\r\", end=\"\", flush=True)\n",
    "    mloss = (out['train']+out['val'])/2.0\n",
    "    return mloss\n",
    "\n",
    "def generate_sample(td, device, prompt=' ', toks=100, state=None, temperature=1.0, top_k=None, pad=True, with_beam=True):\n",
    "    if with_beam is True:\n",
    "        txt = model.generate_with_beam(model,td,prompt,toks, temperature=temperature, top_k=top_k, beam_width=7)\n",
    "    else:\n",
    "        model.eval()\n",
    "        if pad is True:\n",
    "            while len(prompt)<params['context_length']*4:\n",
    "                if len(prompt)==params['context_length']*4-1:\n",
    "                    prompt = '\\n' + prompt\n",
    "                else:\n",
    "                    prompt = ' ' + prompt\n",
    "        context = torch.tensor([td.encode(prompt)]).to(device)\n",
    "        answer = model.generate(context, max_new_tokens=toks, temperature=temperature, top_k=top_k)\n",
    "        txt = td.decode(answer[0].tolist())\n",
    "    # Identify memorisation of text by highlighting verbatim quotes from sources\n",
    "    # that are longer than 10 chars. HTML colorcoded output for source identification:\n",
    "    td.source_highlight(txt, min_quote_size=10, dark_mode=False, display_ref_anchor=False)\n",
    "    model.train()\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "N2uWm6CTC8kT"
   },
   "outputs": [],
   "source": [
    "# @torch.jit.script\n",
    "# @torch.compile\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def get_loss(logits, yb):\n",
    "    output_flat = logits.reshape(-1, params['vocab_size'])\n",
    "    # output_flat = logits.view(-1, params['vocab_size'])\n",
    "    # print(output_flat.shape)\n",
    "    ybr = yb.reshape(-1)\n",
    "    # print(ybr.shape)\n",
    "    loss = criterion(output_flat, ybr)\n",
    "    return loss\n",
    "\n",
    "def do_train_step(xb, yb, device, state=None):\n",
    "    model.train()\n",
    "    logits = model(xb)\n",
    "    loss = get_loss(logits, yb)\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), params['grad_clip']).cpu()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "LNCHCLYsnAny"
   },
   "outputs": [],
   "source": [
    "def start_tu_session():\n",
    "    if indra_avail is True:\n",
    "        with open('indra_creds.json', 'r') as f:\n",
    "            creds = json.load(f)\n",
    "            tu = TrainUtils(indra_server_profile_name='default', username=creds['username'], password=creds['password'])\n",
    "    else:\n",
    "        tu = TrainUtils()\n",
    "    tu.train_session_start(model_name=model_name, model_description=\"Torch-poet tests\", model_version=1, model_params=params, indra_subdomain=\"torch_poet/first_tests/1\", status_string_size=110)\n",
    "    return tu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZZSvzWeenAny"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(optim, n_iter, warmup, max_lr, decay, min_lr):\n",
    "    if n_iter<warmup and warmup>0:\n",
    "        lr = (n_iter+1)/warmup*max_lr\n",
    "    elif n_iter<warmup+decay and decay>0:\n",
    "        i = n_iter-warmup\n",
    "        lr = (decay-i)/decay*(max_lr-min_lr)+min_lr\n",
    "    else:\n",
    "        lr = min_lr\n",
    "\n",
    "    for g in optim.param_groups:\n",
    "        g['lr'] = lr\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "aZpMI7_iMdR6",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_utils):\n",
    "    global start_iter\n",
    "    dt0 = time.time()\n",
    "    sdt = datetime.datetime.now(tz=local_timezone).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"training, start at {sdt}...\")\n",
    "    gen_id = 0\n",
    "    last_print=0\n",
    "    iter_bench = 1\n",
    "    tu = train_utils\n",
    "    lr = params['learning_rate']\n",
    "    # current_loss = estimate_loss(device)\n",
    "    inputs = [\"What is the difference between good and evil? The difference \", \"How did everything come into existence? The origin \", \"What was at the beginning of time? Time itself \", \"How are physics, quantum-mechanics and consciousness related? The relation between \", \"How to attain complete self-awareness? Complete \", \"What is the nature of reality? The nature \", \"How be a good human being? A human \"]\n",
    "    for iter in range(start_iter, params['max_iterations']):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if (iter + 1) % params['sample_every_n_iterations'] == 0 or iter == params['max_iterations'] - 1:\n",
    "            dt = time.time()\n",
    "            print(f\"\\rloss eval\", end=\"\", flush=True)\n",
    "            current_loss = estimate_loss(device)\n",
    "            print(\n",
    "                f\"step {iter+1}: train loss {current_loss:.4f}, time {(dt-dt0)/iter_bench:.3f} sec/iter                       \"\n",
    "            )\n",
    "            iter_bench = 1\n",
    "            sdt = datetime.datetime.now(tz=local_timezone).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"Sample at {sdt}:\", flush=True)\n",
    "            for temperature in [1.0]: # 0.75, 1.1, 1.3, 1.5]:\n",
    "                print(f\"--------temperature: {temperature} ---------\")\n",
    "                prompt = inputs[gen_id%len(inputs)]\n",
    "                print(f\"Prompt: {prompt}\")\n",
    "                generate_sample(td=td, device=device, prompt=prompt, toks=params['sample_size'], temperature=temperature, top_k=10, with_beam=False)\n",
    "                # print(f\"Prompt: {prompt}\")\n",
    "                # generate_sample(td=td, device=device, prompt=prompt, toks=params['sample_size'], temperature=temperature, top_k=10, with_beam=True)\n",
    "            print(\"-------------------------------------------\")\n",
    "            gen_id += 1\n",
    "            dt0 = time.time()\n",
    "\n",
    "        if params['lr_schedule'] is True:\n",
    "            lr = lr_schedule(optimizer, iter, params['warmup'], params['lr_max'], params['decay'], params['lr_min'])\n",
    "\n",
    "        xb, yb = get_torch_batch(td, params['batch_size'], device, \"train\")\n",
    "        cur_loss, cur_norm = do_train_step(xb, yb, device=device)\n",
    "\n",
    "\n",
    "        nt = time.time()\n",
    "        if (nt-last_print)>1:\n",
    "            rec = {\n",
    "                'epoch': iter/num_batches,\n",
    "                'batch': iter%params['sample_every_n_iterations'],\n",
    "                'num_batches': params['sample_every_n_iterations'],\n",
    "                'loss': cur_loss,\n",
    "                'learning_rate': lr,\n",
    "                'gradient_norm': cur_norm.item(),\n",
    "            }\n",
    "            status_string, record = train_utils.train_state(rec)\n",
    "            print(status_string, end=\"\\r\")\n",
    "            last_print=nt\n",
    "\n",
    "        start_iter = iter\n",
    "        iter_bench += 1\n",
    "        if (iter+1)%params['save_every_n_iterations'] == 0:\n",
    "            MJ.save_checkpoint(params, model, optimizer, iter, current_loss, file_path=model_file_path, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrHAASABnAn1",
    "outputId": "95681830-ebd3-458c-9d9c-2cd7a2ac4c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training, start at 2025-03-02 12:20:10...\n",
      "Ep: 0.04 Bat: 706/4096       â¦ââââ                â¦ loss: 7.2529 lr: 0.000283 grad_norm: 2.352 Sec/It: 0.235  "
     ]
    }
   ],
   "source": [
    "tu = start_tu_session()\n",
    "try:\n",
    "    train(train_utils = tu)\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\nTraining interrupted.\")\n",
    "tu.train_session_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bedDtd-4nAn1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VmWbteSFQtfq",
    "yWE_ZZMKEARV"
   ],
   "gpuClass": "premium",
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "torch_transformer_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
