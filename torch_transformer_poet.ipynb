{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-transformer-poet/blob/main/torch_transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Torch-Transformer-Poet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DabS0VZ-1Zp0"
   },
   "source": [
    "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jtpy59Yq-Qfz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml-indie-tools in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (0.5.4)\n",
      "Collecting ml-indie-tools\n",
      "  Downloading ml_indie_tools-0.5.5-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ml-indie-tools\n",
      "  Attempting uninstall: ml-indie-tools\n",
      "    Found existing installation: ml-indie-tools 0.5.4\n",
      "    Uninstalling ml-indie-tools-0.5.4:\n",
      "      Successfully uninstalled ml-indie-tools-0.5.4\n",
      "Successfully installed ml-indie-tools-0.5.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ml-indie-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U5T4m6earb1e"
   },
   "outputs": [],
   "source": [
    "from ml_indie_tools.env_tools import MLEnv\n",
    "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
    "\n",
    "from ml_indie_tools.Calibre_Dataset import Calibre_Dataset\n",
    "from ml_indie_tools.Folder_Dataset import Folder_Dataset\n",
    "\n",
    "from ml_indie_tools.pytorch_custom_layers import MultiHeadSelfAttention\n",
    "from ml_indie_tools.pytorch_meta_tools import ModelJanitor\n",
    "# from pytorch_meta_tools import ModelJanitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jVcwvURB5EZN"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Y7fZl2kv5EZN"
   },
   "outputs": [],
   "source": [
    "# Get text-format books from Calibre:\n",
    "# cd = Calibre_Dataset('~/Nextcloud/MediaArchive/Calibre Library')\n",
    "# cd.load_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "A pytorch deep multi-head attention model for text generation following Andrej Karpathy's [video-lecture-ng](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py)\n",
    "\n",
    "This code can use either CPU, GPU, or Apple Silicon. Google Colab is supported too, select the corresponding Colab runtime (menu: **`Runtime / Change runtime type`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "llPw84PkEAP2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OS: Darwin, Python: 3.10.8 (Conda), Jupyter Notebook Pytorch: 2.0.0.dev20230130, GPU: MPS Metal accelerator (system memory)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
    "\n",
    "ml_env = MLEnv(platform='pt', accelerator='fastest')\n",
    "ml_env.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "t-TP3Pnsrb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path (all projects) : . (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
      "Project path             : . (Changes to the file system happen only below this project path\n",
      "Model path (snapshots)   : ./model/ngpt_philosophers_v1_pt (Model weights and snapshots are stored here)\n",
      "Data path (training data): ./data (Training data will be downloaded here)\n",
      "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
     ]
    }
   ],
   "source": [
    "# project_name = 'women_writers'\n",
    "model_cpu = None\n",
    "project_name='philosophers'\n",
    "model_name=f'ngpt_{project_name}_v1_pt'\n",
    "\n",
    "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
    "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools \n",
    "#\n",
    "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
    "\n",
    "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
    "\n",
    "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
    "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
    "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
    "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
    "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  1. Text library\n",
    "\n",
    "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
    "encoding, batch generation, and formatted source display. It read some \n",
    "books from Project Gutenberg and supports creation of training batches. \n",
    "The output functions support highlighting to allow to compare generated \n",
    "texts with the actual sources to help to identify identical (memorized) \n",
    "parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BF8eyWnCrb1h"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
    "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C66X7ynnrb1h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 matching books found with search {'author': ['Plato'], 'title': ['Timaeus', 'Critias', 'Symposium'], 'language': ['english']}.\n"
     ]
    }
   ],
   "source": [
    "if project_name == 'women_writers':  # sample searches\n",
    "    search_spec= {\n",
    "        \"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"], \n",
    "        \"language\": [\"english\"]\n",
    "    }\n",
    "    book_list=gd.search(search_spec)\n",
    "elif project_name == 'philosophers':\n",
    "    search_spec = {\n",
    "        \"author\": [\"Immanuel Kant\", \"Friedrich Nietzsche\", \"Wilhelm Hegel\"],\n",
    "        \"language\": [\"english\"]\n",
    "    }\n",
    "    book_list=gd.search(search_spec)\n",
    "    search_spec = {\n",
    "        \"author\": [\"Plato\"],\n",
    "        \"title\": [\"Timaeus\", \"Critias\", \"Symposium\"],\n",
    "        \"language\": [\"english\"]\n",
    "    }\n",
    "    book_list+=gd.search(search_spec)\n",
    "\n",
    "book_cnt = len(book_list)\n",
    "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
    "if book_cnt<40:\n",
    "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
    "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
    "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
    "else:\n",
    "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MH6_7IU3upOd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The History of Philosophy: Volume 3 of 3 - Georg Wilhelm Hegel, 58169\n",
      "1: The Will to Power, Books III and IV - Friedrich Nietzsche, 52915\n",
      "2: The Will to Power, Books I and II - Friedrich Nietzsche, 52914\n",
      "3: The Joyful Wisdom - Friedrich Nietzsche, 52881\n",
      "4: Kant's Prolegomena - Immanuel Kant, 52821\n",
      "5: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3 - Georg Wilhelm Hegel, 51636\n",
      "6: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3 - Georg Wilhelm Hegel, 51635\n",
      "7: Early Greek Philosophy & Other Essays - Friedrich Nietzsche, 51548\n",
      "8: Perpetual Peace - Immanuel Kant, 50922\n",
      "9: Kant's Critique of Judgement - Immanuel Kant, 48433\n",
      "10: Thoughts Out of Season, Part 2 - Friedrich Nietzsche, 38226\n",
      "11: Human, All Too Human - Friedrich Nietzsche, 38145\n",
      "12: We Philologists, Volume 8 of 18 - Friedrich Nietzsche, 18267\n",
      "13: The Metaphysical Elements of Ethics - Immanuel Kant, 5684\n",
      "14: The Critique of Practical Reason - Immanuel Kant, 5683\n",
      "15: Fundamental Principles of the Metaphysic of Morals - Immanuel Kant, 5682\n",
      "16: Thoughts out of Season, Part One - Friedrich Nietzsche, 5652\n",
      "17: Beyond Good and Evil - Friedrich Nietzsche, 4363\n",
      "18: The Critique of Pure Reason - Immanuel Kant, 4280\n",
      "19: Thus Spake Zarathustra - Friedrich Nietzsche, 1998\n",
      "20: Symposium - Plato, 1600\n",
      "21: Timaeus - Plato, 1572\n",
      "22: Critias - Plato, 1571\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(book_list)):\n",
    "    print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2jBH3Z15rb1h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Loaded 23 texts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "1: The History of Philosophy: Volume 3 of 3 - Georg Wilhelm Hegel\n",
      "2: The Will to Power, Books III and IV - Friedrich Nietzsche\n",
      "3: The Will to Power, Books I and II - Friedrich Nietzsche\n",
      "4: The Joyful Wisdom - Friedrich Nietzsche\n",
      "5: Kant's Prolegomena - Immanuel Kant\n",
      "6: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3 - Georg Wilhelm Hegel\n",
      "7: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3 - Georg Wilhelm Hegel\n",
      "8: Early Greek Philosophy & Other Essays - Friedrich Nietzsche\n",
      "9: Perpetual Peace - Immanuel Kant\n",
      "10: Kant's Critique of Judgement - Immanuel Kant\n",
      "11: Thoughts Out of Season, Part 2 - Friedrich Nietzsche\n",
      "12: Human, All Too Human - Friedrich Nietzsche\n",
      "13: We Philologists, Volume 8 of 18 - Friedrich Nietzsche\n",
      "14: The Metaphysical Elements of Ethics - Immanuel Kant\n",
      "15: The Critique of Practical Reason - Immanuel Kant\n",
      "16: Fundamental Principles of the Metaphysic of Morals - Immanuel Kant\n",
      "17: Thoughts out of Season, Part One - Friedrich Nietzsche\n",
      "18: Beyond Good and Evil - Friedrich Nietzsche\n",
      "19: The Critique of Pure Reason - Immanuel Kant\n",
      "20: Thus Spake Zarathustra - Friedrich Nietzsche\n",
      "21: Symposium - Plato\n",
      "22: Timaeus - Plato\n",
      "23: Critias - Plato\n"
     ]
    }
   ],
   "source": [
    "if project_name == 'women_writers':\n",
    "    select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
    "    sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
    "else:\n",
    "    sub_book_list = book_list\n",
    "    \n",
    "print(\"Using:\")\n",
    "for i in range(len(sub_book_list)):\n",
    "    print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
    "\n",
    "# obsolete?! textlib_dataset = None  # Forces re-caching\n",
    "td = Text_Dataset(sub_book_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxNIc7gL9UNg"
   },
   "source": [
    "## Additional training material for folder `{data_path}/local_texts`\n",
    "\n",
    "If the folder {data_path} (defined above) contains a sub-folder `local_texts`, and it contains\n",
    "files of structure `<title> - <author> - <language>.txt`, then they are added to the training data.\n",
    "Sample filename: `\"./data/local_texts/works-of-shakespeare - William Shakespeare - English.txt\"`.\n",
    "The titles of those documents are referenced via numeric aliases to preserve privacy on non-public data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1NYdjlW65EZP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:FolderTextLib:Loaded 19 records from Folder.\n",
      "INFO:Datasets:Loaded 42 texts\n"
     ]
    }
   ],
   "source": [
    "use_local_folder_data = True\n",
    "if use_local_folder_data:\n",
    "    local_texts = os.path.join(data_path, 'local_texts')\n",
    "    fd = Folder_Dataset(local_texts)\n",
    "    fd.load_index(use_aliases=False)\n",
    "    td.load_texts(fd.records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_s54dnoW5EZP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Starting tokenizer on 42 texts...\n",
      "INFO:Datasets:Extracting ngrams of length 1..8 from text_list, selecting 20000 most used ngrams.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting NGRAM tokinizer with token length from 1..8 with a max of 20000 unique tokens,\n",
      "this can take considerable time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Encoding text corpora as ngrams.\n",
      "INFO:Datasets:Encoding text The History of Philosophy: Volume 3 of 3...\n",
      "INFO:Datasets:Encoding text The Will to Power, Books III and IV...\n",
      "INFO:Datasets:Encoding text The Will to Power, Books I and II...\n",
      "INFO:Datasets:Encoding text The Joyful Wisdom...\n",
      "INFO:Datasets:Encoding text Kant's Prolegomena...\n",
      "INFO:Datasets:Encoding text Hegel's Lectures on the History of Philosophy: Vol. 2 of 3...\n",
      "INFO:Datasets:Encoding text Hegel's Lectures on the History of Philosophy: Vol. 1 of 3...\n",
      "INFO:Datasets:Encoding text Early Greek Philosophy & Other Essays...\n",
      "INFO:Datasets:Encoding text Perpetual Peace...\n",
      "INFO:Datasets:Encoding text Kant's Critique of Judgement...\n",
      "INFO:Datasets:Encoding text Thoughts Out of Season, Part 2...\n",
      "INFO:Datasets:Encoding text Human, All Too Human...\n",
      "INFO:Datasets:Encoding text We Philologists, Volume 8 of 18...\n",
      "INFO:Datasets:Encoding text The Metaphysical Elements of Ethics...\n",
      "INFO:Datasets:Encoding text The Critique of Practical Reason...\n",
      "INFO:Datasets:Encoding text Fundamental Principles of the Metaphysic of Morals...\n",
      "INFO:Datasets:Encoding text Thoughts out of Season, Part One...\n",
      "INFO:Datasets:Encoding text Beyond Good and Evil...\n",
      "INFO:Datasets:Encoding text The Critique of Pure Reason...\n",
      "INFO:Datasets:Encoding text Thus Spake Zarathustra...\n",
      "INFO:Datasets:Encoding text Symposium...\n",
      "INFO:Datasets:Encoding text Timaeus...\n",
      "INFO:Datasets:Encoding text Critias...\n",
      "INFO:Datasets:Encoding text Neuroscience and Philosophy...\n",
      "INFO:Datasets:Encoding text The Emperor's New Mind...\n",
      "INFO:Datasets:Encoding text Vajra Essence (Dudjom Lingpa's Visions of the Great Per Book 3)...\n",
      "INFO:Datasets:Encoding text White Lotus...\n",
      "INFO:Datasets:Encoding text Yeshe Lama, Jigme Lingpa's Dzogchen Atiyoga Manual...\n",
      "INFO:Datasets:Encoding text Mind, a brief Introduction...\n",
      "INFO:Datasets:Encoding text Vivid Awareness...\n",
      "INFO:Datasets:Encoding text Mind, Language and Society...\n",
      "INFO:Datasets:Encoding text The Shape of Ancient Thought...\n",
      "INFO:Datasets:Encoding text The Adornment of the Middle Way...\n",
      "INFO:Datasets:Encoding text Finding Rest in Illusion (Trilogy of Rest)...\n",
      "INFO:Datasets:Encoding text The Large, the Small and the Human Mind...\n",
      "INFO:Datasets:Encoding text A Lamp to Displel Darkness...\n",
      "INFO:Datasets:Encoding text Dzogchen Collection, LotsawaHouse...\n",
      "INFO:Datasets:Encoding text Finding Rest in Meditation, Trilogy of Rest, Volume 2...\n",
      "INFO:Datasets:Encoding text Self-Liberation Through Seeing With Naked Awareness...\n",
      "INFO:Datasets:Encoding text Speech of Delight...\n",
      "INFO:Datasets:Encoding text The Road to Reality, A Complete Guide to the Laws of the Universe...\n",
      "INFO:Datasets:Encoding text Beacon of Certainty...\n",
      "INFO:Datasets:Encoding text corpora as ngrams done.\n"
     ]
    }
   ],
   "source": [
    "MAX_TOKENS = 20000  # This becomes vocab_size\n",
    "MAX_NGRAM_LEN = 8   # Max length of a token\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Starting NGRAM tokinizer with token length from 1..{MAX_NGRAM_LEN} with a max of {MAX_TOKENS} unique tokens,\")\n",
    "print(\"this can take considerable time...\")\n",
    "td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "726hVApr5EZP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Saving tokenizer to philosophers_tokens.json\n"
     ]
    }
   ],
   "source": [
    "td.save_tokenizer(f\"{project_name}_tokens.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MCbzvhws5EZP"
   },
   "outputs": [],
   "source": [
    "# td.load_tokenizer(\"tok.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4VO0ZugE5EZP"
   },
   "outputs": [],
   "source": [
    "# td.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "f7_tc2Lirb1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5797543 records\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LEN = 256\n",
    "\n",
    "td.init_getitem(sample_type='encoded', sample_length=SEQUENCE_LEN+1, content_stepping=1)\n",
    "\n",
    "num_records = len(td)\n",
    "\n",
    "print(f\"{num_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zZbbsNm0cOeW"
   },
   "outputs": [],
   "source": [
    "def get_sample_batch(td, batch_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    # y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    # return x, y\n",
    "    for i in range(batch_size):\n",
    "        data = td.get_random_item()\n",
    "        Xi = data[:-1]\n",
    "        yi = data[1:]\n",
    "        if i==0:\n",
    "            # smpX=np.array(Xi, dtype=np.float32)\n",
    "            smpX=np.array(Xi, dtype=np.int32)\n",
    "            smpy=np.array(yi, dtype=np.int32)\n",
    "        else:\n",
    "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
    "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
    "    return np.array(smpX), np.array(smpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TI3Fx6bNuR9A"
   },
   "outputs": [],
   "source": [
    "# test_x, test_y = get_sample_batch(td, 2)\n",
    "# for i in range(len(test_x)):\n",
    "#     xi=[int(x) for x in test_x[i]]\n",
    "#     print(f\"[{i}](l={len(xi)}): X=>{td.decode(xi)}<,\\ny=>{td.decode(test_y[i])}<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "qnMxRkkmcOeX"
   },
   "outputs": [],
   "source": [
    "# test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30hi0UPtEAQG"
   },
   "source": [
    "## 2. data for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "znpIUA3ig3gO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meta_name_template': '{mhsa_layers}x{heads}x{units}x{vocab_size}', 'mhsa_layers': 16, 'heads': 16, 'causal': True, 'dropout': 0.1, 'vocab_size': 20000, 'sequence_len': 256, 'embedding_size': 256, 'test_iterations': 10, 'batch_size': 64, 'learning_rate': 0.0004, 'sample_every_n_iterations': 250, 'sample_size': 100, 'save_every_n_iterations': 100, 'max_iterations': 1000000}\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = td.get_unique_token_count()  # vocabulary-size\n",
    "\n",
    "attn_layers = 16;\n",
    "\n",
    "params = { # Multi-head self-attention\n",
    "    'meta_name_template': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
    "\n",
    "    'mhsa_layers': attn_layers, \n",
    "    'heads': 16,\n",
    "    'causal': True,  # Use causal self-attention\n",
    "    'dropout': 0.1,       # no dropout: 0.0\n",
    "    'vocab_size': vocabulary_size,\n",
    "    'sequence_len': SEQUENCE_LEN,\n",
    "    'embedding_size': 256, \n",
    "    'test_iterations': 10,  # number of epocs for loss estimation\n",
    "\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.0004,\n",
    "    'sample_every_n_iterations': 250,\n",
    "    'sample_size': 100,\n",
    "    'save_every_n_iterations': 100,\n",
    "    \n",
    "    'max_iterations': 1000000  # maximum number of training iterations\n",
    "}\n",
    "\n",
    "# When comparing if training-data is compatible with new params set, \n",
    "# the following keys are updatable, they can be changed while continuing\n",
    "# to use existing checkpoints and continue training with those values\n",
    "# changed:\n",
    "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'current_loss', 'dropout', \n",
    "             'sample_every_n_iterations', 'sample_size', 'save_every_n_iterations']\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jY3hUuhQYzdT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches = 90586\n"
     ]
    }
   ],
   "source": [
    "num_batches = num_records // params['batch_size']\n",
    "print(f\"num_batches = {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bgVHUkbhdK9y"
   },
   "outputs": [],
   "source": [
    "def get_torch_batch(td, batch_size, device, split=None):\n",
    "    x, y = get_sample_batch(td, batch_size)\n",
    "    return torch.tensor(x, dtype=torch.long).to(device), torch.tensor(y, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "JTte4VvUdK9z"
   },
   "outputs": [],
   "source": [
    "# get_torch_batch(td, 2, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "QnMCWf5AZn1-"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(device):\n",
    "    # XXX: this does take data for train and val from SAME pool!\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(params['test_iterations'])\n",
    "        for k in range(params['test_iterations']):\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            X, Y = get_torch_batch(td, params['batch_size'], device, split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    print(\"\\r\", end=\"\", flush=True)\n",
    "    mloss = (out['train']+out['val'])/2.0\n",
    "    return mloss\n",
    "\n",
    "def generate_sample(td, device, prompt=' ', toks=100, temperature=1.0, top_k=None ):\n",
    "    # generate from the model\n",
    "    # context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    model.eval()\n",
    "    # while len(prompt)<params['sequence_len']:\n",
    "    #     prompt = ' ' + prompt\n",
    "    context = torch.tensor([td.encode(prompt)]).to(device)\n",
    "    answer = model.generate(context, max_new_tokens=toks, temperature=temperature, top_k=top_k)\n",
    "    txt = td.decode(answer[0].tolist())\n",
    "    # Identify memorisation of text by highlighting verbatim quotes from sources\n",
    "    # that are longer than 10 chars. HTML colorcoded output for source identification:\n",
    "    td.source_highlight(txt, min_quote_size=10, dark_mode=False, display_ref_anchor=False)\n",
    "    return txt\n",
    "    # open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9hEziJ0odK9z"
   },
   "outputs": [],
   "source": [
    "# XXX!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device\n",
    "mj=ModelJanitor(model_path, params, updatable_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "pdaulm1VdK9z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:ModelJanitor:Metadata incompatible, starting from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last checkpoint saved_params[sequence_len]: 192 != current_params[sequence_len]: 256,\n",
      "cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\n",
      "Aborting import.\n",
      "22.94992 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"creating model...\")\n",
    "model_cpu = MultiHeadSelfAttention(params['vocab_size'], params['embedding_size'], \n",
    "                                   params['sequence_len'], params['dropout'], \n",
    "                                   params['heads'], params['mhsa_layers'], params['causal'], device)\n",
    "model = model_cpu.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "ep, ls = mj.load_checkpoint(model, optimizer, params)\n",
    "if ep==0 and ls==0:\n",
    "    start_iter = 0\n",
    "else:\n",
    "    start_iter = ep\n",
    "    current_loss = ls\n",
    "    \n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "QnMCWf5AZn1-"
   },
   "outputs": [],
   "source": [
    "# @torch.jit.script\n",
    "# @torch.compile\n",
    "def do_train_step(xb, yb):\n",
    "    model.train()\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZpMI7_iMdR6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "................"
     ]
    }
   ],
   "source": [
    "dt0 = time.time()\n",
    "print(\"training...\")\n",
    "gen_id = 0\n",
    "iter_bench = 1\n",
    "current_loss = estimate_loss(device)\n",
    "inputs = [\"what is the difference between good and evil? \", \"How did everything come into existence? \", \"What was the beginning of time? \", \"How are physics, quantum-mechanics and consciousness related? \", \"How to attain complete self-awareness? \", \"What is the nature of reality? \", \"How be a good human being? \"]\n",
    "for iter in range(start_iter, params['max_iterations']):\n",
    "    print(f\"\\rIteration: {iter+1:5d}/{((iter+1)//params['sample_every_n_iterations']+1)*params['sample_every_n_iterations']}/{params['max_iterations']}\", end=\"\", flush=True)\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if (iter + 1) % params['sample_every_n_iterations'] == 0 or iter == params['max_iterations'] - 1:\n",
    "        dt = time.time()\n",
    "        print(f\"\\rloss eval\", end=\"\", flush=True)\n",
    "        current_loss = estimate_loss(device)\n",
    "        print(\n",
    "            f\"step {iter+1}: train loss {current_loss:.4f}, time {(dt-dt0)/iter_bench:.3f} sec/iter\"\n",
    "        )\n",
    "        iter_bench = 1\n",
    "        print(\"Sample: \", end=\"\", flush=True)\n",
    "        for temperature in [0.75]:\n",
    "            print(f\"--------temperature: {temperature} ---------\")\n",
    "            prompt = inputs[gen_id%len(inputs)]\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            generate_sample(td, device, prompt=prompt, toks=params['sample_size'], temperature=temperature, top_k=16)\n",
    "        print(\"-------------------------------------------\")\n",
    "        gen_id += 1\n",
    "        dt0 = time.time()\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_torch_batch(td, params['batch_size'], device, \"train\")\n",
    "    # evaluate the loss\n",
    "    do_train_step(xb, yb)\n",
    "    start_iter = iter\n",
    "    iter_bench += 1\n",
    "    if (iter+1)%params['save_every_n_iterations'] == 0:\n",
    "        mj.save_checkpoint(model, optimizer, params, iter, current_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "othN-Vnt5EZT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for t in [0.75, 0.85, 0.95, 1.05]:\n",
    "    print(f\"------Temperature {t}--------\")\n",
    "    generate_sample(td, device, prompt=\"How are consciousness and quantum mechanics related?\", toks=200, temperature=t, top_k=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VmWbteSFQtfq",
    "yWE_ZZMKEARV"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "torch_transformer_poet.ipynb",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
