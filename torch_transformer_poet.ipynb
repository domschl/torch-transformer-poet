{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-transformer-poet/blob/main/torch_transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Torch-Transformer-Poet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DabS0VZ-1Zp0"
   },
   "source": [
    "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gF-7qFzMdnN1"
   },
   "outputs": [],
   "source": [
    "!pip install -U ml-indie-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtpy59Yq-Qfz"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # from: https://github.com/pytorch/pytorch/issues/107960  (libcuda not found)\n",
    "    !export LC_ALL=\"en_US.UTF-8\"\n",
    "    !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "    !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "    !ldconfig /usr/lib64-nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5T4m6earb1e"
   },
   "outputs": [],
   "source": [
    "from ml_indie_tools.env_tools import MLEnv\n",
    "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
    "\n",
    "from ml_indie_tools.Calibre_Dataset import Calibre_Dataset\n",
    "from ml_indie_tools.Folder_Dataset import Folder_Dataset\n",
    "\n",
    "import ml_indie_tools.pytorch_meta_tools as MJ\n",
    "from ml_indie_tools.train_utils import TrainUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional experimental event server to record and propagate training progress, not (yet) recommended!\n",
    "# Functionality is ignored by default.\n",
    "try:\n",
    "    import indralib\n",
    "    indra_avail = True\n",
    "except Exception as e:\n",
    "    indra_avail = False\n",
    "if indra_avail is True:\n",
    "    print(\"Indralib is available, trying to connect to Indrajala server for training progress reports...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVcwvURB5EZN"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.Logger(\"Main\")\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "A pytorch deep multi-head attention model for text generation following Andrej Karpathy's [video-lecture-ng](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py)\n",
    "\n",
    "This code can use either CPU, GPU, or Apple Silicon. Google Colab is supported too, select the corresponding Colab runtime (menu: **`Runtime / Change runtime type`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llPw84PkEAP2"
   },
   "outputs": [],
   "source": [
    "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
    "\n",
    "ml_env = MLEnv(platform='pt', accelerator='fastest')\n",
    "ml_env.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Qg3ZPBmC8kO"
   },
   "source": [
    "## 1. Project configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-TP3Pnsrb1f"
   },
   "outputs": [],
   "source": [
    "# project_name = 'women_writers'\n",
    "# project_name='research'\n",
    "project_name='neo_philosophers'\n",
    "model_cpu = None\n",
    "model_name=f'tr_{project_name}_v3_pt'\n",
    "\n",
    "use_preprocessed_data = True                      # Use already tokenized data\n",
    "use_existing_model_from_checkpoint = False         # Try to load checkpoint of training\n",
    "use_torch_compile = False                           # Requires a modern graphics card with torch compile backend support\n",
    "skip_additional_texts = False                       # Don't look for other data sources in `additional_texts.json`\n",
    "\n",
    "if 'google.colab' in sys.modules:  # Google colab notebooks run on server that provide UTC time, we adapt logs to local time:\n",
    "    local_timezone = ZoneInfo('Europe/Berlin')\n",
    "else:\n",
    "    local_timezone = None\n",
    "\n",
    "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
    "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools\n",
    "#\n",
    "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
    "\n",
    "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device\n",
    "\n",
    "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
    "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
    "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
    "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
    "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  2.1 Text data from Project Gutenberg\n",
    "\n",
    "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training,\n",
    "encoding, batch generation, and formatted source display. It read some\n",
    "books from Project Gutenberg and supports creation of training batches.\n",
    "The output functions support highlighting to allow to compare generated\n",
    "texts with the actual sources to help to identify identical (memorized)\n",
    "parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BF8eyWnCrb1h"
   },
   "outputs": [],
   "source": [
    "token_file = os.path.join(data_path,f\"{project_name}_tokens.json\")\n",
    "if use_preprocessed_data is True:\n",
    "    if os.path.exists(token_file):\n",
    "        td = Text_Dataset()\n",
    "        td.load_tokenizer(token_file)\n",
    "    else:\n",
    "        use_preprocessed_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C66X7ynnrb1h"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False:\n",
    "    cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
    "    gd = Gutenberg_Dataset(cache_dir=cache_dir)\n",
    "\n",
    "    if project_name == 'women_writers':  # sample searches\n",
    "        search_spec= {\n",
    "            \"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"],\n",
    "            \"language\": [\"english\"]\n",
    "        }\n",
    "        book_list=gd.search(search_spec)\n",
    "    elif project_name == 'neo_philosophers':\n",
    "        search_spec = {\n",
    "            \"author\": [\"Immanuel Kant\", \"Friedrich Nietzsche\", \"Wilhelm Hegel\", \"Arthur Schopenhauer\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list=gd.search(search_spec)\n",
    "        search_spec = {\n",
    "            \"author\": [\"Plato\", \"Platon\"],\n",
    "            \"title\": [\"Timaeus\", \"Critias\", \"Symposium\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list+=gd.search(search_spec)\n",
    "        search_spec = {\n",
    "            \"title\": [\"Buddh\", \"Sutra\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list+=gd.search(search_spec)\n",
    "    else:\n",
    "        search_spec = {}\n",
    "        book_list = []\n",
    "\n",
    "    book_cnt = len(book_list)\n",
    "    print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
    "\n",
    "    if book_cnt > 0:\n",
    "        if book_cnt<80:\n",
    "            # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts,\n",
    "            # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
    "            book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)\n",
    "        else:\n",
    "            logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")\n",
    "\n",
    "        for i in range(len(book_list)):\n",
    "            if 'author' not in book_list[i]:\n",
    "                book_list[i]['author']='unknown'\n",
    "            print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")\n",
    "\n",
    "        if project_name == 'women_writers':\n",
    "            select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
    "            sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
    "        else:\n",
    "            sub_book_list = book_list\n",
    "\n",
    "        print(\"Using:\")\n",
    "        for i in range(len(sub_book_list)):\n",
    "            if 'author' not in sub_book_list[i]:\n",
    "                sub_book_list[i]['author']='unknown'\n",
    "            print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
    "\n",
    "        td = Text_Dataset(sub_book_list)\n",
    "    else:\n",
    "        td = Text_Dataset()()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxNIc7gL9UNg"
   },
   "source": [
    "## 2.2 Additional training material from folders or Calibre library\n",
    "\n",
    "This looks for a file `additional_texts.json` in the `project_path` as shown above.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"local_texts\": [[\"/some/directory/that/contains/texts\", [\".txt\", \".md\", \".org\", \".py\"]],\n",
    "  \"calibre\": [\"/home/myuser/Calibre Library\", []]\n",
    "}\n",
    "```\n",
    "\n",
    "If the folder(s) defined in `local_texts` contain text files with default endings `.txt`, `.md`, `.org`, or `.py` (can be configured), they are added to the training data. Folders are searched recursively.\n",
    "\n",
    "If the path defined in `calibre` contains a Calibre database, all text files (`.txt` only) within that library are added to the training data. The list argument can contain search-specs (see ml-indie-tools, calibre_dataset) to qualify which books to import, e.g. [{\"tags\": [\"philosophy\"]}] would import all books that are tagged with \"Philosophy\" within calibre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NYdjlW65EZP"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False and skip_additional_texts is False:\n",
    "    additional = os.path.join(project_path, \"additional_texts.json\")\n",
    "    print(f\"Looking for description of additional sources in {additional}\")\n",
    "    if os.path.exists(additional) is True:\n",
    "        with open(additional, 'r') as f:\n",
    "            add_desc = json.load(f)\n",
    "            if 'local_texts' in add_desc:\n",
    "                fd = Folder_Dataset()\n",
    "                for text_path, qualifier in add_desc['local_texts']:\n",
    "                    if not isinstance(qualifier, list):\n",
    "                        qualifier = [qualifier]\n",
    "                    print(f\"Loading texts from {text_path} using extension restriction {qualifier}\")\n",
    "                    fd.load_index(text_path, use_aliases=False, min_file_size=2048, file_extensions=qualifier)\n",
    "                td.load_texts(fd.records[:10000])\n",
    "            if 'calibre' in add_desc:\n",
    "                cal_path, specs = add_desc['calibre']\n",
    "                if os.path.exists(cal_path):\n",
    "                    print(f\"Loading text from calibre at {cal_path}\")\n",
    "                    cd = Calibre_Dataset(cal_path)\n",
    "                    cd.load_index(max_file_size=100000000) \n",
    "                    if specs is not None and len(specs)!=0:\n",
    "                        ls = cd.search(specs)\n",
    "                        td.load_texts(ls)\n",
    "                    else:\n",
    "                        td.load_texts(cd.records[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSm4f9NSC8kQ"
   },
   "source": [
    "## 2.3 Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsyBjqFyC8kQ"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False:\n",
    "    MAX_TOKENS = 32768  # This becomes vocab_size\n",
    "    MAX_NGRAM_LEN = 5   # Max length of a token\n",
    "    CHUNK_SIZE = 500000 # Split larger texts in chunks, if not None\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"Starting tokenizer with token length from 1..{MAX_NGRAM_LEN} with a max of {MAX_TOKENS} unique tokens,\")\n",
    "    print(\"this can take considerable time...\")\n",
    "\n",
    "    # Better tested NGRAM tokenizer:\n",
    "    # td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS) \n",
    "    # or alternative 'BYTEGRAM' (more experimental, can encode arbitrary UTF-8)\n",
    "    # td.init_tokenizer(tokenizer='bytegram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS, chunk_size=CHUNK_SIZE)\n",
    "    td.init_tokenizer(tokenizer='bytegram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS, chunk_size=CHUNK_SIZE)\n",
    "    td.save_tokenizer(token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_tests = [\"Good morning, this is a simple test sentence for tokenization\", \"Guten Morgen, dies is ein einfach Testsatz zur Aufteilung in Satzbestandteile\", \"སེམས་ཉིད་ངལ་བསོ་རྒྱུད་\"]\n",
    "for test in tok_tests:\n",
    "    enc = td.encode(test)\n",
    "    dec = td.decode(enc)\n",
    "    if dec != test:\n",
    "        print(f\"Tokenizer failed for: {test} -> {dec}\")\n",
    "    else:\n",
    "        r = len(enc)/len(test)*100.0\n",
    "        print(f\"Tokenizer: {test}({len(test)}) -> {enc}({len(enc)}) OK, compressed size: {r:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG03WA_yC8kR"
   },
   "source": [
    "## 3. Model metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPMwIn2gC8kR"
   },
   "outputs": [],
   "source": [
    "params = None\n",
    "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'current_loss',\n",
    "                 'sample_every_n_iterations', 'sample_size', 'save_every_n_iterations', 'max_iterations']\n",
    "attn_layers = 4\n",
    "dims = 256\n",
    "sequence_length = 256\n",
    "\n",
    "params = { # Multi-head self-attention\n",
    "        'meta_name_template': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
    "\n",
    "        'mhsa_layers': attn_layers,\n",
    "        'heads': 16,\n",
    "        'vocab_size': td.get_unique_token_count(),\n",
    "        'sequence_len': sequence_length,\n",
    "        'dropout': 0.1,\n",
    "        'embedding_size': dims,\n",
    "        'test_iterations': 100,  # number of epocs for loss estimation\n",
    "        'use_recur': False,\n",
    "        'share_weights': False,\n",
    "\n",
    "        'batch_size': 48,      \n",
    "        'learning_rate': 1e-4,   # None: Set in dependence of graphics hw\n",
    "        'lr_schedule': True,\n",
    "        'lr_min': 2e-4,\n",
    "        'lr_max': 1e-3,\n",
    "        'warmup': 2000,\n",
    "        'decay': 100000,\n",
    "        'grad_clip': 0.3,\n",
    "\n",
    "        'sample_every_n_iterations': 2048,\n",
    "        'sample_size': 128,\n",
    "        'save_every_n_iterations': 4096,\n",
    "\n",
    "        'max_iterations': 100000000  # maximum number of training iterations\n",
    "    }\n",
    "\n",
    "model_file_path = MJ.get_model_filename(model_path)\n",
    "if use_existing_model_from_checkpoint is True:\n",
    "    params = MJ.load_model_metadata_from_checkpoint(params, updatable_keys, model_file_path, device=device, log=log) # torch.device('cpu'))\n",
    "if params == None or use_existing_model_from_checkpoint is False:\n",
    "    use_existing_model_from_checkpoint = False\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U1R4yDlC8kR"
   },
   "source": [
    "## 4. Batch handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7_tc2Lirb1i"
   },
   "outputs": [],
   "source": [
    "joint_training=0\n",
    "td.init_getitem(sample_type='encoded', sample_length=params['sequence_len']+1+joint_training, content_stepping=1)\n",
    "num_records = len(td)\n",
    "print(f\"{num_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZbbsNm0cOeW"
   },
   "outputs": [],
   "source": [
    "def get_sample_sub_batch(sample_batch, batch_size, sub_index=0):\n",
    "    joint_training=0\n",
    "    for i in range(batch_size):\n",
    "        Xi = sample_batch[sub_index:-1-joint_training+sub_index]\n",
    "        yi = sample_batch[sub_index+1:]\n",
    "        if i==0:\n",
    "            # smpX=np.array(Xi, dtype=np.float32)\n",
    "            smpX=np.array(Xi, dtype=np.int32)\n",
    "            smpy=np.array(yi, dtype=np.int32)\n",
    "        else:\n",
    "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
    "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
    "    return np.array(smpX), np.array(smpy)\n",
    "\n",
    "def get_sample_batch(td, batch_size):\n",
    "    sample_batch = td.get_random_item()\n",
    "    return get_sample_sub_batch(sample_batch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jY3hUuhQYzdT"
   },
   "outputs": [],
   "source": [
    "num_batches = num_records // params['batch_size']\n",
    "print(f\"num_batches = {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_sample_batch(td, 2)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgVHUkbhdK9y"
   },
   "outputs": [],
   "source": [
    "sample_data = None\n",
    "\n",
    "def get_torch_subbatch(td, batch_size, device, split=None, sub_index=0):\n",
    "    global sample_data\n",
    "    if sub_index==0:\n",
    "        sample_data = td.get_random_item()\n",
    "    x, y = get_sample_sub_batch(sample_data, batch_size, sub_index)\n",
    "    tx = torch.tensor(x, dtype=torch.long).to(device)\n",
    "    tx.requires_grad = False\n",
    "    ty = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    ty.requires_grad = False\n",
    "    return tx, ty\n",
    "\n",
    "def get_torch_batch(td, batch_size, device, split=None):\n",
    "    x, y = get_sample_batch(td, batch_size)\n",
    "    tx = torch.tensor(x, dtype=torch.long).to(device)\n",
    "    tx.requires_grad = False\n",
    "    ty = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    ty.requires_grad = False\n",
    "    return tx, ty\n",
    "\n",
    "def get_zero_state(batch_size, sequence_len, hidden_size, device):\n",
    "    zstate = torch.zeros(batch_size, sequence_len, hidden_size, device=device)\n",
    "    zstate.requires_grad = False\n",
    "    return zstate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pvbi6kjXC8kS"
   },
   "source": [
    "## 5. Loss and training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000, device=None):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model, device=device)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "class MultiHeadSelfAttentionWithMemory(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_size,\n",
    "        sequence_len,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        dropout=0.1,\n",
    "        use_recur=False,\n",
    "        share_weights=True,\n",
    "        device=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if device is None:\n",
    "            raise ValueError(\n",
    "                \"Device is None at MultiHeadSelfAttentionWithMemory\"\n",
    "            )\n",
    "        self.device = device\n",
    "        self.sequence_len = sequence_len\n",
    "        self.use_recur = use_recur\n",
    "        context_sub_layers = num_layers // 2\n",
    "        self.context_sub_layers = context_sub_layers\n",
    "        dims = embedding_size\n",
    "        self.dims = dims\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.position_embedding_table = nn.Embedding(\n",
    "            sequence_len, embedding_size, device=device\n",
    "        )\n",
    "\n",
    "        if share_weights is True:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "            self.out_proj = nn.Linear(embedding_size, vocab_size, bias=False)\n",
    "            self.embedding.weight = self.out_proj.weight  # torch.nn.Parameter(self.out_proj.weight.transpose(1,0))\n",
    "            self.out_proj = self.out_proj.to(device)\n",
    "            self.embdding = self.embedding.to(device)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_size, device=device)\n",
    "            self.out_proj = nn.Linear(embedding_size, vocab_size, device=device)\n",
    "\n",
    "\n",
    "        if use_recur is True:\n",
    "            self.rec=nn.RNN(dims, dims, num_layers=3, batch_first=True, device=device)\n",
    "            self.lq=nn.Linear(dims, dims, device=device)\n",
    "            self.lk=nn.Linear(dims, dims, device=device)\n",
    "            self.lv=nn.Linear(dims, dims, device=device)\n",
    "            self.lnorm=nn.BatchNorm1d(dims, device=device)\n",
    "            self.sm = nn.Softmax(dim=2)\n",
    "            self.proj1 = nn.Linear(dims, dims, device=device)\n",
    "            self.sm2 = nn.Softmax(dim=2)\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=dims, nhead=num_heads, dim_feedforward=dims*4, dropout=dropout, batch_first=True, device=device)\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=context_sub_layers)\n",
    "            encoder_layer2 = nn.TransformerEncoderLayer(d_model=dims, nhead=num_heads, dim_feedforward=dims*4, dropout=dropout, batch_first=True, device=device)\n",
    "            self.transformer2 = nn.TransformerEncoder(encoder_layer2, num_layers=num_layers - context_sub_layers)\n",
    "        else:\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=dims, nhead=num_heads, activation=\"gelu\", dim_feedforward=dims*4, dropout=dropout, batch_first=True, device=device)\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_proj.bias.data.zero_()\n",
    "        self.out_proj.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, D = idx.shape\n",
    "        tok_emb = self.embedding(idx)\n",
    "        # idx and targets are both (B,D) tensor of integers\n",
    "        # tok_emb = self.token_embedding_table(idx)  # (B,D,C)\n",
    "\n",
    "        # XXX: move to init, make not trainable:\n",
    "        if self.device is None:\n",
    "            pos_emb = self.position_embedding_table(torch.arange(self.sequence_len))\n",
    "        else:\n",
    "            pos_emb = self.position_embedding_table(\n",
    "                torch.arange(D, device=self.device)\n",
    "            )  # (D,C)\n",
    "\n",
    "        x = tok_emb + pos_emb  # (B,D,C)\n",
    "        \n",
    "        # x = self.pos_encoder(x) \n",
    "        x_mask = nn.Transformer.generate_square_subsequent_mask(D).to(self.device)\n",
    "        x = self.transformer(x, x_mask)\n",
    "\n",
    "        if self.use_recur is True:\n",
    "            skip = x\n",
    "            x = self.rec(x)[0] + x\n",
    "            xk = self.lk(x).permute((0,2,1))\n",
    "            xv = self.lv(x)\n",
    "            xq = self.lq(x)\n",
    "            xqk = torch.matmul(xq, xk)\n",
    "            sm = self.sm(xqk)/math.sqrt(D)\n",
    "            att = torch.matmul(sm, xv)\n",
    "            x = self.lnorm(att)\n",
    "            x = self.sm2(self.proj1(x))\n",
    "            x = x + skip    \n",
    "            x = self.transformer2(x, x_mask)\n",
    "            \n",
    "        logits = self.out_proj(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate new tokens given a context\n",
    "\n",
    "        Note: for apple MPS, top_k is limited max 16 vor older torchs! ((01/2023) implementation limitation)\n",
    "        See: https://github.com/pytorch/pytorch/issues/78915\n",
    "        Solved in: https://github.com/pytorch/pytorch/pull/94639 (03/2023)\n",
    "\n",
    "        :param idx: the context (B,T) tensor of indices\n",
    "        :param max_new_tokens: the maximum number of tokens to generate\n",
    "        :param temperature: the temperature to use for sampling\n",
    "        :param top_k: the number of top tokens to consider\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last sequence_len tokens\n",
    "            idx_cond = idx[:, -self.sequence_len :]\n",
    "            # print(idx_cond.shape)\n",
    "            # get the predictions\n",
    "            logits = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply temperature\n",
    "            if temperature != 1.0 and temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdaulm1VdK9z"
   },
   "outputs": [],
   "source": [
    "print(\"creating model...\")\n",
    "try:\n",
    "    # Colab + torch 2 -> lots of garbage.\n",
    "    if model is not None:\n",
    "        del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "model = MultiHeadSelfAttentionWithMemory(vocab_size=params['vocab_size'], embedding_size=params['embedding_size'],\n",
    "                                       sequence_len=params['sequence_len'],\n",
    "                                       num_heads=params['heads'], num_layers=params['mhsa_layers'], dropout=params['dropout'],\n",
    "                                       use_recur=params['use_recur'], share_weights=params['share_weights'], device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "model = model.to(device)\n",
    "if use_existing_model_from_checkpoint is True:\n",
    "    params_load = MJ.load_checkpoint(params, model, optimizer, file_path=model_file_path, updatable_keys=updatable_keys, device=device, log=log) # torch.device(\"cpu\"))\n",
    "    if params_load is not None:\n",
    "        params = params_load\n",
    "model = model.to(device)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "if use_torch_compile is True:\n",
    "    if device == 'cuda':\n",
    "        print(\"Compiling...\")\n",
    "        model = torch.compile(model)\n",
    "        print(\"Compile ok.\")\n",
    "        try:\n",
    "            torch.set_float32_matmul_precision('high')\n",
    "        except:\n",
    "            print(\"Seems no tensor cores for that.\")\n",
    "    # elif str(device) == 'mps':\n",
    "    #     print(\"Compiling...\")\n",
    "    #     model = torch.compile(model)\n",
    "    #     print(\"Compile ok.\")\n",
    "\n",
    "if 'current_epoch' in params:\n",
    "    ep = params['current_epoch']\n",
    "else:\n",
    "    ep=0\n",
    "if 'current_loss' in params:\n",
    "    ls = params['current_loss']\n",
    "else:\n",
    "    ls=0\n",
    "\n",
    "if ep==0 and ls==0:\n",
    "    start_iter = 0\n",
    "else:\n",
    "    start_iter = ep\n",
    "    current_loss = ls\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnMCWf5AZn1-"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(device):\n",
    "    # XXX: this does take data for train and val from SAME pool!\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(params['test_iterations'])\n",
    "        for k in range(params['test_iterations']):\n",
    "            # if k % (params['test_iterations']/10 + 1) == 0:\n",
    "            #     print(\".\", end=\"\", flush=True)\n",
    "            X, Y = get_torch_batch(td, params['batch_size'], device, split)\n",
    "            logits = model(X)\n",
    "            loss = get_loss(logits, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    print(\"\\r\", end=\"\", flush=True)\n",
    "    mloss = (out['train']+out['val'])/2.0\n",
    "    return mloss\n",
    "\n",
    "def generate_sample(td, device, prompt=' ', toks=100, state=None, temperature=1.0, top_k=None, pad=True):\n",
    "    # generate from the model\n",
    "    # context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    model.eval()\n",
    "    if pad is True:\n",
    "        while len(prompt)<params['sequence_len']*4:\n",
    "            if len(prompt)==params['sequence_len']*4-1:\n",
    "                prompt = '\\n' + prompt\n",
    "            else:\n",
    "                prompt = ' ' + prompt\n",
    "    context = torch.tensor([td.encode(prompt)]).to(device)\n",
    "    answer = model.generate(context, max_new_tokens=toks, temperature=temperature, top_k=top_k)\n",
    "    txt = td.decode(answer[0].tolist())\n",
    "    # Identify memorisation of text by highlighting verbatim quotes from sources\n",
    "    # that are longer than 10 chars. HTML colorcoded output for source identification:\n",
    "    td.source_highlight(txt, min_quote_size=10, dark_mode=False, display_ref_anchor=False)\n",
    "    model.train()\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2uWm6CTC8kT"
   },
   "outputs": [],
   "source": [
    "# @torch.jit.script\n",
    "# @torch.compile\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def get_loss(logits, yb):\n",
    "    output_flat = logits.reshape(-1, params['vocab_size'])\n",
    "    # output_flat = logits.view(-1, params['vocab_size'])\n",
    "    # print(output_flat.shape)\n",
    "    ybr = yb.reshape(-1)\n",
    "    # print(ybr.shape)\n",
    "    loss = criterion(output_flat, ybr)\n",
    "    return loss\n",
    "    \n",
    "def do_train_step(xb, yb, device, state=None):\n",
    "    model.train()\n",
    "    logits = model(xb)\n",
    "    loss = get_loss(logits, yb)\n",
    "    \n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), params['grad_clip']).cpu()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_tu_session():\n",
    "    if indra_avail is True:\n",
    "        with open('indra_creds.json', 'r') as f:\n",
    "            creds = json.load(f)\n",
    "            tu = TrainUtils(indra_server_profile_name='default', username=creds['username'], password=creds['password'])\n",
    "    else:\n",
    "        tu = TrainUtils()\n",
    "    tu.train_session_start(model_name=model_name, model_description=\"Torch-poet tests\", model_version=1, model_params=params, indra_subdomain=\"torch_poet/first_tests/1\", status_string_size=110)\n",
    "    return tu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(optim, n_iter, warmup, max_lr, decay, min_lr):\n",
    "    if n_iter<warmup and warmup>0:\n",
    "        lr = (n_iter+1)/warmup*max_lr\n",
    "    elif n_iter<warmup+decay and decay>0:\n",
    "        i = n_iter-warmup\n",
    "        lr = (decay-i)/decay*(max_lr-min_lr)+min_lr\n",
    "    else:\n",
    "        lr = min_lr\n",
    "\n",
    "    for g in optim.param_groups:\n",
    "        g['lr'] = lr\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZpMI7_iMdR6",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_utils):\n",
    "    global start_iter\n",
    "    dt0 = time.time()\n",
    "    sdt = datetime.datetime.now(tz=local_timezone).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"training, start at {sdt}...\")\n",
    "    gen_id = 0\n",
    "    last_print=0\n",
    "    iter_bench = 1\n",
    "    tu = train_utils\n",
    "    lr = params['learning_rate']\n",
    "    # current_loss = estimate_loss(device)\n",
    "    inputs = [\"What is the difference between good and evil? The difference \", \"How did everything come into existence? The origin \", \"What was at the beginning of time? Time itself \", \"How are physics, quantum-mechanics and consciousness related? The relation between \", \"How to attain complete self-awareness? Complete \", \"What is the nature of reality? The nature \", \"How be a good human being? A human \"]\n",
    "    for iter in range(start_iter, params['max_iterations']):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if (iter + 1) % params['sample_every_n_iterations'] == 0 or iter == params['max_iterations'] - 1:\n",
    "            dt = time.time()\n",
    "            print(f\"\\rloss eval\", end=\"\", flush=True)\n",
    "            current_loss = estimate_loss(device)\n",
    "            print(\n",
    "                f\"step {iter+1}: train loss {current_loss:.4f}, time {(dt-dt0)/iter_bench:.3f} sec/iter                       \"\n",
    "            )\n",
    "            iter_bench = 1\n",
    "            sdt = datetime.datetime.now(tz=local_timezone).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"Sample at {sdt}:\", flush=True)\n",
    "            for temperature in [0.75]:\n",
    "                print(f\"--------temperature: {temperature} ---------\")\n",
    "                prompt = inputs[gen_id%len(inputs)]\n",
    "                print(f\"Prompt: {prompt}\")\n",
    "                generate_sample(td=td, device=device, prompt=prompt, toks=params['sample_size'], temperature=temperature, top_k=4)\n",
    "            print(\"-------------------------------------------\")\n",
    "            gen_id += 1\n",
    "            dt0 = time.time()\n",
    "    \n",
    "        if params['lr_schedule'] is True:\n",
    "            lr = lr_schedule(optimizer, iter, params['warmup'], params['lr_max'], params['decay'], params['lr_min'])\n",
    "    \n",
    "        xb, yb = get_torch_batch(td, params['batch_size'], device, \"train\")\n",
    "        cur_loss, cur_norm = do_train_step(xb, yb, device=device)\n",
    "\n",
    "        \n",
    "        nt = time.time()\n",
    "        if (nt-last_print)>1:\n",
    "            rec = {\n",
    "                'epoch': iter/num_batches,\n",
    "                'batch': iter%params['sample_every_n_iterations'],\n",
    "                'num_batches': params['sample_every_n_iterations'],\n",
    "                'loss': cur_loss,\n",
    "                'learning_rate': lr,\n",
    "                'gradient_norm': cur_norm.item(),\n",
    "            }\n",
    "            status_string, record = train_utils.train_state(rec)\n",
    "            print(status_string, end=\"\\r\")\n",
    "            last_print=nt\n",
    "        \n",
    "        start_iter = iter\n",
    "        iter_bench += 1\n",
    "        if (iter+1)%params['save_every_n_iterations'] == 0:\n",
    "            MJ.save_checkpoint(params, model, optimizer, iter, current_loss, file_path=model_file_path, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tu = start_tu_session()\n",
    "try:\n",
    "    train(train_utils = tu)\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\nTraining interrupted.\")\n",
    "tu.train_session_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "othN-Vnt5EZT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for t in [0.5, 1.5]:\n",
    "#     print(f\"------Temperature {t}--------\")\n",
    "#     generate_sample(td, device, prompt=\"How are consciousness and quantum mechanics related?\", toks=150, temperature=t, top_k=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-code below, unfinished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UNG5wWhC8kU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "enc_texts = []\n",
    "for i in range(500):\n",
    "    e = td[i*50000][:256]\n",
    "    tx = torch.tensor([e]).to(device)\n",
    "    enc_texts.append(tx)\n",
    "    texts.append(td.decode(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_texts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADm9ycuA2ik7"
   },
   "outputs": [],
   "source": [
    "emb_text = []\n",
    "cont_text = []\n",
    "for et in enc_texts:\n",
    "    emb_text.append(model.embedding(et))\n",
    "    cont_text.append(model.context(et))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_text[0].shape, cont_text[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_vec = []\n",
    "cont_vec = []\n",
    "for i in range(len(emb_text)):\n",
    "    emb_vec.append(emb_text[i][0].sum(axis=0))\n",
    "    cont_vec.append(cont_text[i][0].sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_vec(a, b):\n",
    "    al = torch.sqrt(torch.dot(a,a))\n",
    "    bl = torch.sqrt(torch.dot(b,b))\n",
    "    an = a/al\n",
    "    bn = b/bl\n",
    "    return torch.dot(an,bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0):\n",
    "    best = -10.0\n",
    "    ind = -1\n",
    "    for j in range(len(emb_vec)):\n",
    "        if i==j:\n",
    "            continue\n",
    "        cos_val = cos_vec(emb_vec[i], emb_vec[j])\n",
    "        if cos_val > best:\n",
    "            best = cos_val\n",
    "            ind = j\n",
    "    # print(f\"{texts[i][:20]} ->{best}: {texts[ind][:20]}\")\n",
    "print()        \n",
    "for i in range(200):\n",
    "    best = 0\n",
    "    ind = -1\n",
    "    for j in range(len(emb_vec)):\n",
    "        if i==j:\n",
    "            continue\n",
    "        cos_val = cos_vec(cont_vec[i], cont_vec[j])\n",
    "        if cos_val > best:\n",
    "            best = cos_val\n",
    "            ind = j\n",
    "    print(f\"{texts[i]} \\n\\n->{best}:\\n\\n {texts[ind]}\")\n",
    "    print(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td[200002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VmWbteSFQtfq",
    "yWE_ZZMKEARV"
   ],
   "gpuClass": "premium",
   "gpuType": "V100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "torch_transformer_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
