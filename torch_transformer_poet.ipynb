{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-transformer-poet/blob/main/torch_transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Torch-Transformer-Poet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DabS0VZ-1Zp0"
   },
   "source": [
    "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtpy59Yq-Qfz",
    "outputId": "cf29265d-e4ce-4c03-a06f-794f188b3384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml-indie-tools in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (0.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ml-indie-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U5T4m6earb1e"
   },
   "outputs": [],
   "source": [
    "from ml_indie_tools.env_tools import MLEnv\n",
    "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
    "\n",
    "from ml_indie_tools.pytorch_custom_layers import MultiHeadSelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "A pytorch deep multi-head attention model for text generation following Andrej Karpathy's [video-lecture-ng](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py)\n",
    "\n",
    "This code can use either CPU, GPU, or Apple Silicon. Google Colab is supported too, select the corresponding Colab runtime (menu: **`Runtime / Change runtime type`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "llPw84PkEAP2",
    "outputId": "477151a2-c9d4-4916-a714-6ebfa21745ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OS: Darwin, Python: 3.10.8 (Conda), Jupyter Notebook Pytorch: 2.0.0.dev20230121, GPU: MPS Metal accelerator (system memory)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
    "\n",
    "ml_env = MLEnv(platform='pt', accelerator='fastest')\n",
    "ml_env.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-TP3Pnsrb1f",
    "outputId": "8cb516d8-460b-43d7-8a9b-3d4a4f96c33b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path (all projects) : . (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
      "Project path             : . (Changes to the file system happen only below this project path\n",
      "Model path (snapshots)   : ./model/ngpt_v1_pt (Model weights and snapshots are stored here)\n",
      "Data path (training data): ./data (Training data will be downloaded here)\n",
      "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
     ]
    }
   ],
   "source": [
    "project_name='women_writers'\n",
    "model_name='ngpt_v1_pt'\n",
    "\n",
    "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
    "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools \n",
    "#\n",
    "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
    "\n",
    "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
    "\n",
    "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
    "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
    "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
    "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
    "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  1. Text library\n",
    "\n",
    "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
    "encoding, batch generation, and formatted source display. It read some \n",
    "books from Project Gutenberg and supports creation of training batches. \n",
    "The output functions support highlighting to allow to compare generated \n",
    "texts with the actual sources to help to identify identical (memorized) \n",
    "parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BF8eyWnCrb1h"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
    "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C66X7ynnrb1h",
    "outputId": "dc0b9bd7-6d3e-4eb7-884b-9bc6842d1693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 matching books found with search {'author': ['Emily Brontë', 'Jane Austen', 'Virginia Woolf'], 'language': ['english']}.\n"
     ]
    }
   ],
   "source": [
    "# sample searches\n",
    "search_spec= {\"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"], \"language\": [\"english\"]}\n",
    "\n",
    "book_list=gd.search(search_spec)\n",
    "book_cnt = len(book_list)\n",
    "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
    "if book_cnt<40:\n",
    "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
    "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
    "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
    "else:\n",
    "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MH6_7IU3upOd",
    "outputId": "7aa638fa-c13e-4dd0-ea57-ced30c4f9ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The Common Reader - Virginia Woolf, 64457\n",
      "1: Mr. Bennett and Mrs. Brown - Virginia Woolf, 63022\n",
      "2: The Younger Sister, Volumes 1-3 - Catherine Anne Austen Hubback and Jane Austen, 54066\n",
      "3: The Younger Sister, Vol. 3 - Catherine Anne Austen Hubback and Jane Austen, 54012\n",
      "4: The Younger Sister, Vol. 2 - Catherine Anne Austen Hubback and Jane Austen, 54011\n",
      "5: The Younger Sister, Vol. 1 - Catherine Anne Austen Hubback and Jane Austen, 54010\n",
      "6: Pride and Prejudice - Jane Austen, 42671\n",
      "7: The Letters of Jane Austen - Jane Austen, 42078\n",
      "8: The Complete Project Gutenberg Works of Jane Austen - Jane Austen, 31100\n",
      "9: Jacob's Room - Virginia Woolf, 5670\n",
      "10: Pride and Prejudice - Jane Austen, 1342\n",
      "11: Night and Day - Virginia Woolf, 1245\n",
      "12: Love And Friendship And Other Early Works - Jane Austen, 1212\n",
      "13: Lady Susan - Jane Austen, 946\n",
      "14: Wuthering Heights - Emily Brontë, 768\n",
      "15: Sense and Sensibility - Jane Austen, 161\n",
      "16: Emma - Jane Austen, 158\n",
      "17: The Voyage Out - Virginia Woolf, 144\n",
      "18: Mansfield Park - Jane Austen, 141\n",
      "19: Northanger Abbey - Jane Austen, 121\n",
      "20: Persuasion - Jane Austen, 105\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(book_list)):\n",
    "    print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jBH3Z15rb1h",
    "outputId": "b63999fa-f4e8-4250-cba0-47422094fcfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Loaded 12 texts\n",
      "INFO:Datasets:Extracting ngrams of length 1..20 from text_list, selecting 20000 most used ngrams.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "1: Mr. Bennett and Mrs. Brown - Virginia Woolf\n",
      "2: Jacob's Room - Virginia Woolf\n",
      "3: Pride and Prejudice - Jane Austen\n",
      "4: Night and Day - Virginia Woolf\n",
      "5: Lady Susan - Jane Austen\n",
      "6: Wuthering Heights - Emily Brontë\n",
      "7: Sense and Sensibility - Jane Austen\n",
      "8: Emma - Jane Austen\n",
      "9: The Voyage Out - Virginia Woolf\n",
      "10: Mansfield Park - Jane Austen\n",
      "11: Northanger Abbey - Jane Austen\n",
      "12: Persuasion - Jane Austen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Encoding text corpora as ngrams.\n",
      "INFO:Datasets:Encoding text Mr. Bennett and Mrs. Brown...\n",
      "INFO:Datasets:Encoding text Jacob's Room...\n",
      "INFO:Datasets:Encoding text Pride and Prejudice...\n",
      "INFO:Datasets:Encoding text Night and Day...\n",
      "INFO:Datasets:Encoding text Lady Susan...\n",
      "INFO:Datasets:Encoding text Wuthering Heights...\n",
      "INFO:Datasets:Encoding text Sense and Sensibility...\n",
      "INFO:Datasets:Encoding text Emma...\n",
      "INFO:Datasets:Encoding text The Voyage Out...\n",
      "INFO:Datasets:Encoding text Mansfield Park...\n",
      "INFO:Datasets:Encoding text Northanger Abbey...\n",
      "INFO:Datasets:Encoding text Persuasion...\n",
      "INFO:Datasets:Encoding text corpora as ngrams done.\n"
     ]
    }
   ],
   "source": [
    "MAX_TOKENS = 20000  # This becomes vocab_size\n",
    "MAX_NGRAM_LEN = 20   # Max length of a token\n",
    "\n",
    "select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
    "sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
    "\n",
    "print(\"Using:\")\n",
    "for i in range(len(sub_book_list)):\n",
    "    print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
    "\n",
    "textlib_dataset = None  # Forces re-caching\n",
    "td = Text_Dataset(sub_book_list)\n",
    "td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "f7_tc2Lirb1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1526465 records\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LEN = 64\n",
    "# SUB_PROBABILITY = 0.15  # like BERT\n",
    "\n",
    "td.init_getitem(sample_type='encoded', sample_length=SEQUENCE_LEN+1, content_stepping=1)\n",
    "\n",
    "num_records = len(td)\n",
    "\n",
    "print(f\"{num_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "zZbbsNm0cOeW"
   },
   "outputs": [],
   "source": [
    "def get_sample_batch(td, batch_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    # y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    # return x, y\n",
    "    for i in range(batch_size):\n",
    "        data = td.get_random_item()\n",
    "        Xi = data[:-1]\n",
    "        yi = data[1:]\n",
    "        if i==0:\n",
    "            # smpX=np.array(Xi, dtype=np.float32)\n",
    "            smpX=np.array(Xi, dtype=np.int32)\n",
    "            smpy=np.array(yi, dtype=np.int32)\n",
    "        else:\n",
    "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
    "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
    "    return np.array(smpX), np.array(smpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "TI3Fx6bNuR9A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0](l=64): X=>of excitement associated with the\n",
      "thought of Cassandra once more took possession of him. No longer was it\n",
      "the excitement of anticipation and ignorance; it was the excitement of\n",
      "something greater than a possibility, for now he knew her and had\n",
      "measure of the sympathy between them. But who could give him certainty<,\n",
      "y=>xcitement associated with the\n",
      "thought of Cassandra once more took possession of him. No longer was it\n",
      "the excitement of anticipation and ignorance; it was the excitement of\n",
      "something greater than a possibility, for now he knew her and had\n",
      "measure of the sympathy between them. But who could give him certainty?<\n",
      "[1](l=64): X=>vemonth a more\n",
      "important advantage to Mrs. Price resulted from it. Mrs. Norris was\n",
      "often observing to the others that she could not get her poor sister\n",
      "and her family out of her head, and that, much as they had all done for\n",
      "her, she seemed to be wanting to do more; and at length she could not\n",
      "but own it to be her wish that poor Mrs. <,\n",
      "y=>month a more\n",
      "important advantage to Mrs. Price resulted from it. Mrs. Norris was\n",
      "often observing to the others that she could not get her poor sister\n",
      "and her family out of her head, and that, much as they had all done for\n",
      "her, she seemed to be wanting to do more; and at length she could not\n",
      "but own it to be her wish that poor Mrs. P<\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = get_sample_batch(td, 2)\n",
    "for i in range(len(test_x)):\n",
    "    xi=[int(x) for x in test_x[i]]\n",
    "    print(f\"[{i}](l={len(xi)}): X=>{td.decode(xi)}<,\\ny=>{td.decode(test_y[i])}<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "qnMxRkkmcOeX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 64), (2, 64))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30hi0UPtEAQG"
   },
   "source": [
    "## 2. data for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "jn_LcJ6g9Mzy"
   },
   "outputs": [],
   "source": [
    "def expand_name_template(template, params):\n",
    "    exp=copy.copy(template)\n",
    "    for key in params:\n",
    "        src=\"{\"+key+\"}\"\n",
    "        dst=f\"{params[key]}\"\n",
    "        exp=exp.replace(src,dst).replace('[','(').replace(']',')')\n",
    "    return exp\n",
    "\n",
    "def save_model_metadata(epoch, suffix='std'):\n",
    "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
    "    params['current_epoch'] = epoch\n",
    "    try:\n",
    "        with open(meta_file, 'w') as f:\n",
    "            f.write(json.dumps(params))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to store model metadata at {model_path}: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def read_model_metadata(suffix=\"std\"):\n",
    "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
    "    try:\n",
    "        with open(meta_file, 'r') as f:\n",
    "            meta = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot access project meta-data at {meta_file}: {e}, starting anew.\")\n",
    "        return None\n",
    "    return meta\n",
    "\n",
    "def is_metadata_compatible(params, meta):\n",
    "    is_valid=True\n",
    "    keys=set(list(params.keys())+list(meta.keys()))\n",
    "    for key in keys:\n",
    "        if key in updatable_keys:\n",
    "            continue\n",
    "        if key not in meta:\n",
    "            print(f\"Key {key} not available in last checkpoint model_meta, params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
    "            is_valid = False\n",
    "        elif key not in params:\n",
    "            print(f\"Key {key} not available in params, last checkpoint model_meta[{key}]: {meta[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
    "            is_valid = False\n",
    "        elif meta[key]!=params[key]:\n",
    "            print(f\"Last checkpoint model_meta[{key}]: {meta[key]} != params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
    "            is_valid = False\n",
    "    if is_valid is False:\n",
    "        print(\"Aborting import.\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "znpIUA3ig3gO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot access project meta-data at ./model/ngpt_v1_pt/model_meta_32x12x{units}x20000.json: [Errno 2] No such file or directory: './model/ngpt_v1_pt/model_meta_32x12x{units}x20000.json', starting anew.\n",
      "Starting new model\n",
      "{'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}', 'mhsa_layers': 32, 'heads': 12, 'causal': True, 'dropout': 0.1, 'vocab_size': 20000, 'sequence_len': 64, 'embedding_size': 144, 'test_iterations': 5, 'batch_size': 32, 'learning_rate': 0.002, 'sample_every_n_iterations': 50, 'max_iterations': 1000000}\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = td.get_unique_token_count()  # vocabulary-size\n",
    "\n",
    "lyrs = 32;\n",
    "\n",
    "params = { # Multi-head self-attention\n",
    "    'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
    "\n",
    "    'mhsa_layers': lyrs, \n",
    "    'heads': 12, # [16]*lyrs,\n",
    "    # 'units': [512]*lyrs,\n",
    "    # 'norm': 'softmax', # this is for within each head (all-you-need: softmax)\n",
    "    # 'mh_normalize': True,  # use layer-norm after concatenation (or additiona) of the heads\n",
    "    'causal': True,  # Use causal self-attention\n",
    "    # 'l2_regularizer': 1e-9,\n",
    "    'dropout': 0.1,       # no dropout: 0.0\n",
    "    # 'join_heads_by_add': True,  # stragegy how multi-heads are joined: False: concat (as in all-you-need), True: relu+add of all the heads: less resources, similar perf.\n",
    "    'vocab_size': vocabulary_size,\n",
    "    'sequence_len': SEQUENCE_LEN,\n",
    "    'embedding_size': 144, \n",
    "    'test_iterations': 5,  # number of epocs for loss estimation\n",
    "\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.002,\n",
    "    # 'clipvalue': None,\n",
    "    'sample_every_n_iterations': 50,\n",
    "    \n",
    "    'max_iterations': 1000000  # maximum number of training iterations\n",
    "}\n",
    "\n",
    "# if len(params['heads'])!=params['mhsa_layers']: # or len(params['units'])!=params['mhsa_layers']:\n",
    "#     print(\"ERROR: lenght of 'heads' and 'units' must be equal to mhsa_layers!\")\n",
    "    \n",
    "model_suffix = expand_name_template(params['name'], params)\n",
    "# Put 'important' params in checkpoint-pathname to separate model-data:\n",
    "checkpoint_dir = os.path.join(model_path, f\"training_checkpoints_{model_suffix}\")\n",
    "if os.path.exists(checkpoint_dir) is False:\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# When comparing if training-data is compatible with new params set, \n",
    "# the following keys are updatable, they can be changed while continuing\n",
    "# to use existing checkpoints and continue training with those values\n",
    "# changed:\n",
    "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'dropout', \n",
    "             'sample_every_n_epochs']\n",
    "\n",
    "# These values are taking from saved checkpoint:\n",
    "keep_keys=['current_epoch']\n",
    "\n",
    "continue_last = True\n",
    "if continue_last is False:\n",
    "    print(\"NOT continuing based on existing training! New start.\")\n",
    "\n",
    "meta = read_model_metadata(suffix=model_suffix)\n",
    "if meta is not None and is_metadata_compatible(params, meta) is True and continue_last is True:\n",
    "    for key in keep_keys:\n",
    "        if key in meta:\n",
    "            params[key]=meta[key]\n",
    "    if params is not None:\n",
    "        print(f\"Continuing last session from epoch {params['current_epoch']}\")\n",
    "    else:\n",
    "        print(f\"No previous data, starting new model\")\n",
    "else:\n",
    "    print(\"Starting new model\")\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "jY3hUuhQYzdT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches = 47702\n"
     ]
    }
   ],
   "source": [
    "num_batches = num_records // params['batch_size']\n",
    "print(f\"num_batches = {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "bgVHUkbhdK9y"
   },
   "outputs": [],
   "source": [
    "def get_torch_batch(td, batch_size, device, split=None):\n",
    "    x, y = get_sample_batch(td, batch_size)\n",
    "    return torch.tensor(x, dtype=torch.long).to(device), torch.tensor(y, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "QnMCWf5AZn1-"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(device):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(params['test_iterations'])\n",
    "        for k in range(params['test_iterations']):\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            X, Y = get_torch_batch(td, params['batch_size'], device, split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    print(\"]\\r\", end=\"\", flush=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_sample(td, toks=100, temperature=1.0):\n",
    "    # generate from the model\n",
    "    # context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    context = torch.tensor([td.encode(' ')]).to(device)\n",
    "    print(td.decode(model.generate(context, max_new_tokens=toks, temperature=temperature)[0].tolist()))\n",
    "    # open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n",
    "\n",
    "\n",
    "# @torch.compile\n",
    "def do_train_step(xb, yb):\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9hEziJ0odK9z"
   },
   "outputs": [],
   "source": [
    "# XXX!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdaulm1VdK9z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model...\n",
      "13.798208 M parameters\n",
      "training...\n",
      "step 50: train loss 9.0074, val loss 9.0103, time 1.712 sec/iter\n",
      "step 100: train loss 8.9859, val loss 8.9289, time 1.618 sec/iter\n",
      "step 150: train loss 8.9295, val loss 8.9204, time 1.631 sec/iter\n",
      "step 200: train loss 8.9475, val loss 8.9319, time 1.645 sec/iter\n",
      "step 250: train loss 8.9279, val loss 8.9157, time 1.688 sec/iter\n",
      "step 300: train loss 8.9040, val loss 8.9014, time 1.705 sec/iter\n",
      "step 350: train loss 8.8423, val loss 8.8758, time 1.711 sec/iter\n",
      "step 400: train loss 8.7424, val loss 8.7406, time 1.737 sec/iter\n",
      "step 450: train loss 8.6242, val loss 8.6082, time 1.762 sec/iter\n",
      "step 500: train loss 8.4302, val loss 8.4354, time 1.783 sec/iter\n",
      "step 550: train loss 8.3627, val loss 8.2946, time 1.791 sec/iter\n",
      "step 600: train loss 8.2417, val loss 8.2467, time 1.801 sec/iter\n",
      "step 650: train loss 8.1717, val loss 8.1916, time 1.857 sec/iter\n",
      "step 700: train loss 8.1208, val loss 8.1317, time 1.816 sec/iter\n",
      "step 750: train loss 8.0777, val loss 8.0919, time 1.646 sec/iter\n",
      "step 800: train loss 7.9958, val loss 8.0022, time 1.633 sec/iter\n",
      "step 850: train loss 7.9573, val loss 7.9286, time 1.630 sec/iter\n",
      "step 900: train loss 7.8983, val loss 7.8789, time 1.640 sec/iter\n",
      "step 950: train loss 7.7926, val loss 7.8152, time 1.675 sec/iter\n",
      "step 1000: train loss 7.7305, val loss 7.7503, time 1.662 sec/iter\n",
      "step 1050: train loss 7.7303, val loss 7.7034, time 1.711 sec/iter\n",
      "step 1100: train loss 7.6293, val loss 7.6461, time 1.790 sec/iter\n",
      "step 1150: train loss 7.5720, val loss 7.5460, time 1.742 sec/iter\n",
      "step 1200: train loss 7.4937, val loss 7.5488, time 1.773 sec/iter\n",
      "step 1250: train loss 7.4869, val loss 7.4465, time 1.849 sec/iter\n",
      "step 1300: train loss 7.3482, val loss 7.3293, time 1.802 sec/iter\n",
      "step 1350: train loss 7.2664, val loss 7.2810, time 1.861 sec/iter\n",
      "step 1400: train loss 7.2208, val loss 7.2756, time 1.890 sec/iter\n",
      "step 1450: train loss 7.1469, val loss 7.1241, time 1.737 sec/iter\n",
      "step 1500: train loss 7.1196, val loss 7.1177, time 1.759 sec/iter\n",
      "step 1550: train loss 6.9495, val loss 6.9672, time 1.793 sec/iter\n",
      "step 1600: train loss 6.8933, val loss 6.8926, time 1.761 sec/iter\n",
      "step 1650: train loss 6.8881, val loss 6.8741, time 1.779 sec/iter\n",
      "step 1700: train loss 6.8773, val loss 6.7927, time 1.800 sec/iter\n",
      "step 1750: train loss 6.7531, val loss 6.7433, time 1.781 sec/iter\n",
      "step 1800: train loss 6.6093, val loss 6.5995, time 1.820 sec/iter\n",
      "step 1850: train loss 6.6268, val loss 6.6290, time 1.875 sec/iter\n",
      "step 1900: train loss 6.5159, val loss 6.5066, time 1.819 sec/iter\n",
      "step 1950: train loss 6.4826, val loss 6.5428, time 1.926 sec/iter\n",
      "step 2000: train loss 6.4451, val loss 6.3857, time 1.908 sec/iter\n",
      "step 2050: train loss 6.4119, val loss 6.4088, time 1.864 sec/iter\n",
      "step 2100: train loss 6.3503, val loss 6.3239, time 1.791 sec/iter\n",
      "step 2150: train loss 6.2702, val loss 6.3262, time 1.761 sec/iter\n",
      "step 2200: train loss 6.3374, val loss 6.3119, time 1.720 sec/iter\n",
      "step 2250: train loss 6.2326, val loss 6.2189, time 1.789 sec/iter\n",
      "step 2300: train loss 6.1760, val loss 6.1597, time 1.672 sec/iter\n",
      "step 2350: train loss 6.1638, val loss 6.1342, time 1.680 sec/iter\n",
      "step 2400: train loss 6.1384, val loss 6.1228, time 1.780 sec/iter\n",
      "step 2450: train loss 6.1072, val loss 6.0416, time 1.732 sec/iter\n",
      "step 2500: train loss 6.0693, val loss 6.0584, time 1.790 sec/iter\n",
      "step 2550: train loss 6.0716, val loss 6.0964, time 1.889 sec/iter\n",
      "step 2600: train loss 6.0116, val loss 5.9154, time 1.824 sec/iter\n",
      "step 2650: train loss 5.9961, val loss 5.9825, time 1.850 sec/iter\n",
      "step 2700: train loss 5.9234, val loss 5.9882, time 1.922 sec/iter\n",
      "step 2750: train loss 5.9267, val loss 5.8785, time 1.846 sec/iter\n",
      "step 2800: train loss 5.9052, val loss 5.8421, time 1.798 sec/iter\n",
      "step 2850: train loss 5.8375, val loss 5.7954, time 1.795 sec/iter\n",
      "step 2900: train loss 5.8156, val loss 5.9092, time 1.732 sec/iter\n",
      "step 2950: train loss 5.8391, val loss 5.7723, time 1.753 sec/iter\n",
      "step 3000: train loss 5.8187, val loss 5.7821, time 1.785 sec/iter\n",
      "step 3050: train loss 5.7538, val loss 5.8366, time 1.760 sec/iter\n",
      "step 3100: train loss 5.7042, val loss 5.7020, time 1.787 sec/iter\n",
      "step 3150: train loss 5.6759, val loss 5.6922, time 1.746 sec/iter\n",
      "step 3200: train loss 5.6770, val loss 5.6931, time 1.806 sec/iter\n",
      "step 3250: train loss 5.7112, val loss 5.6729, time 1.869 sec/iter\n",
      "step 3300: train loss 5.6198, val loss 5.6115, time 1.866 sec/iter\n",
      "step 3350: train loss 5.6808, val loss 5.6193, time 1.859 sec/iter\n",
      "step 3400: train loss 5.6221, val loss 5.5968, time 1.900 sec/iter\n",
      "step 3450: train loss 5.5978, val loss 5.6151, time 1.803 sec/iter\n",
      "step 3500: train loss 5.5221, val loss 5.5708, time 1.637 sec/iter\n",
      "step 3550: train loss 5.6292, val loss 5.5432, time 1.666 sec/iter\n",
      "step 3600: train loss 5.5940, val loss 5.5408, time 1.634 sec/iter\n",
      "step 3650: train loss 5.5345, val loss 5.5527, time 1.698 sec/iter\n",
      "step 3700: train loss 5.5051, val loss 5.5327, time 1.677 sec/iter\n",
      "step 3750: train loss 5.4591, val loss 5.5459, time 1.694 sec/iter\n",
      "step 3800: train loss 5.4954, val loss 5.4566, time 1.751 sec/iter\n",
      "step 3850: train loss 5.5404, val loss 5.4643, time 1.737 sec/iter\n",
      "step 3900: train loss 5.4124, val loss 5.3869, time 1.764 sec/iter\n",
      "step 3950: train loss 5.4443, val loss 5.3897, time 1.848 sec/iter\n",
      "step 4000: train loss 5.3850, val loss 5.4436, time 1.801 sec/iter\n",
      "step 4050: train loss 5.4399, val loss 5.3590, time 1.824 sec/iter\n",
      "step 4100: train loss 5.3884, val loss 5.3663, time 1.842 sec/iter\n",
      "step 4150: train loss 5.3685, val loss 5.3955, time 1.695 sec/iter\n",
      "step 4200: train loss 5.3889, val loss 5.2932, time 1.621 sec/iter\n",
      "step 4250: train loss 5.3404, val loss 5.3027, time 1.646 sec/iter\n",
      "step 4300: train loss 5.3605, val loss 5.3745, time 1.651 sec/iter\n",
      "step 4350: train loss 5.3988, val loss 5.3587, time 1.701 sec/iter\n",
      "step 4400: train loss 5.3610, val loss 5.3214, time 1.690 sec/iter\n",
      "step 4450: train loss 5.3152, val loss 5.3565, time 1.716 sec/iter\n",
      "step 4500: train loss 5.2449, val loss 5.2710, time 1.755 sec/iter\n",
      "step 4550: train loss 5.2883, val loss 5.2963, time 1.761 sec/iter\n",
      "step 4600: train loss 5.2153, val loss 5.2810, time 1.776 sec/iter\n",
      "step 4650: train loss 5.3409, val loss 5.1609, time 1.812 sec/iter\n",
      "step 4700: train loss 5.1837, val loss 5.1919, time 1.802 sec/iter\n",
      "step 4750: train loss 5.1684, val loss 5.3086, time 1.868 sec/iter\n",
      "step 4800: train loss 5.1854, val loss 5.2125, time 1.774 sec/iter\n",
      "step 4850: train loss 5.1983, val loss 5.2015, time 1.609 sec/iter\n",
      "step 4900: train loss 5.1896, val loss 5.2056, time 1.665 sec/iter\n",
      "step 4950: train loss 5.2053, val loss 5.1651, time 1.638 sec/iter\n",
      "step 5000: train loss 5.1397, val loss 5.1655, time 1.667 sec/iter\n",
      "step 5050: train loss 5.1728, val loss 5.1865, time 1.724 sec/iter\n",
      "step 5100: train loss 5.1748, val loss 5.1479, time 1.730 sec/iter\n",
      "step 5150: train loss 5.1401, val loss 5.1620, time 1.813 sec/iter\n",
      "step 5200: train loss 5.1947, val loss 5.1427, time 1.819 sec/iter\n",
      "step 5250: train loss 5.0359, val loss 5.0757, time 1.809 sec/iter\n",
      "step 5300: train loss 5.1429, val loss 5.0980, time 1.838 sec/iter\n",
      "step 5350: train loss 5.1388, val loss 5.1307, time 1.866 sec/iter\n",
      "step 5400: train loss 5.0850, val loss 5.0580, time 1.832 sec/iter\n",
      "step 5450: train loss 5.1060, val loss 5.0427, time 1.832 sec/iter\n",
      "step 5500: train loss 5.0847, val loss 5.1430, time 1.702 sec/iter\n",
      "step 5550: train loss 5.0739, val loss 5.0807, time 1.619 sec/iter\n",
      "step 5600: train loss 5.0632, val loss 5.0181, time 1.656 sec/iter\n",
      "step 5650: train loss 5.0553, val loss 5.0748, time 1.637 sec/iter\n",
      "step 5700: train loss 5.1342, val loss 5.0062, time 1.666 sec/iter\n",
      "step 5750: train loss 5.0603, val loss 4.9884, time 1.709 sec/iter\n",
      "step 5800: train loss 4.9775, val loss 5.0555, time 1.692 sec/iter\n",
      "step 5850: train loss 5.0375, val loss 5.0203, time 1.746 sec/iter\n",
      "step 5900: train loss 5.0174, val loss 5.0087, time 1.758 sec/iter\n",
      "step 5950: train loss 4.9145, val loss 5.0147, time 1.765 sec/iter\n",
      "step 6000: train loss 4.9688, val loss 5.0128, time 1.785 sec/iter\n",
      "step 6050: train loss 4.9404, val loss 4.9567, time 1.802 sec/iter\n",
      "step 6100: train loss 4.8839, val loss 4.9378, time 1.835 sec/iter\n",
      "step 6150: train loss 4.9832, val loss 4.9193, time 1.826 sec/iter\n",
      "step 6200: train loss 4.9390, val loss 4.9220, time 1.633 sec/iter\n",
      "step 6250: train loss 4.9118, val loss 4.9103, time 1.638 sec/iter\n",
      "step 6300: train loss 4.9546, val loss 4.8933, time 1.659 sec/iter\n",
      "step 6350: train loss 4.9330, val loss 5.0132, time 1.644 sec/iter\n",
      "step 6400: train loss 4.8986, val loss 4.8169, time 1.682 sec/iter\n",
      "step 6450: train loss 4.9196, val loss 4.9767, time 1.715 sec/iter\n",
      "step 6500: train loss 4.9292, val loss 4.9187, time 1.711 sec/iter\n",
      "step 6550: train loss 4.9591, val loss 4.8585, time 1.748 sec/iter\n",
      "step 6600: train loss 4.8559, val loss 4.8883, time 1.768 sec/iter\n",
      "step 6650: train loss 4.8800, val loss 4.8854, time 1.772 sec/iter\n",
      "step 6700: train loss 4.8125, val loss 4.8493, time 1.834 sec/iter\n",
      "step 6750: train loss 4.9247, val loss 4.8997, time 1.798 sec/iter\n",
      "step 6800: train loss 4.8317, val loss 4.9403, time 1.859 sec/iter\n",
      "step 6850: train loss 4.8326, val loss 4.8444, time 1.752 sec/iter\n",
      "step 6900: train loss 4.9033, val loss 4.8493, time 1.612 sec/iter\n",
      "step 6950: train loss 4.8530, val loss 4.8245, time 1.629 sec/iter\n",
      "step 7000: train loss 4.8328, val loss 4.8320, time 1.664 sec/iter\n",
      "step 7050: train loss 4.8910, val loss 4.7724, time 1.656 sec/iter\n",
      "step 7100: train loss 4.8300, val loss 4.7949, time 1.707 sec/iter\n",
      "step 7150: train loss 4.8818, val loss 4.8226, time 1.791 sec/iter\n",
      "step 7200: train loss 4.7571, val loss 4.8062, time 1.782 sec/iter\n",
      "step 7250: train loss 4.7748, val loss 4.7753, time 1.816 sec/iter\n",
      "step 7300: train loss 4.7857, val loss 4.7884, time 1.810 sec/iter\n",
      "step 7350: train loss 4.7161, val loss 4.7613, time 1.827 sec/iter\n",
      "step 7400: train loss 4.7653, val loss 4.8442, time 1.897 sec/iter\n",
      "step 7450: train loss 4.7240, val loss 4.7777, time 1.812 sec/iter\n",
      "step 7500: train loss 4.6650, val loss 4.7587, time 1.832 sec/iter\n",
      "step 7550: train loss 4.7092, val loss 4.7942, time 1.662 sec/iter\n",
      "step 7600: train loss 4.7147, val loss 4.7283, time 1.644 sec/iter\n",
      "step 7650: train loss 4.7777, val loss 4.6966, time 1.653 sec/iter\n",
      "step 7700: train loss 4.7070, val loss 4.6880, time 1.664 sec/iter\n",
      "step 7750: train loss 4.7477, val loss 4.7279, time 1.674 sec/iter\n",
      "step 7800: train loss 4.7282, val loss 4.8416, time 1.708 sec/iter\n",
      "step 7850: train loss 4.6587, val loss 4.7026, time 1.721 sec/iter\n",
      "step 7900: train loss 4.7260, val loss 4.7882, time 1.716 sec/iter\n",
      "step 7950: train loss 4.6627, val loss 4.6522, time 1.787 sec/iter\n",
      "step 8000: train loss 4.6947, val loss 4.6615, time 1.757 sec/iter\n",
      "step 8050: train loss 4.6774, val loss 4.6575, time 1.772 sec/iter\n",
      "step 8100: train loss 4.6955, val loss 4.5528, time 1.858 sec/iter\n",
      "step 8150: train loss 4.6468, val loss 4.6976, time 1.823 sec/iter\n",
      "step 8200: train loss 4.7357, val loss 4.7401, time 1.801 sec/iter\n",
      "step 8250: train loss 4.6804, val loss 4.6961, time 1.656 sec/iter\n",
      "step 8300: train loss 4.6158, val loss 4.5602, time 1.616 sec/iter\n",
      "step 8350: train loss 4.5802, val loss 4.6207, time 1.657 sec/iter\n",
      "step 8400: train loss 4.6060, val loss 4.5918, time 1.677 sec/iter\n",
      "step 8450: train loss 4.6198, val loss 4.6737, time 1.671 sec/iter\n",
      "step 8500: train loss 4.6090, val loss 4.5626, time 1.723 sec/iter\n",
      "step 8550: train loss 4.6067, val loss 4.5856, time 1.727 sec/iter\n",
      "step 8600: train loss 4.5441, val loss 4.6493, time 1.731 sec/iter\n",
      "step 8650: train loss 4.5808, val loss 4.6098, time 1.810 sec/iter\n",
      "step 8700: train loss 4.5497, val loss 4.4572, time 1.779 sec/iter\n",
      "step 8750: train loss 4.5874, val loss 4.5556, time 1.809 sec/iter\n",
      "step 8800: train loss 4.5931, val loss 4.5633, time 1.861 sec/iter\n",
      "step 8850: train loss 4.4759, val loss 4.5946, time 1.829 sec/iter\n",
      "step 8900: train loss 4.6170, val loss 4.6276, time 1.715 sec/iter\n",
      "step 8950: train loss 4.5495, val loss 4.5149, time 1.626 sec/iter\n",
      "step 9000: train loss 4.5166, val loss 4.5134, time 1.621 sec/iter\n",
      "step 9050: train loss 4.5631, val loss 4.4526, time 1.669 sec/iter\n",
      "step 9100: train loss 4.5415, val loss 4.4937, time 1.698 sec/iter\n",
      "step 9150: train loss 4.5163, val loss 4.5390, time 1.765 sec/iter\n",
      "step 9200: train loss 4.5018, val loss 4.5275, time 1.833 sec/iter\n",
      "step 9250: train loss 4.5926, val loss 4.4310, time 1.777 sec/iter\n",
      "step 9300: train loss 4.5151, val loss 4.4669, time 1.813 sec/iter\n",
      "step 9350: train loss 4.4698, val loss 4.5026, time 1.849 sec/iter\n",
      "step 9400: train loss 4.5637, val loss 4.4198, time 1.823 sec/iter\n",
      "step 9450: train loss 4.4323, val loss 4.4714, time 1.827 sec/iter\n",
      "step 9500: train loss 4.4525, val loss 4.4020, time 1.890 sec/iter\n",
      "step 9550: train loss 4.5046, val loss 4.4153, time 1.753 sec/iter\n",
      "step 9600: train loss 4.5382, val loss 4.4896, time 1.661 sec/iter\n",
      "step 9650: train loss 4.4353, val loss 4.4738, time 1.661 sec/iter\n",
      "step 9700: train loss 4.4879, val loss 4.4636, time 1.644 sec/iter\n",
      "step 9750: train loss 4.4197, val loss 4.5008, time 1.695 sec/iter\n",
      "step 9800: train loss 4.4992, val loss 4.4103, time 1.674 sec/iter\n",
      "step 9850: train loss 4.4799, val loss 4.4387, time 1.704 sec/iter\n",
      "step 9900: train loss 4.4202, val loss 4.4437, time 1.737 sec/iter\n",
      "step 9950: train loss 4.3959, val loss 4.4313, time 1.727 sec/iter\n",
      "step 10000: train loss 4.4969, val loss 4.4820, time 1.789 sec/iter\n",
      "step 10050: train loss 4.3508, val loss 4.3688, time 1.801 sec/iter\n",
      "step 10100: train loss 4.3420, val loss 4.5160, time 1.883 sec/iter\n",
      "step 10150: train loss 4.4215, val loss 4.4439, time 1.941 sec/iter\n",
      "step 10200: train loss 4.4543, val loss 4.3661, time 2.033 sec/iter\n",
      "step 10250: train loss 4.3568, val loss 4.3481, time 1.739 sec/iter\n",
      "step 10300: train loss 4.4102, val loss 4.4387, time 1.629 sec/iter\n",
      "step 10350: train loss 4.3982, val loss 4.4129, time 1.643 sec/iter\n",
      "step 10400: train loss 4.3722, val loss 4.3799, time 1.660 sec/iter\n",
      "step 10450: train loss 4.4384, val loss 4.4231, time 1.709 sec/iter\n",
      "step 10500: train loss 4.3690, val loss 4.3983, time 1.682 sec/iter\n",
      "step 10550: train loss 4.3314, val loss 4.3215, time 1.739 sec/iter\n",
      "step 10600: train loss 4.4468, val loss 4.3590, time 1.778 sec/iter\n",
      "step 10650: train loss 4.4218, val loss 4.3425, time 1.741 sec/iter\n",
      "step 10700: train loss 4.3492, val loss 4.3543, time 1.806 sec/iter\n",
      "step 10750: train loss 4.3689, val loss 4.3028, time 1.824 sec/iter\n"
     ]
    }
   ],
   "source": [
    "print(\"creating model...\")\n",
    "model_cpu = MultiHeadSelfAttention(params['vocab_size'], params['embedding_size'], \n",
    "                                   params['sequence_len'], params['dropout'], \n",
    "                                   params['heads'], params['mhsa_layers'], params['causal'], device)\n",
    "model = model_cpu.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "dt0 = time.time()\n",
    "print(\"training...\")\n",
    "for iter in range(params['max_iterations']):\n",
    "    print(f\"Iteration: {iter+1:5d}\", end=\"\\r\", flush=True)\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if (iter + 1) % params['sample_every_n_iterations'] == 0 or iter == params['max_iterations'] - 1:\n",
    "        dt = time.time()\n",
    "        print(f\"loss eval)[\", end=\"\", flush=True)\n",
    "        losses = estimate_loss(device)\n",
    "        print(\n",
    "            f\"step {iter+1}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time {(dt-dt0)/params['sample_every_n_iterations']:.3f} sec/iter\"\n",
    "        )\n",
    "        if losses['train'] < 3: # Don't waste time with garbage gen\n",
    "            print(\"Sample: \", end=\"\", flush=True)\n",
    "            for temperature in [1.0 ]:\n",
    "                print(f\"--------temperature: {temperature} ---------\")\n",
    "                generate_sample(td, temperature=temperature)\n",
    "            print(\"-------------------------------------------\")\n",
    "        dt0 = time.time()\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_torch_batch(td, params['batch_size'], device, \"train\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    do_train_step(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZozsv2RdK90"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOzxQP23dK90"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VmWbteSFQtfq",
    "yWE_ZZMKEARV"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "torch_transformer_poet.ipynb",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
