{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/torch-transformer-poet/blob/main/torch_transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk"
      },
      "source": [
        "# Torch-Transformer-Poet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DabS0VZ-1Zp0"
      },
      "source": [
        "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtpy59Yq-Qfz",
        "outputId": "cf29265d-e4ce-4c03-a06f-794f188b3384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ml-indie-tools in /usr/local/lib/python3.8/dist-packages (0.4.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U ml-indie-tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EgLLjG4yQtft"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U5T4m6earb1e"
      },
      "outputs": [],
      "source": [
        "from ml_indie_tools.env_tools import MLEnv\n",
        "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
        "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
        "\n",
        "from ml_indie_tools.pytorch_custom_layers import MultiHeadSelfAttention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "## Preliminary\n",
        "\n",
        "A pytorch deep multi-head attention model for text generation following Andrej Karpathy's [video-lecture-ng](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py)\n",
        "\n",
        "This code can use either CPU, GPU, or Apple Silicon. Google Colab is supported too, select the corresponding Colab runtime (menu: **`Runtime / Change runtime type`**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1"
      },
      "source": [
        "## 0. Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "llPw84PkEAP2",
        "outputId": "477151a2-c9d4-4916-a714-6ebfa21745ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OS: Linux, Python: 3.8.10, Colab Jupyter Notebook Pytorch: 1.13.1+cu116, GPU: A100-SXM4-40GB (3MiB / 40536MiB), CPU'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
        "\n",
        "ml_env = MLEnv(platform='pt', accelerator='fastest')\n",
        "ml_env.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-TP3Pnsrb1f",
        "outputId": "8cb516d8-460b-43d7-8a9b-3d4a4f96c33b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root path (all projects) : /content/drive/My Drive (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
            "Project path             : /content/drive/My Drive/Colab Notebooks/women_writers (Changes to the file system happen only below this project path\n",
            "Model path (snapshots)   : /content/drive/My Drive/Colab Notebooks/women_writers/model/ngpt_v1_pt (Model weights and snapshots are stored here)\n",
            "Data path (training data): /content/drive/My Drive/Colab Notebooks/women_writers/data (Training data will be downloaded here)\n",
            "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
          ]
        }
      ],
      "source": [
        "project_name='women_writers'\n",
        "model_name='ngpt_v1_pt'\n",
        "\n",
        "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
        "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools \n",
        "#\n",
        "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
        "\n",
        "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
        "\n",
        "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
        "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
        "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
        "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
        "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library\n",
        "\n",
        "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
        "encoding, batch generation, and formatted source display. It read some \n",
        "books from Project Gutenberg and supports creation of training batches. \n",
        "The output functions support highlighting to allow to compare generated \n",
        "texts with the actual sources to help to identify identical (memorized) \n",
        "parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HjkelBcNO5WV"
      },
      "outputs": [],
      "source": [
        "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BF8eyWnCrb1h"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
        "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C66X7ynnrb1h",
        "outputId": "dc0b9bd7-6d3e-4eb7-884b-9bc6842d1693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21 matching books found with search {'author': ['Emily Brontë', 'Jane Austen', 'Virginia Woolf'], 'language': ['english']}.\n"
          ]
        }
      ],
      "source": [
        "# sample searches\n",
        "search_spec= {\"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"], \"language\": [\"english\"]}\n",
        "\n",
        "book_list=gd.search(search_spec)\n",
        "book_cnt = len(book_list)\n",
        "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
        "if book_cnt<40:\n",
        "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
        "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
        "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
        "else:\n",
        "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH6_7IU3upOd",
        "outputId": "7aa638fa-c13e-4dd0-ea57-ced30c4f9ea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: The Common Reader - Virginia Woolf, 64457\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf, 63022\n",
            "2: The Younger Sister, Volumes 1-3 - Catherine Anne Austen Hubback and Jane Austen, 54066\n",
            "3: The Younger Sister, Vol. 3 - Catherine Anne Austen Hubback and Jane Austen, 54012\n",
            "4: The Younger Sister, Vol. 2 - Catherine Anne Austen Hubback and Jane Austen, 54011\n",
            "5: The Younger Sister, Vol. 1 - Catherine Anne Austen Hubback and Jane Austen, 54010\n",
            "6: Pride and Prejudice - Jane Austen, 42671\n",
            "7: The Letters of Jane Austen - Jane Austen, 42078\n",
            "8: The Complete Project Gutenberg Works of Jane Austen - Jane Austen, 31100\n",
            "9: Jacob's Room - Virginia Woolf, 5670\n",
            "10: Pride and Prejudice - Jane Austen, 1342\n",
            "11: Night and Day - Virginia Woolf, 1245\n",
            "12: Love And Friendship And Other Early Works - Jane Austen, 1212\n",
            "13: Lady Susan - Jane Austen, 946\n",
            "14: Wuthering Heights - Emily Brontë, 768\n",
            "15: Sense and Sensibility - Jane Austen, 161\n",
            "16: Emma - Jane Austen, 158\n",
            "17: The Voyage Out - Virginia Woolf, 144\n",
            "18: Mansfield Park - Jane Austen, 141\n",
            "19: Northanger Abbey - Jane Austen, 121\n",
            "20: Persuasion - Jane Austen, 105\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(book_list)):\n",
        "    print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jBH3Z15rb1h",
        "outputId": "b63999fa-f4e8-4250-cba0-47422094fcfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using:\n",
            "1: Mr. Bennett and Mrs. Brown - Virginia Woolf\n",
            "2: Jacob's Room - Virginia Woolf\n",
            "3: Pride and Prejudice - Jane Austen\n",
            "4: Night and Day - Virginia Woolf\n",
            "5: Lady Susan - Jane Austen\n",
            "6: Wuthering Heights - Emily Brontë\n",
            "7: Sense and Sensibility - Jane Austen\n",
            "8: Emma - Jane Austen\n",
            "9: The Voyage Out - Virginia Woolf\n",
            "10: Mansfield Park - Jane Austen\n",
            "11: Northanger Abbey - Jane Austen\n",
            "12: Persuasion - Jane Austen\n"
          ]
        }
      ],
      "source": [
        "MAX_TOKENS = 20000  # This becomes vocab_size\n",
        "MAX_NGRAM_LEN = 10   # Max length of a token\n",
        "\n",
        "select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
        "sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
        "\n",
        "print(\"Using:\")\n",
        "for i in range(len(sub_book_list)):\n",
        "    print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
        "\n",
        "textlib_dataset = None  # Forces re-caching\n",
        "td = Text_Dataset(sub_book_list)\n",
        "td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f7_tc2Lirb1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ed375b1-0f73-459e-d45d-256dbb7da0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1528046 records\n"
          ]
        }
      ],
      "source": [
        "SEQUENCE_LEN = 64\n",
        "# SUB_PROBABILITY = 0.15  # like BERT\n",
        "\n",
        "td.init_getitem(sample_type='encoded', sample_length=SEQUENCE_LEN+1, content_stepping=1)\n",
        "\n",
        "num_records = len(td)\n",
        "\n",
        "print(f\"{num_records} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zZbbsNm0cOeW"
      },
      "outputs": [],
      "source": [
        "def get_sample_batch(td, batch_size):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    # ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    # x = torch.stack([data[i : i + block_size] for i in ix])\n",
        "    # y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
        "    # x, y = x.to(device), y.to(device)\n",
        "    # return x, y\n",
        "    for i in range(batch_size):\n",
        "        data = td.get_random_item()\n",
        "        Xi = data[:-1]\n",
        "        yi = data[1:]\n",
        "        if i==0:\n",
        "            # smpX=np.array(Xi, dtype=np.float32)\n",
        "            smpX=np.array(Xi, dtype=np.int32)\n",
        "            smpy=np.array(yi, dtype=np.int32)\n",
        "        else:\n",
        "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
        "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
        "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
        "    return np.array(smpX), np.array(smpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TI3Fx6bNuR9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb032a6-792d-4451-9f36-38b5f7b96b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0](l=64): X=>’s, and he hastily exclaimed,\n",
            "“Ha! this must be the very shop that every body attends every day of\n",
            "their lives, as my father informs me. He comes to Highbury himself, he\n",
            "says, six days out of the seven, and has always business at Ford’s. If\n",
            "it be not inconvenient to <,\n",
            "y=>, and he hastily exclaimed,\n",
            "“Ha! this must be the very shop that every body attends every day of\n",
            "their lives, as my father informs me. He comes to Highbury himself, he\n",
            "says, six days out of the seven, and has always business at Ford’s. If\n",
            "it be not inconvenient to you, <\n",
            "[1](l=64): X=>, that\n",
            "he was sorry for the disappointment of the others, and with\n",
            "considerable kindness added,\n",
            "\n",
            "“You, Emma, who have so few opportunities of dancing, you are really\n",
            "out of luck; you are very much out of luck!”\n",
            "\n",
            "It was some days before she saw Jane Fairfax, to judge of her honest\n",
            "regret in this woeful change; but when they did meet<,\n",
            "y=>\n",
            "he was sorry for the disappointment of the others, and with\n",
            "considerable kindness added,\n",
            "\n",
            "“You, Emma, who have so few opportunities of dancing, you are really\n",
            "out of luck; you are very much out of luck!”\n",
            "\n",
            "It was some days before she saw Jane Fairfax, to judge of her honest\n",
            "regret in this woeful change; but when they did meet, her <\n"
          ]
        }
      ],
      "source": [
        "test_x, test_y = get_sample_batch(td, 2)\n",
        "for i in range(len(test_x)):\n",
        "    xi=[int(x) for x in test_x[i]]\n",
        "    print(f\"[{i}](l={len(xi)}): X=>{td.decode(xi)}<,\\ny=>{td.decode(test_y[i])}<\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qnMxRkkmcOeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30aa9c36-ecb2-4749-9dda-4f3db5a8db17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 64), (2, 64))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "test_x.shape, test_y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hi0UPtEAQG"
      },
      "source": [
        "## 2. data for texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jn_LcJ6g9Mzy"
      },
      "outputs": [],
      "source": [
        "def expand_name_template(template, params):\n",
        "    exp=copy.copy(template)\n",
        "    for key in params:\n",
        "        src=\"{\"+key+\"}\"\n",
        "        dst=f\"{params[key]}\"\n",
        "        exp=exp.replace(src,dst).replace('[','(').replace(']',')')\n",
        "    return exp\n",
        "\n",
        "def save_model_metadata(epoch, suffix='std'):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    params['current_epoch'] = epoch\n",
        "    try:\n",
        "        with open(meta_file, 'w') as f:\n",
        "            f.write(json.dumps(params))\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to store model metadata at {model_path}: {e}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def read_model_metadata(suffix=\"std\"):\n",
        "    meta_file = os.path.join(model_path, f'model_meta_{suffix}.json')\n",
        "    try:\n",
        "        with open(meta_file, 'r') as f:\n",
        "            meta = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Cannot access project meta-data at {meta_file}: {e}, starting anew.\")\n",
        "        return None\n",
        "    return meta\n",
        "\n",
        "def is_metadata_compatible(params, meta):\n",
        "    is_valid=True\n",
        "    keys=set(list(params.keys())+list(meta.keys()))\n",
        "    for key in keys:\n",
        "        if key in updatable_keys:\n",
        "            continue\n",
        "        if key not in meta:\n",
        "            print(f\"Key {key} not available in last checkpoint model_meta, params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif key not in params:\n",
        "            print(f\"Key {key} not available in params, last checkpoint model_meta[{key}]: {meta[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "        elif meta[key]!=params[key]:\n",
        "            print(f\"Last checkpoint model_meta[{key}]: {meta[key]} != params[{key}]: {params[key]}, cannot import incompatible model. Put key in `updatable_keys` list, if irrelevant.\")\n",
        "            is_valid = False\n",
        "    if is_valid is False:\n",
        "        print(\"Aborting import.\")\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "znpIUA3ig3gO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca139d45-4566-446e-cb28-48b19843ac2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot access project meta-data at /content/drive/My Drive/Colab Notebooks/women_writers/model/ngpt_v1_pt/model_meta_16x16x{units}x20000.json: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/women_writers/model/ngpt_v1_pt/model_meta_16x16x{units}x20000.json', starting anew.\n",
            "Starting new model\n",
            "{'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}', 'mhsa_layers': 16, 'heads': 16, 'causal': True, 'dropout': 0.1, 'vocab_size': 20000, 'sequence_len': 64, 'embedding_size': 512, 'test_iterations': 20, 'batch_size': 256, 'learning_rate': 0.0004, 'sample_every_n_iterations': 200, 'max_iterations': 1000000}\n"
          ]
        }
      ],
      "source": [
        "vocabulary_size = td.get_unique_token_count()  # vocabulary-size\n",
        "\n",
        "lyrs = 16;\n",
        "\n",
        "params = { # Multi-head self-attention\n",
        "    'name': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
        "\n",
        "    'mhsa_layers': lyrs, \n",
        "    'heads': 16, # [16]*lyrs,\n",
        "    # 'units': [512]*lyrs,\n",
        "    # 'norm': 'softmax', # this is for within each head (all-you-need: softmax)\n",
        "    # 'mh_normalize': True,  # use layer-norm after concatenation (or additiona) of the heads\n",
        "    'causal': True,  # Use causal self-attention\n",
        "    # 'l2_regularizer': 1e-9,\n",
        "    'dropout': 0.1,       # no dropout: 0.0\n",
        "    # 'join_heads_by_add': True,  # stragegy how multi-heads are joined: False: concat (as in all-you-need), True: relu+add of all the heads: less resources, similar perf.\n",
        "    'vocab_size': vocabulary_size,\n",
        "    'sequence_len': SEQUENCE_LEN,\n",
        "    'embedding_size': 512, \n",
        "    'test_iterations': 20,  # number of epocs for loss estimation\n",
        "\n",
        "    'batch_size': 256,\n",
        "    'learning_rate': 0.0004,\n",
        "    # 'clipvalue': None,\n",
        "    'sample_every_n_iterations': 500,\n",
        "    \n",
        "    'max_iterations': 1000000  # maximum number of training iterations\n",
        "}\n",
        "\n",
        "# if len(params['heads'])!=params['mhsa_layers']: # or len(params['units'])!=params['mhsa_layers']:\n",
        "#     print(\"ERROR: lenght of 'heads' and 'units' must be equal to mhsa_layers!\")\n",
        "    \n",
        "model_suffix = expand_name_template(params['name'], params)\n",
        "# Put 'important' params in checkpoint-pathname to separate model-data:\n",
        "checkpoint_dir = os.path.join(model_path, f\"training_checkpoints_{model_suffix}\")\n",
        "if os.path.exists(checkpoint_dir) is False:\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "# When comparing if training-data is compatible with new params set, \n",
        "# the following keys are updatable, they can be changed while continuing\n",
        "# to use existing checkpoints and continue training with those values\n",
        "# changed:\n",
        "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'dropout', \n",
        "             'sample_every_n_epochs']\n",
        "\n",
        "# These values are taking from saved checkpoint:\n",
        "keep_keys=['current_epoch']\n",
        "\n",
        "continue_last = True\n",
        "if continue_last is False:\n",
        "    print(\"NOT continuing based on existing training! New start.\")\n",
        "\n",
        "meta = read_model_metadata(suffix=model_suffix)\n",
        "if meta is not None and is_metadata_compatible(params, meta) is True and continue_last is True:\n",
        "    for key in keep_keys:\n",
        "        if key in meta:\n",
        "            params[key]=meta[key]\n",
        "    if params is not None:\n",
        "        print(f\"Continuing last session from epoch {params['current_epoch']}\")\n",
        "    else:\n",
        "        print(f\"No previous data, starting new model\")\n",
        "else:\n",
        "    print(\"Starting new model\")\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "jY3hUuhQYzdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd3d628d-e4eb-4320-a118-4341e0b5ed56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches = 5968\n"
          ]
        }
      ],
      "source": [
        "num_batches = num_records // params['batch_size']\n",
        "print(f\"num_batches = {num_batches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "bgVHUkbhdK9y"
      },
      "outputs": [],
      "source": [
        "def get_torch_batch(td, batch_size, device, split=None):\n",
        "    x, y = get_sample_batch(td, batch_size)\n",
        "    return torch.tensor(x, dtype=torch.long).to(device), torch.tensor(y, dtype=torch.long).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTte4VvUdK9z",
        "outputId": "1d2c8c71-baa6-43be-f4ac-dba7caae5b1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[17368,  5058,    13, 11330,  5476,  3529,   665,  1107,  2889,   659,\n",
              "           1178,  6058,  4546,  2932,   986,  9579, 13797,  1114,   280,   186,\n",
              "            737, 10135,    16,    22,  8591,  8881,  2143,  7722, 13606,   383,\n",
              "           6833, 12197, 17085,  7212,  2518,  6111, 17426, 13455,  7138, 15507,\n",
              "          11330,  5476,  3529, 17591,  1107,  2883, 18532, 11987,   351, 14101,\n",
              "          19252,   202,  2301, 12441,  4223,  7071,  7023, 10751,  8161,  2617,\n",
              "           1719,  7245,  4097, 15901],\n",
              "         [    8,     8, 18514,  2854, 13679,  9932,     8,     8,  1686, 15964,\n",
              "           7057,  1117,  8082,  1358, 16606,    62,  3310, 19534,  8509,   791,\n",
              "             14, 12696,   698,  6585, 12393, 15058,   214,    62,    26,  2828,\n",
              "          14173,  2101, 16606,    62,    18,  5773,   860, 19484,  6532,  8576,\n",
              "            230,  9708,  6611, 12393,    17, 17021, 13396, 16368, 11207, 15058,\n",
              "            214,    62, 12511,  4529, 10919,  1532,  3503, 18594,   592,    61,\n",
              "           1434, 15663,    62,  9928]]),\n",
              " tensor([[ 5058,    13, 11330,  5476,  3529,   665,  1107,  2889,   659,  1178,\n",
              "           6058,  4546,  2932,   986,  9579, 13797,  1114,   280,   186,   737,\n",
              "          10135,    16,    22,  8591,  8881,  2143,  7722, 13606,   383,  6833,\n",
              "          12197, 17085,  7212,  2518,  6111, 17426, 13455,  7138, 15507, 11330,\n",
              "           5476,  3529, 17591,  1107,  2883, 18532, 11987,   351, 14101, 19252,\n",
              "            202,  2301, 12441,  4223,  7071,  7023, 10751,  8161,  2617,  1719,\n",
              "           7245,  4097, 15901,  6698],\n",
              "         [    8, 18514,  2854, 13679,  9932,     8,     8,  1686, 15964,  7057,\n",
              "           1117,  8082,  1358, 16606,    62,  3310, 19534,  8509,   791,    14,\n",
              "          12696,   698,  6585, 12393, 15058,   214,    62,    26,  2828, 14173,\n",
              "           2101, 16606,    62,    18,  5773,   860, 19484,  6532,  8576,   230,\n",
              "           9708,  6611, 12393,    17, 17021, 13396, 16368, 11207, 15058,   214,\n",
              "             62, 12511,  4529, 10919,  1532,  3503, 18594,   592,    61,  1434,\n",
              "          15663,    62,  9928, 13304]]))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "get_torch_batch(td, 2, 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "QnMCWf5AZn1-"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(device):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(params['test_iterations'])\n",
        "        for k in range(params['test_iterations']):\n",
        "            print(\".\", end=\"\", flush=True)\n",
        "            X, Y = get_torch_batch(td, params['batch_size'], device, split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    print(\"]\\r\", end=\"\", flush=True)\n",
        "    return out\n",
        "\n",
        "\n",
        "def generate_sample(td, toks=100, temperature=1.0):\n",
        "    # generate from the model\n",
        "    # context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    context = torch.tensor([td.encode(' ')]).to(device)\n",
        "    print(td.decode(model.generate(context, max_new_tokens=toks, temperature=temperature)[0].tolist()))\n",
        "    # open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n",
        "\n",
        "\n",
        "# @torch.compile\n",
        "def do_train_step(xb, yb):\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sample(td, toks=20, temperature=0.8)"
      ],
      "metadata": {
        "id": "CENbrUAonTqn",
        "outputId": "3adfbbdc-278c-4932-814e-de8dffd698d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \"Yes, indeed, that letter.  Now, the demoved on\"\n",
            "\n",
            "\"Now from all those horses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "9hEziJ0odK9z"
      },
      "outputs": [],
      "source": [
        "# XXX!\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pdaulm1VdK9z",
        "outputId": "0353a4ad-01cb-4267-9d33-c411a84ffd6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating model...\n",
            "70.94736 M parameters\n",
            "training...\n",
            "step 200: train loss 7.6962, val loss 7.6902, time 0.671 sec/iter\n",
            "Sample: --------temperature: 0.9 ---------\n",
            " X Co\n",
            "and no strongal find given firositage daughter-moring to s; and whversat\n",
            "hall sho. I the countreemed saw XXand\n",
            "      just mayed’s\n",
            "being dance\n",
            "f.  most accountyesterleaseds bthe conn,, sing inupfulaway.\n",
            "\n",
            "Westonan anl cusion.”\n",
            "\n",
            "“Nourprisetting \n",
            "of a great nothing it sty, set ay. Smitbeen she be\n",
            "of, at  drrp,\n",
            "a Sut, and else; but\n",
            "wish for Mas; and Allen, to him thought of, and soose mentionain, or\n",
            "openw ioubtthan h. I on the sus go\n",
            "--------temperature: 1.0 ---------\n",
            " (jenowled ca_  the next  canuntil before mes hope ried as wellbrother than thepeaking ish the infer\n",
            "whlld.es\n",
            "assayoung lasidera’” said M’llir outg,will think thty, but\n",
            "sonkeekther hke tilis es. The;\n",
            "urrants, and it, reseluous said t thingsterdayre aof the unc her, and observedying it.”\n",
            "\n",
            "“Hilbery looking atinto the remion of theere bit was tld\n",
            "f\n",
            "TuesThey w and just rel\n",
            "havety, to cwnearly  buspleasure Mrs onc fuloor  person olis tafraid, Elinor better for m\n",
            "o\n",
            "--------temperature: 1.1 ---------\n",
            " Yes, say. Wege might be friendets of as highs abface have timely with all  Her asterpriseconnecconviight arouely been I had, almeetdeed,” she repliedwn, little or three of this e atake be increasy. Wicky rbeen not eack, and close. Wecause oced in placely, in the sbserveonce I should for aminuch lalways  in\n",
            "the she mly e to cup t short will be as the pre of reto coming for onling hid heomioveodink to the Elliote, as convinced s, anddistresust rest ander Mt lastominging at red \n",
            "-------------------------------------------\n",
            "Iteration:   211/200/1000000"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-95bb36c8d265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# evaluate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdo_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-3c958b98adfd>\u001b[0m in \u001b[0;36mdo_train_step\u001b[0;34m(xb, yb)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"creating model...\")\n",
        "model_cpu = MultiHeadSelfAttention(params['vocab_size'], params['embedding_size'], \n",
        "                                   params['sequence_len'], params['dropout'], \n",
        "                                   params['heads'], params['mhsa_layers'], params['causal'], device)\n",
        "model = model_cpu.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
        "\n",
        "dt0 = time.time()\n",
        "print(\"training...\")\n",
        "for iter in range(params['max_iterations']):\n",
        "    print(f\"\\rIteration: {iter+1:5d}/{params['sample_every_n_iterations']}/{params['max_iterations']}\", end=\"\", flush=True)\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if (iter + 1) % params['sample_every_n_iterations'] == 0 or iter == params['max_iterations'] - 1:\n",
        "        dt = time.time()\n",
        "        print(f\"\\rloss eval)[\", end=\"\", flush=True)\n",
        "        losses = estimate_loss(device)\n",
        "        print(\n",
        "            f\"step {iter+1}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time {(dt-dt0)/params['sample_every_n_iterations']:.3f} sec/iter\"\n",
        "        )\n",
        "        print(\"Sample: \", end=\"\", flush=True)\n",
        "        for temperature in [0.9, 1.0, 1.1]:\n",
        "            print(f\"--------temperature: {temperature} ---------\")\n",
        "            generate_sample(td, temperature=temperature)\n",
        "        print(\"-------------------------------------------\")\n",
        "        dt0 = time.time()\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_torch_batch(td, params['batch_size'], device, \"train\")\n",
        "\n",
        "    # evaluate the loss\n",
        "    do_train_step(xb, yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZozsv2RdK90"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOzxQP23dK90"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
      "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
    },
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VmWbteSFQtfq",
        "yWE_ZZMKEARV"
      ],
      "machine_shape": "hm",
      "name": "torch_transformer_poet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}