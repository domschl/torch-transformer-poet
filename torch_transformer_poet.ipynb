{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/torch-transformer-poet/blob/main/torch_transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk"
      },
      "source": [
        "# Torch-Transformer-Poet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DabS0VZ-1Zp0"
      },
      "source": [
        "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF-7qFzMdnN1",
        "outputId": "b483238b-f52f-4aec-86ac-de80e2612d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ml-indie-tools\n",
            "  Downloading ml_indie_tools-0.9.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.9/52.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-indie-tools\n",
            "Successfully installed ml-indie-tools-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U ml-indie-tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtpy59Yq-Qfz",
        "outputId": "f26a776d-9e61-4edc-ab89-03658d0ff0c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    # from: https://github.com/pytorch/pytorch/issues/107960  (libcuda not found)\n",
        "    !export LC_ALL=\"en_US.UTF-8\"\n",
        "    !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "    !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "    !ldconfig /usr/lib64-nvidia\n",
        "#     print(\"While default colab is still stuck with pytorch 1.13, we update to 2.0 using PIP. This can be removed, once Colab arrives in the presence.\")\n",
        "#     !pip install -U torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EgLLjG4yQtft"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import copy\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U5T4m6earb1e"
      },
      "outputs": [],
      "source": [
        "from ml_indie_tools.env_tools import MLEnv\n",
        "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
        "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
        "\n",
        "from ml_indie_tools.Calibre_Dataset import Calibre_Dataset\n",
        "from ml_indie_tools.Folder_Dataset import Folder_Dataset\n",
        "\n",
        "from ml_indie_tools.pytorch_custom_layers import MultiHeadSelfAttention\n",
        "from ml_indie_tools.pytorch_tr_compr_layers import MultiHeadSelfAttentionWithCompression, MultiHeadSelfAttentionWithCompressionState\n",
        "import ml_indie_tools.pytorch_meta_tools as MJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jVcwvURB5EZN"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "log = logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "## Preliminary\n",
        "\n",
        "A pytorch deep multi-head attention model for text generation following Andrej Karpathy's [video-lecture-ng](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py)\n",
        "\n",
        "This code can use either CPU, GPU, or Apple Silicon. Google Colab is supported too, select the corresponding Colab runtime (menu: **`Runtime / Change runtime type`**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1"
      },
      "source": [
        "## 0. Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "llPw84PkEAP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "106a92af-797a-4474-d6bc-0c7c10505fcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OS: Linux, Python: 3.10.12, Colab Jupyter Notebook Pytorch: 2.1.0+cu118, GPU: Tesla V100-SXM2-16GB (2MiB / 16384MiB), CPU'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
        "\n",
        "ml_env = MLEnv(platform='pt', accelerator='fastest')\n",
        "ml_env.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qg3ZPBmC8kO"
      },
      "source": [
        "## 1. Project configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "t-TP3Pnsrb1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ea0ee9-143c-48df-cc3a-180c5527f8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root path (all projects) : /content/drive/My Drive (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
            "Project path             : /content/drive/My Drive/Colab Notebooks/neo_philosophers (Changes to the file system happen only below this project path\n",
            "Model path (snapshots)   : /content/drive/My Drive/Colab Notebooks/neo_philosophers/model/ngpt_COMP_neo_philosophers_v2_pt (Model weights and snapshots are stored here)\n",
            "Data path (training data): /content/drive/My Drive/Colab Notebooks/neo_philosophers/data (Training data will be downloaded here)\n",
            "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
          ]
        }
      ],
      "source": [
        "# project_name = 'women_writers'\n",
        "model_cpu = None\n",
        "project_name='neo_philosophers'\n",
        "model_name=f'ngpt_COMP_{project_name}_v2_pt'\n",
        "\n",
        "use_preprocessed_data = False\n",
        "use_existing_model_from_checkpoint = True\n",
        "\n",
        "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
        "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools\n",
        "#\n",
        "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
        "\n",
        "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device\n",
        "\n",
        "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
        "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
        "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
        "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
        "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  2.1 Text data from Project Gutenberg\n",
        "\n",
        "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training,\n",
        "encoding, batch generation, and formatted source display. It read some\n",
        "books from Project Gutenberg and supports creation of training batches.\n",
        "The output functions support highlighting to allow to compare generated\n",
        "texts with the actual sources to help to identify identical (memorized)\n",
        "parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HjkelBcNO5WV"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BF8eyWnCrb1h"
      },
      "outputs": [],
      "source": [
        "token_file = os.path.join(data_path,f\"{project_name}_tokens.json\")\n",
        "if use_preprocessed_data is True:\n",
        "    if os.path.exists(token_file):\n",
        "        td = Text_Dataset()\n",
        "        td.load_tokenizer(token_file)\n",
        "    else:\n",
        "        use_preprocessed_data = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "C66X7ynnrb1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba2b7d5a-1027-4c5c-a558-807a27adce20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 matching books found with search {'author': ['Plato'], 'title': ['Timaeus', 'Critias', 'Symposium'], 'language': ['english']}.\n",
            "0: The History of Philosophy: Volume 3 of 3 - Georg Wilhelm Hegel, 58169\n",
            "1: The Will to Power, Books III and IV - Friedrich Nietzsche, 52915\n",
            "2: The Will to Power, Books I and II - Friedrich Nietzsche, 52914\n",
            "3: The Joyful Wisdom - Friedrich Nietzsche, 52881\n",
            "4: Kant's Prolegomena - Immanuel Kant, 52821\n",
            "5: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3 - Georg Wilhelm Hegel, 51636\n",
            "6: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3 - Georg Wilhelm Hegel, 51635\n",
            "7: Early Greek Philosophy & Other Essays - Friedrich Nietzsche, 51548\n",
            "8: Perpetual Peace - Immanuel Kant, 50922\n",
            "9: Kant's Critique of Judgement - Immanuel Kant, 48433\n",
            "10: Thoughts Out of Season, Part 2 - Friedrich Nietzsche, 38226\n",
            "11: Human, All Too Human - Friedrich Nietzsche, 38145\n",
            "12: We Philologists, Volume 8 of 18 - Friedrich Nietzsche, 18267\n",
            "13: The Metaphysical Elements of Ethics - Immanuel Kant, 5684\n",
            "14: The Critique of Practical Reason - Immanuel Kant, 5683\n",
            "15: Fundamental Principles of the Metaphysic of Morals - Immanuel Kant, 5682\n",
            "16: Thoughts out of Season, Part One - Friedrich Nietzsche, 5652\n",
            "17: Beyond Good and Evil - Friedrich Nietzsche, 4363\n",
            "18: The Critique of Pure Reason - Immanuel Kant, 4280\n",
            "19: Thus Spake Zarathustra - Friedrich Nietzsche, 1998\n",
            "Using:\n",
            "1: The History of Philosophy: Volume 3 of 3 - Georg Wilhelm Hegel\n",
            "2: The Will to Power, Books III and IV - Friedrich Nietzsche\n",
            "3: The Will to Power, Books I and II - Friedrich Nietzsche\n",
            "4: The Joyful Wisdom - Friedrich Nietzsche\n",
            "5: Kant's Prolegomena - Immanuel Kant\n",
            "6: Hegel's Lectures on the History of Philosophy: Vol. 2 of 3 - Georg Wilhelm Hegel\n",
            "7: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3 - Georg Wilhelm Hegel\n",
            "8: Early Greek Philosophy & Other Essays - Friedrich Nietzsche\n",
            "9: Perpetual Peace - Immanuel Kant\n",
            "10: Kant's Critique of Judgement - Immanuel Kant\n",
            "11: Thoughts Out of Season, Part 2 - Friedrich Nietzsche\n",
            "12: Human, All Too Human - Friedrich Nietzsche\n",
            "13: We Philologists, Volume 8 of 18 - Friedrich Nietzsche\n",
            "14: The Metaphysical Elements of Ethics - Immanuel Kant\n",
            "15: The Critique of Practical Reason - Immanuel Kant\n",
            "16: Fundamental Principles of the Metaphysic of Morals - Immanuel Kant\n",
            "17: Thoughts out of Season, Part One - Friedrich Nietzsche\n",
            "18: Beyond Good and Evil - Friedrich Nietzsche\n",
            "19: The Critique of Pure Reason - Immanuel Kant\n",
            "20: Thus Spake Zarathustra - Friedrich Nietzsche\n"
          ]
        }
      ],
      "source": [
        "if use_preprocessed_data is False:\n",
        "    cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
        "    gd = Gutenberg_Dataset(cache_dir=cache_dir)\n",
        "\n",
        "    if project_name == 'women_writers':  # sample searches\n",
        "        search_spec= {\n",
        "            \"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"],\n",
        "            \"language\": [\"english\"]\n",
        "        }\n",
        "        book_list=gd.search(search_spec)\n",
        "    elif project_name == 'neo_philosophers':\n",
        "        search_spec = {\n",
        "            \"author\": [\"Immanuel Kant\", \"Friedrich Nietzsche\", \"Wilhelm Hegel\"],\n",
        "            \"language\": [\"english\"]\n",
        "        }\n",
        "        book_list=gd.search(search_spec)\n",
        "        search_spec = {\n",
        "            \"author\": [\"Plato\"],\n",
        "            \"title\": [\"Timaeus\", \"Critias\", \"Symposium\"],\n",
        "            \"language\": [\"english\"]\n",
        "        }\n",
        "        book_list+=gd.search(search_spec)\n",
        "\n",
        "    book_cnt = len(book_list)\n",
        "    print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
        "    if book_cnt<40:\n",
        "        # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts,\n",
        "        # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
        "        book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)\n",
        "    else:\n",
        "        logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")\n",
        "\n",
        "    for i in range(len(book_list)):\n",
        "        print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")\n",
        "\n",
        "    if project_name == 'women_writers':\n",
        "        select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
        "        sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
        "    else:\n",
        "        sub_book_list = book_list\n",
        "\n",
        "    print(\"Using:\")\n",
        "    for i in range(len(sub_book_list)):\n",
        "        print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
        "\n",
        "    td = Text_Dataset(sub_book_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxNIc7gL9UNg"
      },
      "source": [
        "## 2.2 Additional training material for folder `{data_path}/local_texts`\n",
        "\n",
        "If the folder {data_path} (defined above) contains a sub-folder `local_texts`, and it contains\n",
        "files of structure `<title> - <author> - <language>.txt`, then they are added to the training data.\n",
        "Sample filename: `\"./data/local_texts/works-of-shakespeare - William Shakespeare - English.txt\"`.\n",
        "The titles of those documents are referenced via numeric aliases to preserve privacy on non-public data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1NYdjlW65EZP"
      },
      "outputs": [],
      "source": [
        "if use_preprocessed_data is False:\n",
        "    use_local_folder_data = False\n",
        "    if use_local_folder_data:\n",
        "        local_texts = os.path.join(data_path, 'local_texts')\n",
        "        if os.path.isdir(local_texts) is False:\n",
        "            print(f\"You have no local texts at {local_texts}\")\n",
        "        else:\n",
        "            fd = Folder_Dataset()\n",
        "            fd.load_index(local_texts, use_aliases=False)\n",
        "            td.load_texts(fd.records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSm4f9NSC8kQ"
      },
      "source": [
        "## 2.3 Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsyBjqFyC8kQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee564500-ca52-4d17-f825-01a1c183505c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting NGRAM tokinizer with token length from 1..6 with a max of 10000 unique tokens,\n",
            "this can take considerable time...\n"
          ]
        }
      ],
      "source": [
        "if use_preprocessed_data is False:\n",
        "    MAX_TOKENS = 10000  # This becomes vocab_size\n",
        "    MAX_NGRAM_LEN = 6   # Max length of a token\n",
        "\n",
        "    print(\"\")\n",
        "    print(f\"Starting NGRAM tokinizer with token length from 1..{MAX_NGRAM_LEN} with a max of {MAX_TOKENS} unique tokens,\")\n",
        "    print(\"this can take considerable time...\")\n",
        "\n",
        "    td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS)\n",
        "    td.save_tokenizer(token_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG03WA_yC8kR"
      },
      "source": [
        "## 3. Model metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UPMwIn2gC8kR"
      },
      "outputs": [],
      "source": [
        "params = None\n",
        "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'current_loss', 'stateful',\n",
        "                 'sample_every_n_iterations', 'sample_size', 'save_every_n_iterations']\n",
        "attn_layers = 4\n",
        "embs = 128\n",
        "linear_yoke_hidden_index = -1  # Set to -1, if no yoke is wanted (standard transformer model)\n",
        "linear_yoke_size = 96\n",
        "\n",
        "params = { # Multi-head self-attention\n",
        "        'meta_name_template': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
        "\n",
        "        'mhsa_layers': attn_layers,\n",
        "        'heads': 8,\n",
        "        'causal': True,  # Use causal self-attention\n",
        "        'linear_non_linearity': 'relu',  # relurelu: use additional relu for state gating\n",
        "        'linear_yoke_hidden_index': linear_yoke_hidden_index,  # no residual for non-default hidden_size only\n",
        "        'linear_yoke_size': linear_yoke_size,\n",
        "        'linear_yoke_residual': True,\n",
        "        'stateful': False,\n",
        "        'joint_state_training': 4,  # use consecutive training samples with shared state for 32 chars\n",
        "        'dropout': 0.1,\n",
        "        'vocab_size': td.get_unique_token_count(),\n",
        "        'sequence_len': embs,\n",
        "        'embedding_size': embs,\n",
        "        'test_iterations': 10,  # number of epocs for loss estimation\n",
        "\n",
        "        'batch_size': 128,\n",
        "        'learning_rate': 0.0002,\n",
        "        'sample_every_n_iterations': 256,\n",
        "        'sample_size': 100,\n",
        "        'save_every_n_iterations': 256,\n",
        "\n",
        "        'max_iterations': 1000000  # maximum number of training iterations\n",
        "    }\n",
        "if params['stateful'] is False:\n",
        "    params['joint_state_training'] = 0\n",
        "if use_existing_model_from_checkpoint is True:\n",
        "    model_file_path = MJ.get_model_filename(model_path)\n",
        "    params = MJ.load_model_metadata_from_checkpoint(params, updatable_keys, model_file_path, device=device, log=log) # torch.device('cpu'))\n",
        "if params == None or use_existing_model_from_checkpoint is False:\n",
        "    use_existing_model_from_checkpoint = False\n",
        "# print(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U1R4yDlC8kR"
      },
      "source": [
        "## 4. Batch handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "f7_tc2Lirb1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f90121a-97fa-4e36-ee20-0685ad16154c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2090332 records\n"
          ]
        }
      ],
      "source": [
        "td.init_getitem(sample_type='encoded', sample_length=params['sequence_len']+1+params['joint_state_training'], content_stepping=1)\n",
        "num_records = len(td)\n",
        "print(f\"{num_records} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zZbbsNm0cOeW"
      },
      "outputs": [],
      "source": [
        "def get_sample_sub_batch(sample_batch, batch_size, sub_index=0):\n",
        "    for i in range(batch_size):\n",
        "        Xi = sample_batch[sub_index:-1-params['joint_state_training']+sub_index]\n",
        "        if params['joint_state_training']+sub_index == 0:\n",
        "            yi = sample_batch[sub_index+1:]\n",
        "        else:\n",
        "            yi = sample_batch[sub_index+1:-params['joint_state_training']+sub_index]\n",
        "        if i==0:\n",
        "            # smpX=np.array(Xi, dtype=np.float32)\n",
        "            smpX=np.array(Xi, dtype=np.int32)\n",
        "            smpy=np.array(yi, dtype=np.int32)\n",
        "        else:\n",
        "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
        "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
        "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
        "    return np.array(smpX), np.array(smpy)\n",
        "\n",
        "def get_sample_batch(td, batch_size):\n",
        "    sample_batch = td.get_random_item()\n",
        "    return get_sample_sub_batch(sample_batch, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jY3hUuhQYzdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0948ee-62d6-4bbe-b5c8-c9c39984ad22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches = 8165\n"
          ]
        }
      ],
      "source": [
        "num_batches = num_records // params['batch_size']\n",
        "print(f\"num_batches = {num_batches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bgVHUkbhdK9y"
      },
      "outputs": [],
      "source": [
        "sample_data = None\n",
        "\n",
        "def get_torch_subbatch(td, batch_size, device, split=None, sub_index=0):\n",
        "    global sample_data\n",
        "    if sub_index==0:\n",
        "        sample_data = td.get_random_item()\n",
        "    x, y = get_sample_sub_batch(sample_data, batch_size, sub_index)\n",
        "    tx = torch.tensor(x, dtype=torch.long).to(device)\n",
        "    tx.requires_grad = False\n",
        "    ty = torch.tensor(y, dtype=torch.long).to(device)\n",
        "    ty.requires_grad = False\n",
        "    return tx, ty\n",
        "\n",
        "def get_torch_batch(td, batch_size, device, split=None):\n",
        "    x, y = get_sample_batch(td, batch_size)\n",
        "    tx = torch.tensor(x, dtype=torch.long).to(device)\n",
        "    tx.requires_grad = False\n",
        "    ty = torch.tensor(y, dtype=torch.long).to(device)\n",
        "    ty.requires_grad = False\n",
        "    return tx, ty\n",
        "\n",
        "def get_zero_state(batch_size, sequence_len, hidden_size, device):\n",
        "    zstate = torch.zeros(batch_size, sequence_len, hidden_size, device=device)\n",
        "    zstate.requires_grad = False\n",
        "    return zstate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvbi6kjXC8kS"
      },
      "source": [
        "## 5. Loss and training helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pdaulm1VdK9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "581fc417-8fb2-4c47-a551-cd7622667414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating model...\n",
            "No saved state, no /content/drive/My Drive/Colab Notebooks/neo_philosophers/model/ngpt_COMP_neo_philosophers_v2_pt/model.pt, starting from scratch.\n",
            "Compiling...\n",
            "Compile ok.\n",
            "OptimizedModule(\n",
            "  (_orig_mod): MultiHeadSelfAttentionWithCompressionState(\n",
            "    (token_embedding_table): Embedding(50000, 128)\n",
            "    (position_embedding_table): Embedding(128, 128)\n",
            "    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (lm_head): Linear(in_features=128, out_features=50000, bias=True)\n",
            "  )\n",
            ")\n",
            "12.86664 M parameters\n"
          ]
        }
      ],
      "source": [
        "print(\"creating model...\")\n",
        "try:\n",
        "    # Colab + torch 2 -> lots of garbage.\n",
        "    if model is not None:\n",
        "        del model\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "if params['stateful'] is False:\n",
        "    if params['linear_yoke_hidden_index'] == -1:\n",
        "        model = MultiHeadSelfAttention(vocab_size=params['vocab_size'], embedding_size=params['embedding_size'],\n",
        "                                       sequence_len=params['sequence_len'], dropout=params['dropout'],\n",
        "                                       num_heads=params['heads'], num_layers=params['mhsa_layers'],\n",
        "                                       causal=params['causal'], device=device)\n",
        "    else:\n",
        "        model = MultiHeadSelfAttentionWithCompression(vocab_size=params['vocab_size'], embedding_size=params['embedding_size'],\n",
        "                                       sequence_len=params['sequence_len'], dropout=params['dropout'],\n",
        "                                       num_heads=params['heads'], num_layers=params['mhsa_layers'],\n",
        "                                       causal=params['causal'], linear_non_linearity=params['linear_non_linearity'],\n",
        "                                       linear_yoke=(params['linear_yoke_hidden_index'], params['linear_yoke_size'], params['linear_yoke_residual']),\n",
        "                                       device=device)\n",
        "else:\n",
        "    model = MultiHeadSelfAttentionWithCompressionState(vocab_size=params['vocab_size'], embedding_size=params['embedding_size'],\n",
        "                                       sequence_len=params['sequence_len'], dropout=params['dropout'],\n",
        "                                       num_heads=params['heads'], num_layers=params['mhsa_layers'],\n",
        "                                       causal=params['causal'], linear_non_linearity=params['linear_non_linearity'],\n",
        "                                       linear_yoke=(params['linear_yoke_hidden_index'], params['linear_yoke_size'], params['linear_yoke_residual']),\n",
        "                                       device=device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
        "\n",
        "model = model.to(device)\n",
        "if use_existing_model_from_checkpoint is True:\n",
        "    params_load = MJ.load_checkpoint(params, model, optimizer, file_path=model_file_path, updatable_keys=updatable_keys, device=device, log=log) # torch.device(\"cpu\"))\n",
        "    if params_load is not None:\n",
        "        params = params_load\n",
        "model = model.to(device)\n",
        "for state in optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.to(device)\n",
        "\n",
        "if device == 'cuda':\n",
        "    print(\"Compiling...\")\n",
        "    model = torch.compile(model)\n",
        "    print(\"Compile ok.\")\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision('high')\n",
        "    except:\n",
        "        print(\"Seems no tensor cores for that.\")\n",
        "if 'current_epoch' in params:\n",
        "    ep = params['current_epoch']\n",
        "else:\n",
        "    ep=0\n",
        "if 'current_loss' in params:\n",
        "    ls = params['current_loss']\n",
        "else:\n",
        "    ls=0\n",
        "\n",
        "if ep==0 and ls==0:\n",
        "    start_iter = 0\n",
        "else:\n",
        "    start_iter = ep\n",
        "    current_loss = ls\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(model)\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QnMCWf5AZn1-"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(device):\n",
        "    # XXX: this does take data for train and val from SAME pool!\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(params['test_iterations'])\n",
        "        for k in range(params['test_iterations']):\n",
        "            print(\".\", end=\"\", flush=True)\n",
        "            X, Y = get_torch_batch(td, params['batch_size'], device, split)\n",
        "            if params['stateful'] is False:\n",
        "                logits, loss = model(X, Y)\n",
        "            else:\n",
        "                state = get_zero_state(X.shape[0], params['sequence_len'], params['linear_yoke_size'], device)\n",
        "                logits, loss, state = model(X, Y, state=state)\n",
        "                # print(k, state)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    print(\"\\r\", end=\"\", flush=True)\n",
        "    mloss = (out['train']+out['val'])/2.0\n",
        "    return mloss\n",
        "\n",
        "def generate_sample(td, device, prompt=' ', toks=100, state=None, temperature=1.0, top_k=None, pad=False):\n",
        "    # generate from the model\n",
        "    # context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    model.eval()\n",
        "    if pad is True:\n",
        "        while len(prompt)<params['sequence_len']:\n",
        "            if len(prompt)==params['sequence_len']-1:\n",
        "                prompt = '\\n' + prompt\n",
        "            else:\n",
        "                prompt = ' ' + prompt\n",
        "    context = torch.tensor([td.encode(prompt)]).to(device)\n",
        "    if params['stateful'] is False:\n",
        "        answer = model.generate(context, max_new_tokens=toks, temperature=temperature, top_k=top_k)\n",
        "    else:\n",
        "        if state is None:\n",
        "            print()\n",
        "            print(\"Please don't put state=None in generator!\")\n",
        "            state = get_zero_state(1, params['sequence_len'], params['linear_yoke_size'], device)\n",
        "        answer, state = model.generate(idx=context, max_new_tokens=toks, state=state, temperature=temperature, top_k=top_k)\n",
        "\n",
        "    txt = td.decode(answer[0].tolist())\n",
        "    # Identify memorisation of text by highlighting verbatim quotes from sources\n",
        "    # that are longer than 10 chars. HTML colorcoded output for source identification:\n",
        "    td.source_highlight(txt, min_quote_size=10, dark_mode=False, display_ref_anchor=False)\n",
        "    if params['stateful'] is False:\n",
        "        return txt\n",
        "    else:\n",
        "        return txt, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "N2uWm6CTC8kT"
      },
      "outputs": [],
      "source": [
        "# @torch.jit.script\n",
        "# @torch.compile\n",
        "def do_train_step(xb, yb, device, state=None):\n",
        "    model.train()\n",
        "    if params['stateful'] is False:\n",
        "        logits, loss = model(xb, yb)\n",
        "    else:\n",
        "        # XXX continuous training date & state!\n",
        "        if state is None:\n",
        "            state = get_zero_state(xb.shape[0], params['sequence_len'], params['linear_yoke_size'], device)\n",
        "        logits, loss, state = model(xb, targets=yb, state=state)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if params['stateful'] is True:\n",
        "        return state.detach()\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aZpMI7_iMdR6",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "a08c6943-5709-4366-f501-1ce4888a35fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training, start at 2023-11-13 18:27:27...\n",
            "Iteration:     1[1/4]/256/1000000"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-08f5173df4eb>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\rIteration: {iter+1:5d}[{i+1}/{params['joint_state_training']}]/{((iter+1)//params['sample_every_n_iterations']+1)*params['sample_every_n_iterations']}/{params['max_iterations']}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_torch_subbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;31m# state = torch.cat((state, state[:, -1:, :]), dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# state[:, -1, :] = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-b59774d9fa24>\u001b[0m in \u001b[0;36mdo_train_step\u001b[0;34m(xb, yb, device, state)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_zero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequence_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'linear_yoke_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mdynamic_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ml_indie_tools/pytorch_tr_compr_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets, state)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mdynamic_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   3903\u001b[0m         \u001b[0mfull_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3904\u001b[0m         \u001b[0mfull_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3905\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3907\u001b[0m     \u001b[0;31m# Just for convenience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_boxed_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boxed_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mruntime_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                     \u001b[0margs_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_force_original_view_tracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 all_outs = call_func_with_args(\n\u001b[0m\u001b[1;32m   2528\u001b[0m                     \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m                     \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mcall_func_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m   1504\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_boxed_call\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0;31m# TODO: Please remove soon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_boxed_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boxed_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_context\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_SingleLevelFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   3008\u001b[0m             \u001b[0;31m# - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3009\u001b[0m             \u001b[0;31m#   of the original view, and not the synthetic base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3010\u001b[0;31m             fw_outs = call_func_with_args(\n\u001b[0m\u001b[1;32m   3011\u001b[0m                 \u001b[0mCompiledFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_fw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3012\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mcall_func_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m   1504\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_boxed_call\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0;31m# TODO: Please remove soon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_current_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0mcopy_misaligned_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_to_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\u001b[0m in \u001b[0;36m_run_from_cache\u001b[0;34m(compiled_graph, inputs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         ).call\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/torchinductor_root/s3/cs3mhagrxml7xhf2mfr4okjdzngxczjbtspabbmhfuz32iklp7w6.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mextern_kernels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals_150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreinterpret_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals_149\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuf513\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mprimals_150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0mbuf516\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty_strided\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m         \u001b[0;31m# Source Nodes: [cross_entropy], Original ATen: [aten._log_softmax]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m         \u001b[0mtriton_red_fused__log_softmax_12\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf513\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf516\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.10 GiB. GPU 0 has a total capacty of 15.77 GiB of which 1.99 GiB is free. Process 35452 has 13.78 GiB memory in use. Of the allocated memory 8.31 GiB is allocated by PyTorch, and 4.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "dt0 = time.time()\n",
        "sdt = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "print(f\"training, start at {sdt}...\")\n",
        "gen_id = 0\n",
        "iter_bench = 1\n",
        "current_loss = estimate_loss(device)\n",
        "if params['stateful'] is True:\n",
        "    gen_state = get_zero_state(1, params['sequence_len'], params['linear_yoke_size'], device=device)\n",
        "else:\n",
        "    gen_state = None\n",
        "inputs = [\"What is the difference between good and evil? The difference \", \"How did everything come into existence? The origin \", \"What was at the beginning of time? Time itself \", \"How are physics, quantum-mechanics and consciousness related? The relation between \", \"How to attain complete self-awareness? Complete \", \"What is the nature of reality? The nature \", \"How be a good human being? A human \"]\n",
        "for iter in range(start_iter, params['max_iterations']):\n",
        "    print(f\"\\rIteration: {iter+1:5d}/{((iter+1)//params['sample_every_n_iterations']+1)*params['sample_every_n_iterations']}/{params['max_iterations']}\", end=\"\", flush=True)\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if (iter + 1) % params['sample_every_n_iterations'] == 0 or iter == params['max_iterations'] - 1:\n",
        "        dt = time.time()\n",
        "        print(f\"\\rloss eval\", end=\"\", flush=True)\n",
        "        current_loss = estimate_loss(device)\n",
        "        print(\n",
        "            f\"step {iter+1}: train loss {current_loss:.4f}, time {(dt-dt0)/iter_bench:.3f} sec/iter\"\n",
        "        )\n",
        "        iter_bench = 1\n",
        "        sdt = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"Sample at {sdt}:\", flush=True)\n",
        "        for temperature in [0.75]:\n",
        "            print(f\"--------temperature: {temperature} ---------\")\n",
        "            prompt = inputs[gen_id%len(inputs)]\n",
        "            print(f\"Prompt: {prompt}\")\n",
        "            generate_sample(td=td, device=device, prompt=prompt, toks=params['sample_size'], state=gen_state, temperature=temperature, top_k=16)\n",
        "        print(\"-------------------------------------------\")\n",
        "        gen_id += 1\n",
        "        dt0 = time.time()\n",
        "\n",
        "    if params['stateful'] is False or params['joint_state_training'] == 0:\n",
        "        xb, yb = get_torch_batch(td, params['batch_size'], device, \"train\")\n",
        "        do_train_step(xb, yb, device=device)\n",
        "    else:\n",
        "        state = get_zero_state(1, params['sequence_len'], params['linear_yoke_size'], device=device)\n",
        "        state.requires_grad = False\n",
        "        for i in range(params['joint_state_training']):\n",
        "            print(f\"\\rIteration: {iter+1:5d}[{i+1}/{params['joint_state_training']}]/{((iter+1)//params['sample_every_n_iterations']+1)*params['sample_every_n_iterations']}/{params['max_iterations']}\", end=\"\", flush=True)\n",
        "            xb, yb = get_torch_subbatch(td, params['batch_size'], device, \"train\", i)\n",
        "            state = do_train_step(xb, yb, device=device, state=state)\n",
        "            # state = torch.cat((state, state[:, -1:, :]), dim=1)\n",
        "            # state[:, -1, :] = 0\n",
        "            state = torch.cat((state[:, :1, :], state), dim=1)\n",
        "            # state[:, 0, :] = 0\n",
        "            state = state [:, -params['sequence_len']:, :]\n",
        "            # state.detach() # requires_grad = False\n",
        "\n",
        "    start_iter = iter\n",
        "    iter_bench += 1\n",
        "    if (iter+1)%params['save_every_n_iterations'] == 0:\n",
        "        MJ.save_checkpoint(params, model, optimizer, iter, current_loss, file_path=model_file_path, log=log)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "othN-Vnt5EZT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# for t in [0.5, 1.5]:\n",
        "#     print(f\"------Temperature {t}--------\")\n",
        "#     generate_sample(td, device, prompt=\"How are consciousness and quantum mechanics related?\", toks=150, temperature=t, top_k=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UNG5wWhC8kU",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADm9ycuA2ik7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
      "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
    },
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VmWbteSFQtfq",
        "yWE_ZZMKEARV"
      ],
      "name": "torch_transformer_poet.ipynb",
      "provenance": [],
      "gpuType": "V100",
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}