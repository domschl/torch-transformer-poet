{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-transformer-poet/blob/main/torch_transformer_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Torch-Transformer-Poet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DabS0VZ-1Zp0"
   },
   "source": [
    "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gF-7qFzMdnN1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml-indie-tools in ./lib/python3.12/site-packages (0.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ml-indie-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jtpy59Yq-Qfz"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # from: https://github.com/pytorch/pytorch/issues/107960  (libcuda not found)\n",
    "    !export LC_ALL=\"en_US.UTF-8\"\n",
    "    !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "    !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "    !ldconfig /usr/lib64-nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U5T4m6earb1e"
   },
   "outputs": [],
   "source": [
    "from ml_indie_tools.env_tools import MLEnv\n",
    "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from ml_indie_tools.Text_Dataset import Text_Dataset\n",
    "\n",
    "from ml_indie_tools.Calibre_Dataset import Calibre_Dataset\n",
    "from ml_indie_tools.Folder_Dataset import Folder_Dataset\n",
    "\n",
    "import ml_indie_tools.pytorch_meta_tools as MJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jVcwvURB5EZN"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.Logger(\"Main\")\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "A pytorch deep multi-head attention model for text generation following Andrej Karpathy's [video-lecture-ng](https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py)\n",
    "\n",
    "This code can use either CPU, GPU, or Apple Silicon. Google Colab is supported too, select the corresponding Colab runtime (menu: **`Runtime / Change runtime type`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "llPw84PkEAP2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OS: Linux, Python: 3.12.3, Jupyter Notebook Pytorch: 2.4.0.dev20240501+cu121, GPU: NVIDIA GeForce RTX 4070 Ti (/  285W |       4MiB), CPU'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_batch_data = None   # Do regenerate time-consuming training data, if aleady cached.\n",
    "\n",
    "ml_env = MLEnv(platform='pt', accelerator='fastest')\n",
    "ml_env.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Qg3ZPBmC8kO"
   },
   "source": [
    "## 1. Project configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "t-TP3Pnsrb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path (all projects) : . (This will be '.' (current dir) for local projects, and a google drive path for Colab)\n",
      "Project path             : . (Changes to the file system happen only below this project path\n",
      "Model path (snapshots)   : ./model/tr_neo_philosophers_v3_pt (Model weights and snapshots are stored here)\n",
      "Data path (training data): ./data (Training data will be downloaded here)\n",
      "Log dir (tensorboard)    : ./logs (it doesn't work to put logs on gdrive due to caching, hence local dir)\n"
     ]
    }
   ],
   "source": [
    "# project_name = 'women_writers'\n",
    "# project_name='research'\n",
    "project_name='neo_philosophers'\n",
    "model_cpu = None\n",
    "model_name=f'tr_{project_name}_v3_pt'\n",
    "\n",
    "use_preprocessed_data = True                      # Use already tokenized data\n",
    "use_existing_model_from_checkpoint = False         # Try to load checkpoint of training\n",
    "use_torch_compile = False                           # Requires a modern graphics card with torch compile backend support\n",
    "skip_additional_texts = True                       # Don't look for other data sources in `additional_texts.json`\n",
    "\n",
    "if 'google.colab' in sys.modules:  # Google colab notebooks run on server that provide UTC time, we adapt logs to local time:\n",
    "    local_timezone = ZoneInfo('Europe/Berlin')\n",
    "else:\n",
    "    local_timezone = None\n",
    "\n",
    "# NOTICE: This will request access to Google Drive, if running on Google Colab. Google Drive is used to store snapshots\n",
    "# training data. See project ml-indie-tools: https://github.com/domschl/ml-indie-tools\n",
    "#\n",
    "# Note: you need to allow popups in your browser for COLAB, otherwise you won't see the google-drive login box, and drive access will fail!\n",
    "\n",
    "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device\n",
    "\n",
    "print(f\"Root path (all projects) : {root_path} (This will be '.' (current dir) for local projects, and a google drive path for Colab)\")\n",
    "print(f\"Project path             : {project_path} (Changes to the file system happen only below this project path\")\n",
    "print(f\"Model path (snapshots)   : {model_path} (Model weights and snapshots are stored here)\")\n",
    "print(f\"Data path (training data): {data_path} (Training data will be downloaded here)\")\n",
    "print(f\"Log dir (tensorboard)    : {log_path} (it doesn't work to put logs on gdrive due to caching, hence local dir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  2.1 Text data from Project Gutenberg\n",
    "\n",
    "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training,\n",
    "encoding, batch generation, and formatted source display. It read some\n",
    "books from Project Gutenberg and supports creation of training batches.\n",
    "The output functions support highlighting to allow to compare generated\n",
    "texts with the actual sources to help to identify identical (memorized)\n",
    "parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False # Set to false for white background. HTML-text-compare uses background-colorization to identify different sources. Those background colors are dependent on the theme type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BF8eyWnCrb1h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Loading tokenizer from ./data/neo_philosophers_tokens.json\n",
      "INFO:Datasets:Loading tokenizer done.\n"
     ]
    }
   ],
   "source": [
    "token_file = os.path.join(data_path,f\"{project_name}_tokens.json\")\n",
    "if use_preprocessed_data is True:\n",
    "    if os.path.exists(token_file):\n",
    "        td = Text_Dataset()\n",
    "        td.load_tokenizer(token_file)\n",
    "    else:\n",
    "        use_preprocessed_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "C66X7ynnrb1h"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False:\n",
    "    cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
    "    gd = Gutenberg_Dataset(cache_dir=cache_dir)\n",
    "\n",
    "    if project_name == 'women_writers':  # sample searches\n",
    "        search_spec= {\n",
    "            \"author\": [\"Emily Brontë\", \"Jane Austen\", \"Virginia Woolf\"],\n",
    "            \"language\": [\"english\"]\n",
    "        }\n",
    "        book_list=gd.search(search_spec)\n",
    "    elif project_name == 'neo_philosophers':\n",
    "        search_spec = {\n",
    "            \"author\": [\"Immanuel Kant\", \"Friedrich Nietzsche\", \"Wilhelm Hegel\", \"Arthur Schopenhauer\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list=gd.search(search_spec)\n",
    "        search_spec = {\n",
    "            \"author\": [\"Plato\", \"Platon\"],\n",
    "            \"title\": [\"Timaeus\", \"Critias\", \"Symposium\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list+=gd.search(search_spec)\n",
    "        search_spec = {\n",
    "            \"title\": [\"Buddh\", \"Sutra\"],\n",
    "            \"language\": [\"english\", \"german\"]\n",
    "        }\n",
    "        book_list+=gd.search(search_spec)\n",
    "    else:\n",
    "        search_spec = {}\n",
    "        book_list = []\n",
    "\n",
    "    book_cnt = len(book_list)\n",
    "    print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
    "\n",
    "    if book_cnt > 0:\n",
    "        if book_cnt<80:\n",
    "            # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts,\n",
    "            # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
    "            book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)\n",
    "        else:\n",
    "            logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")\n",
    "\n",
    "        for i in range(len(book_list)):\n",
    "            if 'author' not in book_list[i]:\n",
    "                book_list[i]['author']='unknown'\n",
    "            print(f\"{i}: {book_list[i]['title']} - {book_list[i]['author']}, {book_list[i]['ebook_id']}\")\n",
    "\n",
    "        if project_name == 'women_writers':\n",
    "            select = (\"Bennett\", \"1342\", \"5670\", \"1245\", \"161\", \"141\", \"121\", \"105\", \"Susan\", \"Wuthering\", \"Emma\", \"Voyage\")  # List unique single-words from title or ebook_id to select a given book\n",
    "            sub_book_list = [book_list[i] for i in range(len(book_list)) if not set([book_list[i]['ebook_id']]+book_list[i]['title'].split(' ')).isdisjoint(set(select))]\n",
    "        else:\n",
    "            sub_book_list = book_list\n",
    "\n",
    "        print(\"Using:\")\n",
    "        for i in range(len(sub_book_list)):\n",
    "            if 'author' not in sub_book_list[i]:\n",
    "                sub_book_list[i]['author']='unknown'\n",
    "            print(f\"{i+1}: {sub_book_list[i]['title']} - {sub_book_list[i]['author']}\")\n",
    "\n",
    "        td = Text_Dataset(sub_book_list)\n",
    "    else:\n",
    "        td = Text_Dataset()()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxNIc7gL9UNg"
   },
   "source": [
    "## 2.2 Additional training material from folders or Calibre library\n",
    "\n",
    "This looks for a file `additional_texts.json` in the `project_path` as shown above.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"local_texts\": [\"/some/directory/that/contains/texts\"],\n",
    "  \"calibre\": \"/home/myuser/Calibre Library\"\n",
    "}\n",
    "```\n",
    "\n",
    "If the folder(s) defined in `local_texts` contain text files with default endings `.txt`, `.md`, `.org`, or `.py` (can be configured), they are added to the training data. Folders are searched recursively.\n",
    "\n",
    "If the path defined in `calibre` contains a Calibre database, all text files (`.txt` only) within that library are added to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1NYdjlW65EZP"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False and skip_additional_texts is False:\n",
    "    additional = os.path.join(project_path, \"additional_texts.json\")\n",
    "    print(f\"Looking for description of additional sources in {additional}\")\n",
    "    if os.path.exists(additional) is True:\n",
    "        with open(additional, 'r') as f:\n",
    "            add_desc = json.load(f)\n",
    "            if 'local_texts' in add_desc:\n",
    "                fd = Folder_Dataset()\n",
    "                for text_path in add_desc['local_texts']:\n",
    "                    print(f\"Loading texts from {text_path}\")\n",
    "                    fd.load_index(text_path, use_aliases=False, max_file_size=100000)\n",
    "                td.load_texts(fd.records[:10000])\n",
    "            if 'calibre' in add_desc:\n",
    "                cal_path = add_desc['calibre']\n",
    "                if os.path.exists(cal_path):\n",
    "                    print(f\"Loading text from calibre at {cal_path}\")\n",
    "                    cd = Calibre_Dataset(cal_path)\n",
    "                    cd.load_index(max_file_size=100000000)\n",
    "                    td.load_texts(cd.records[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSm4f9NSC8kQ"
   },
   "source": [
    "## 2.3 Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bsyBjqFyC8kQ"
   },
   "outputs": [],
   "source": [
    "if use_preprocessed_data is False:\n",
    "    MAX_TOKENS = 10000  # This becomes vocab_size\n",
    "    MAX_NGRAM_LEN = 4   # Max length of a token\n",
    "    CHUNK_SIZE = 500000 # Split larger texts in chunks, if not None\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"Starting tokenizer with token length from 1..{MAX_NGRAM_LEN} with a max of {MAX_TOKENS} unique tokens,\")\n",
    "    print(\"this can take considerable time...\")\n",
    "\n",
    "    # Better tested NGRAM tokenizer:\n",
    "    # td.init_tokenizer(tokenizer='ngram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS) \n",
    "    # or alternative 'BYTEGRAM' (more experimental, can encode arbitrary UTF-8)\n",
    "    # td.init_tokenizer(tokenizer='bytegram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS, chunk_size=CHUNK_SIZE)\n",
    "    td.init_tokenizer(tokenizer='bytegram', max_ngrams=MAX_NGRAM_LEN, max_tokens=MAX_TOKENS, chunk_size=CHUNK_SIZE)\n",
    "    td.save_tokenizer(token_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG03WA_yC8kR"
   },
   "source": [
    "## 3. Model metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UPMwIn2gC8kR"
   },
   "outputs": [],
   "source": [
    "params = None\n",
    "updatable_keys=['learning_rate', 'batch_size', 'current_epoch', 'current_loss',\n",
    "                 'sample_every_n_iterations', 'sample_size', 'save_every_n_iterations', 'max_iterations']\n",
    "attn_layers = 8\n",
    "dims = 256\n",
    "sequence_length = 256\n",
    "\n",
    "params = { # Multi-head self-attention\n",
    "        'meta_name_template': '{mhsa_layers}x{heads}x{units}x{vocab_size}',\n",
    "\n",
    "        'mhsa_layers': attn_layers,\n",
    "        'heads': 8,\n",
    "        'vocab_size': td.get_unique_token_count(),\n",
    "        'sequence_len': sequence_length,\n",
    "        'embedding_size': dims,\n",
    "        'test_iterations': 20,  # number of epocs for loss estimation\n",
    "\n",
    "        'batch_size': 64,      # A100: 80, V100: 32, None: set in depedence of graphics card (s.b.)\n",
    "        'learning_rate': 2e-4,   # None: Set in dependence of graphics hw\n",
    "\n",
    "        'sample_every_n_iterations': 256,\n",
    "        'sample_size': 128,\n",
    "        'save_every_n_iterations': 4096,\n",
    "\n",
    "        'max_iterations': 100000000  # maximum number of training iterations\n",
    "    }\n",
    "\n",
    "model_file_path = MJ.get_model_filename(model_path)\n",
    "if use_existing_model_from_checkpoint is True:\n",
    "    params = MJ.load_model_metadata_from_checkpoint(params, updatable_keys, model_file_path, device=device, log=log) # torch.device('cpu'))\n",
    "if params == None or use_existing_model_from_checkpoint is False:\n",
    "    use_existing_model_from_checkpoint = False\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U1R4yDlC8kR"
   },
   "source": [
    "## 4. Batch handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f7_tc2Lirb1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15392802 records\n"
     ]
    }
   ],
   "source": [
    "joint_training=0\n",
    "td.init_getitem(sample_type='encoded', sample_length=params['sequence_len']+1+joint_training, content_stepping=1)\n",
    "num_records = len(td)\n",
    "print(f\"{num_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zZbbsNm0cOeW"
   },
   "outputs": [],
   "source": [
    "def get_sample_sub_batch(sample_batch, batch_size, sub_index=0):\n",
    "    joint_training=0\n",
    "    for i in range(batch_size):\n",
    "        Xi = sample_batch[sub_index:-1-joint_training+sub_index]\n",
    "        yi = sample_batch[sub_index+1:]\n",
    "        if i==0:\n",
    "            # smpX=np.array(Xi, dtype=np.float32)\n",
    "            smpX=np.array(Xi, dtype=np.int32)\n",
    "            smpy=np.array(yi, dtype=np.int32)\n",
    "        else:\n",
    "            # smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.int32)))\n",
    "            smpy = np.vstack((smpy, np.array(yi, dtype=np.int32)))\n",
    "    return np.array(smpX), np.array(smpy)\n",
    "\n",
    "def get_sample_batch(td, batch_size):\n",
    "    sample_batch = td.get_random_item()\n",
    "    return get_sample_sub_batch(sample_batch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jY3hUuhQYzdT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches = 240512\n"
     ]
    }
   ],
   "source": [
    "num_batches = num_records // params['batch_size']\n",
    "print(f\"num_batches = {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 256), (2, 256))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_sample_batch(td, 2)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bgVHUkbhdK9y"
   },
   "outputs": [],
   "source": [
    "sample_data = None\n",
    "\n",
    "def get_torch_subbatch(td, batch_size, device, split=None, sub_index=0):\n",
    "    global sample_data\n",
    "    if sub_index==0:\n",
    "        sample_data = td.get_random_item()\n",
    "    x, y = get_sample_sub_batch(sample_data, batch_size, sub_index)\n",
    "    tx = torch.tensor(x, dtype=torch.long).to(device)\n",
    "    tx.requires_grad = False\n",
    "    ty = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    ty.requires_grad = False\n",
    "    return tx, ty\n",
    "\n",
    "def get_torch_batch(td, batch_size, device, split=None):\n",
    "    x, y = get_sample_batch(td, batch_size)\n",
    "    tx = torch.tensor(x, dtype=torch.long).to(device)\n",
    "    tx.requires_grad = False\n",
    "    ty = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    ty.requires_grad = False\n",
    "    return tx, ty\n",
    "\n",
    "def get_zero_state(batch_size, sequence_len, hidden_size, device):\n",
    "    zstate = torch.zeros(batch_size, sequence_len, hidden_size, device=device)\n",
    "    zstate.requires_grad = False\n",
    "    return zstate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pvbi6kjXC8kS"
   },
   "source": [
    "## 5. Loss and training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000, device=None):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model, device=device)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "class MultiHeadSelfAttentionWithMemory(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_size,\n",
    "        sequence_len,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        device=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if device is None:\n",
    "            raise ValueError(\n",
    "                \"Device is None at MultiHeadSelfAttentionWithMemory\"\n",
    "            )\n",
    "        self.device = device\n",
    "        self.sequence_len = sequence_len\n",
    "        context_sub_layers = num_layers // 2\n",
    "        self.context_sub_layers = context_sub_layers\n",
    "        dims = embedding_size\n",
    "        self.dims = dims\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size, device=device)\n",
    "        self.pos_encoder = PositionalEncoding(dims, dropout=0.0, max_len=10000, device=device)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dims, nhead=num_heads, dim_feedforward=dims*4, dropout=0.0, batch_first=True, device=device) # , batch_first=True\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers) # context_sub_layers)\n",
    "        # encoder_layer2 = nn.TransformerEncoderLayer(d_model=dims, nhead=num_heads, dim_feedforward=dims*4, dropout=0.0, device=device)\n",
    "        # self.transformer2 = nn.TransformerEncoder(encoder_layer2, num_layers=num_layers - context_sub_layers)\n",
    "        self.out_proj = nn.Linear(dims, vocab_size, device=device)\n",
    "\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, D = idx.shape\n",
    "        x = self.embedding(idx) * math.sqrt(D)\n",
    "        x = self.pos_encoder(x) \n",
    "        x_mask = nn.Transformer.generate_square_subsequent_mask(D).to(device)\n",
    "        x = self.transformer(x, x_mask)\n",
    "        # x = self.transformer2(x, x_mask)\n",
    "        logits = self.out_proj(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate new tokens given a context\n",
    "\n",
    "        Note: for apple MPS, top_k is limited max 16 vor older torchs! ((01/2023) implementation limitation)\n",
    "        See: https://github.com/pytorch/pytorch/issues/78915\n",
    "        Solved in: https://github.com/pytorch/pytorch/pull/94639 (03/2023)\n",
    "\n",
    "        :param idx: the context (B,T) tensor of indices\n",
    "        :param max_new_tokens: the maximum number of tokens to generate\n",
    "        :param temperature: the temperature to use for sampling\n",
    "        :param top_k: the number of top tokens to consider\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last sequence_len tokens\n",
    "            idx_cond = idx[:, -self.sequence_len :]\n",
    "            # print(idx_cond.shape)\n",
    "            # get the predictions\n",
    "            logits = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply temperature\n",
    "            if temperature != 1.0 and temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pdaulm1VdK9z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model...\n",
      "MultiHeadSelfAttentionWithMemory(\n",
      "  (embedding): Embedding(10000, 256)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (out_proj): Linear(in_features=256, out_features=10000, bias=True)\n",
      ")\n",
      "11.44808 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"creating model...\")\n",
    "try:\n",
    "    # Colab + torch 2 -> lots of garbage.\n",
    "    if model is not None:\n",
    "        del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "model = MultiHeadSelfAttentionWithMemory(vocab_size=params['vocab_size'], embedding_size=params['embedding_size'],\n",
    "                                       sequence_len=params['sequence_len'],\n",
    "                                       num_heads=params['heads'], num_layers=params['mhsa_layers'],\n",
    "                                       device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "model = model.to(device)\n",
    "if use_existing_model_from_checkpoint is True:\n",
    "    params_load = MJ.load_checkpoint(params, model, optimizer, file_path=model_file_path, updatable_keys=updatable_keys, device=device, log=log) # torch.device(\"cpu\"))\n",
    "    if params_load is not None:\n",
    "        params = params_load\n",
    "model = model.to(device)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "if use_torch_compile is True:\n",
    "    if device == 'cuda':\n",
    "        print(\"Compiling...\")\n",
    "        model = torch.compile(model)\n",
    "        print(\"Compile ok.\")\n",
    "        try:\n",
    "            torch.set_float32_matmul_precision('high')\n",
    "        except:\n",
    "            print(\"Seems no tensor cores for that.\")\n",
    "    # elif str(device) == 'mps':\n",
    "    #     print(\"Compiling...\")\n",
    "    #     model = torch.compile(model)\n",
    "    #     print(\"Compile ok.\")\n",
    "\n",
    "if 'current_epoch' in params:\n",
    "    ep = params['current_epoch']\n",
    "else:\n",
    "    ep=0\n",
    "if 'current_loss' in params:\n",
    "    ls = params['current_loss']\n",
    "else:\n",
    "    ls=0\n",
    "\n",
    "if ep==0 and ls==0:\n",
    "    start_iter = 0\n",
    "else:\n",
    "    start_iter = ep\n",
    "    current_loss = ls\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "QnMCWf5AZn1-"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(device):\n",
    "    # XXX: this does take data for train and val from SAME pool!\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(params['test_iterations'])\n",
    "        for k in range(params['test_iterations']):\n",
    "            # if k % (params['test_iterations']/10 + 1) == 0:\n",
    "            #     print(\".\", end=\"\", flush=True)\n",
    "            X, Y = get_torch_batch(td, params['batch_size'], device, split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    print(\"\\r\", end=\"\", flush=True)\n",
    "    mloss = (out['train']+out['val'])/2.0\n",
    "    return mloss\n",
    "\n",
    "def generate_sample(td, device, prompt=' ', toks=100, state=None, temperature=1.0, top_k=None, pad=False):\n",
    "    # generate from the model\n",
    "    # context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    model.eval()\n",
    "    if pad is True:\n",
    "        while len(prompt)<params['sequence_len']:\n",
    "            if len(prompt)==params['sequence_len']-1:\n",
    "                prompt = '\\n' + prompt\n",
    "            else:\n",
    "                prompt = ' ' + prompt\n",
    "    context = torch.tensor([td.encode(prompt)]).to(device)\n",
    "    answer = model.generate(context, max_new_tokens=toks, temperature=temperature, top_k=top_k)\n",
    "    txt = td.decode(answer[0].tolist())\n",
    "    # Identify memorisation of text by highlighting verbatim quotes from sources\n",
    "    # that are longer than 10 chars. HTML colorcoded output for source identification:\n",
    "    td.source_highlight(txt, min_quote_size=10, dark_mode=False, display_ref_anchor=False)\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "N2uWm6CTC8kT"
   },
   "outputs": [],
   "source": [
    "# @torch.jit.script\n",
    "# @torch.compile\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def do_train_step(xb, yb, device, state=None):\n",
    "    model.train()\n",
    "    logits = model(xb)\n",
    "    # print(logits.shape)\n",
    "    output_flat = logits.view(-1, params['vocab_size'])\n",
    "    # output_flat = logits.view(-1, params['vocab_size'])\n",
    "    # print(output_flat.shape)\n",
    "    ybr = yb.view(-1)\n",
    "    # print(ybr.shape)\n",
    "    loss = criterion(output_flat, ybr)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aZpMI7_iMdR6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training, start at 2024-05-02 13:12:47...\n",
      "loss eval:   255/256/100000000 loss: 7.0350step 256: train loss 7.0350, time 0.097 sec/iter\n",
      "Sample at 2024-05-02 13:13:12:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What is the difference between good and evil? The difference \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#edebd0;\">What is the difference between </span><span style=\"background-color:#edebd0;\">good and evil? </span><span style=\"background-color:#ebdef0;\">The difference </span>pthe othe ssand  h and oshthe  t d<br>Efthe  the fthe o theoitiand <span style=\"background-color:#d4e6f1;\">l and<br>the s</span>li and ss<span style=\"background-color:#d4e6e1;\">s<br>and othe</span> os o and s<br>osg sthe d the   theothe dttttsothe sstt<br>ohhssli tslthe to to stss andi ttsthe<span style=\"background-color:#d4efdf;\">  the sand</span> i tds the  the sh theis"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#edebd0;\">Friedrich Nietzsche: The Will to Power, Books III and IV</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#d4e6f1;\">Georg Wilhelm Hegel: Hegel's Lectures on the History of Philosophy: Vol. 1 of 3</span>, <span style=\"background-color:#d4e6e1;\">V. Fausböll: Buddhist birth stories; or, Jataka tales, Vol. 1</span>, <span style=\"background-color:#d4efdf;\">Sir Monier Monier-Williams: Buddhism, In its Connexion With Brahmanism and Hinduism, and In Its Contrast with Christianity</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "loss eval:   511/512/100000000 loss: 6.8775step 512: train loss 6.8775, time 0.097 sec/iter\n",
      "Sample at 2024-05-02 13:13:41:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: How did everything come into existence? The origin \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "How<span style=\"background-color:#d4efdf;\"> did everything </span><span style=\"background-color:#ebdef0;\">come into existence</span>?<span style=\"background-color:#ecf3cf;\"> The origin </span> the sf tho andoand  tionl thetthe  f th the   thes thef to   the f th  in g of s in fwhic and  the f<span style=\"background-color:#e5e8e8;\">      of c</span>tiono of   of otion  the  in ithe gthe oand   in o in  and t the  theothe   of cthe sthe o andh     tion the  the   of  the  the t      thes of c the<span style=\"background-color:#edebd0;\">h and the d</span> thettherotha<span style=\"background-color:#ebdef0;\">to the<br>the</span> lthe <br>theriand d in ws"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d4efdf;\">Friedrich Nietzsche: Thus Spake Zarathustra</span>, <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#ecf3cf;\">Friedrich Nietzsche: The Will to Power, Books I and II</span>, <span style=\"background-color:#e5e8e8;\">Immanuel Kant: Kant's Critique of Judgement</span>, <span style=\"background-color:#edebd0;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 2 of 3)</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "loss eval:   767/768/100000000 loss: 6.7218step 768: train loss 6.7218, time 0.097 sec/iter\n",
      "Sample at 2024-05-02 13:14:11:\n",
      "--------temperature: 0.75 ---------\n",
      "Prompt: What was at the beginning of time? Time itself \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\">What was at</span><span style=\"background-color:#ebdef0;\"> the beginning of t</span>ime?<span style=\"background-color:#e5e8e8;\"> Time itself </span>the t andshe sof thb of  the  the s in l the of th andi to i <span style=\"background-color:#f6ddcc;\">and and of </span>tho thet andt theif ththe st thes thed thed thes in o of  the l thei thes theh of t in g to b to <span style=\"background-color:#f6ddcc;\">o the<br>    </span>t of <span style=\"background-color:#edebd0;\">d the     </span><span style=\"background-color:#ebdef0;\">hand of th</span>o the of a<br>of t the  the if tht of  thati t<span style=\"background-color:#d8daef;\">hes the an</span>d of th and y for l theh andt the thaty the w the t thet in "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Georg Wilhelm Hegel: The History of Philosophy: Volume 3 of 3</span>, <span style=\"background-color:#e5e8e8;\">Lafcadio Hearn: Gleanings in Buddha-Fields</span>, <span style=\"background-color:#f6ddcc;\">Arthur Schopenhauer: On the Fourfold Root of the Principle of Sufficient Reason and On the Will in Nature: Two Essays (revised edition)</span>, <span style=\"background-color:#edebd0;\">Arthur Schopenhauer: The World as Will and Idea (Vol. 2 of 3)</span>, <span style=\"background-color:#d8daef;\">Charles Eliot: Hinduism and Buddhism, Vol. 1 of 3</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Iteration:   888/1024/100000000 loss: 6.6877"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     dt0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     31\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_torch_batch(td, params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], device, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m cur_loss \u001b[38;5;241m=\u001b[39m \u001b[43mdo_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_loss_m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     34\u001b[0m     cur_loss_m \u001b[38;5;241m=\u001b[39m cur_loss\n",
      "Cell \u001b[0;32mIn[22], line 16\u001b[0m, in \u001b[0;36mdo_train_step\u001b[0;34m(xb, yb, device, state)\u001b[0m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output_flat, ybr)\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/gith/domschl/torch-transformer-poet/lib/python3.12/site-packages/torch/_tensor.py:523\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    515\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    516\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[0;32m--> 523\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gith/domschl/torch-transformer-poet/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gith/domschl/torch-transformer-poet/lib/python3.12/site-packages/torch/autograd/graph.py:767\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dt0 = time.time()\n",
    "sdt = datetime.datetime.now(tz=local_timezone).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"training, start at {sdt}...\")\n",
    "gen_id = 0\n",
    "iter_bench = 1\n",
    "cur_loss_m = 0\n",
    "cur_loss_m_avg = 25\n",
    "# current_loss = estimate_loss(device)\n",
    "inputs = [\"What is the difference between good and evil? The difference \", \"How did everything come into existence? The origin \", \"What was at the beginning of time? Time itself \", \"How are physics, quantum-mechanics and consciousness related? The relation between \", \"How to attain complete self-awareness? Complete \", \"What is the nature of reality? The nature \", \"How be a good human being? A human \"]\n",
    "for iter in range(start_iter, params['max_iterations']):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if (iter + 1) % params['sample_every_n_iterations'] == 0 or iter == params['max_iterations'] - 1:\n",
    "        dt = time.time()\n",
    "        print(f\"\\rloss eval\", end=\"\", flush=True)\n",
    "        current_loss = cur_loss_m # estimate_loss(device)\n",
    "        print(\n",
    "            f\"step {iter+1}: train loss {current_loss:.4f}, time {(dt-dt0)/iter_bench:.3f} sec/iter\"\n",
    "        )\n",
    "        iter_bench = 1\n",
    "        sdt = datetime.datetime.now(tz=local_timezone).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"Sample at {sdt}:\", flush=True)\n",
    "        for temperature in [0.75]:\n",
    "            print(f\"--------temperature: {temperature} ---------\")\n",
    "            prompt = inputs[gen_id%len(inputs)]\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            generate_sample(td=td, device=device, prompt=prompt, toks=params['sample_size'], temperature=temperature, top_k=16)\n",
    "        print(\"-------------------------------------------\")\n",
    "        gen_id += 1\n",
    "        dt0 = time.time()\n",
    "\n",
    "    xb, yb = get_torch_batch(td, params['batch_size'], device, \"train\")\n",
    "    cur_loss = do_train_step(xb, yb, device=device)\n",
    "    if cur_loss_m == 0:\n",
    "        cur_loss_m = cur_loss\n",
    "    else:\n",
    "        cur_loss_m = (cur_loss + cur_loss_m * (cur_loss_m_avg-1))/cur_loss_m_avg\n",
    "    print(f\"\\rIteration: {iter+1:5d}/{((iter+1)//params['sample_every_n_iterations']+1)*params['sample_every_n_iterations']}/{params['max_iterations']} loss: {cur_loss_m:.4f}\", end=\"\", flush=True)\n",
    "\n",
    "    start_iter = iter\n",
    "    iter_bench += 1\n",
    "    if (iter+1)%params['save_every_n_iterations'] == 0:\n",
    "        MJ.save_checkpoint(params, model, optimizer, iter, current_loss, file_path=model_file_path, log=log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "othN-Vnt5EZT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for t in [0.5, 1.5]:\n",
    "#     print(f\"------Temperature {t}--------\")\n",
    "#     generate_sample(td, device, prompt=\"How are consciousness and quantum mechanics related?\", toks=150, temperature=t, top_k=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-code below, unfinished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UNG5wWhC8kU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "enc_texts = []\n",
    "for i in range(500):\n",
    "    e = td[i*50000][:256]\n",
    "    tx = torch.tensor([e]).to(device)\n",
    "    enc_texts.append(tx)\n",
    "    texts.append(td.decode(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_texts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADm9ycuA2ik7"
   },
   "outputs": [],
   "source": [
    "emb_text = []\n",
    "cont_text = []\n",
    "for et in enc_texts:\n",
    "    emb_text.append(model.embedding(et))\n",
    "    cont_text.append(model.context(et))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_text[0].shape, cont_text[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_vec = []\n",
    "cont_vec = []\n",
    "for i in range(len(emb_text)):\n",
    "    emb_vec.append(emb_text[i][0].sum(axis=0))\n",
    "    cont_vec.append(cont_text[i][0].sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_vec(a, b):\n",
    "    al = torch.sqrt(torch.dot(a,a))\n",
    "    bl = torch.sqrt(torch.dot(b,b))\n",
    "    an = a/al\n",
    "    bn = b/bl\n",
    "    return torch.dot(an,bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0):\n",
    "    best = -10.0\n",
    "    ind = -1\n",
    "    for j in range(len(emb_vec)):\n",
    "        if i==j:\n",
    "            continue\n",
    "        cos_val = cos_vec(emb_vec[i], emb_vec[j])\n",
    "        if cos_val > best:\n",
    "            best = cos_val\n",
    "            ind = j\n",
    "    # print(f\"{texts[i][:20]} ->{best}: {texts[ind][:20]}\")\n",
    "print()        \n",
    "for i in range(200):\n",
    "    best = 0\n",
    "    ind = -1\n",
    "    for j in range(len(emb_vec)):\n",
    "        if i==j:\n",
    "            continue\n",
    "        cos_val = cos_vec(cont_vec[i], cont_vec[j])\n",
    "        if cos_val > best:\n",
    "            best = cos_val\n",
    "            ind = j\n",
    "    print(f\"{texts[i]} \\n\\n->{best}:\\n\\n {texts[ind]}\")\n",
    "    print(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td[200002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VmWbteSFQtfq",
    "yWE_ZZMKEARV"
   ],
   "gpuClass": "premium",
   "gpuType": "V100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "torch_transformer_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
